#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Unified Agent Framework - Enterprise Edition
Microsoft Agent Framework íŒ¨í„´ í†µí•© (MCP, Approval, Streaming ì§€ì›)
+ Anthropic Skills ì‹œìŠ¤í…œ í†µí•©

============================================================================
ğŸš€ ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ
============================================================================

1. í™˜ê²½ë³€ìˆ˜ ì„¤ì • (.env íŒŒì¼):
   AZURE_OPENAI_API_KEY=your-api-key
   AZURE_OPENAI_ENDPOINT=https://your-endpoint.openai.azure.com/
   AZURE_OPENAI_DEPLOYMENT=your-deployment-name

2. ê°€ì¥ ê°„ë‹¨í•œ ì‚¬ìš©ë²•:
   ```python
   import asyncio
   from Semantic_agent_framework import quick_run

   response = asyncio.run(quick_run("íŒŒì´ì¬ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?"))
   print(response)
   ```

3. í”„ë ˆì„ì›Œí¬ ì§ì ‘ ì‚¬ìš©:
   ```python
   import asyncio
   from Semantic_agent_framework import UnifiedAgentFramework

   async def main():
       # í”„ë ˆì„ì›Œí¬ ìƒì„± (í™˜ê²½ë³€ìˆ˜ ìë™ ë¡œë“œ)
       framework = UnifiedAgentFramework.create()

       # ë¹ ë¥¸ ì§ˆì˜ì‘ë‹µ
       response = await framework.quick_chat("ì•ˆë…•í•˜ì„¸ìš”!")
       print(response)

       # ì›Œí¬í”Œë¡œìš° ìƒì„± ë° ì‹¤í–‰
       framework.create_simple_workflow("my_bot", "ë„ˆëŠ” ì¹œì ˆí•œ AIì•¼.")
       state = await framework.run("session-1", "my_bot", "ì§ˆë¬¸ì…ë‹ˆë‹¤")

   asyncio.run(main())
   ```

4. Skills ì‹œìŠ¤í…œ ì‚¬ìš©:
   ```python
   from Semantic_agent_framework import Skill, SkillManager

   # ìŠ¤í‚¬ ìƒì„±
   coding_skill = Skill(
       name="python-expert",
       description="Python ì½”ë”© ì „ë¬¸ê°€. ì½”ë“œ ì‘ì„±, ë””ë²„ê¹…, ìµœì í™” ìš”ì²­ ì‹œ ì‚¬ìš©.",
       instructions='''
       ## ì—­í• 
       Python ì „ë¬¸ ê°œë°œìë¡œì„œ ì½”ë“œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.

       ## ê°€ì´ë“œë¼ì¸
       - PEP 8 ìŠ¤íƒ€ì¼ ê°€ì´ë“œ ì¤€ìˆ˜
       - íƒ€ì… íŒíŠ¸ ì‚¬ìš©
       - ëª…í™•í•œ docstring ì‘ì„±
       ''',
       triggers=["python", "ì½”ë”©", "í”„ë¡œê·¸ë˜ë°", "ì½”ë“œ"]
   )

   # í”„ë ˆì„ì›Œí¬ì— ìŠ¤í‚¬ ë“±ë¡
   framework.skill_manager.register_skill(coding_skill)

   # ìŠ¤í‚¬ ê¸°ë°˜ ì—ì´ì „íŠ¸ ìƒì„±
   agent = framework.create_skilled_agent("coder", skills=["python-expert"])
   ```

============================================================================
ì£¼ìš” ê¸°ëŠ¥
============================================================================
1. MCP (Model Context Protocol) ì„œë²„ í†µí•© - ì™¸ë¶€ ë„êµ¬ ì—°ë™
2. Human-in-the-loop ìŠ¹ì¸ ì‹œìŠ¤í…œ - ë¯¼ê°í•œ ì‘ì—… ìŠ¹ì¸ í•„ìš”
3. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì§€ì› - ì‹¤ì‹œê°„ í† í° ì¶œë ¥
4. ì¬ì‹œë„ ë¡œì§ ë° íšŒë¡œ ì°¨ë‹¨ê¸° íŒ¨í„´ - ì¥ì•  ê²©ë¦¬
5. ë¹„ë™ê¸° ì´ë²¤íŠ¸ ì‹œìŠ¤í…œ - Pub-Sub íŒ¨í„´
6. í–¥ìƒëœ ë©”ëª¨ë¦¬ ê´€ë¦¬ - LRU ìºì‹œ
7. Supervisor Agent íŒ¨í„´ - ë©€í‹° ì—ì´ì „íŠ¸ í˜‘ì—…
8. ì¡°ê±´ë¶€ ë¼ìš°íŒ… ë° ë£¨í”„ ì§€ì› - ë™ì  ì›Œí¬í”Œë¡œìš°
9. ë²„ì „ ê´€ë¦¬ ë° ë¡¤ë°± - ìƒíƒœ ë³µì›
10. ìƒì„¸ ë©”íŠ¸ë¦­ ë° ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
11. Anthropic Skills ì‹œìŠ¤í…œ - ëª¨ë“ˆí™”ëœ ì „ë¬¸ ì§€ì‹ ê´€ë¦¬ (NEW!)

============================================================================
í•„ìš” íŒ¨í‚¤ì§€
============================================================================
pip install semantic-kernel python-dotenv opentelemetry-api opentelemetry-sdk pydantic pyyaml
"""

import os
import sys
import asyncio
import json
import logging
import re
import glob
import fnmatch
from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Any, Callable, Set, AsyncIterator, Union
from datetime import datetime, timezone
from enum import Enum
from collections import defaultdict
from dataclasses import dataclass, field
from pathlib import Path
import time

# UTF-8 ì¸ì½”ë”© ê¸°ë³¸ ì„¤ì • (Windows í™˜ê²½ ì§€ì›)
if sys.stdout and hasattr(sys.stdout, 'reconfigure'):
    sys.stdout.reconfigure(encoding='utf-8')
if sys.stderr and hasattr(sys.stderr, 'reconfigure'):
    sys.stderr.reconfigure(encoding='utf-8')

try:
    import yaml
    YAML_AVAILABLE = True
except ImportError:
    YAML_AVAILABLE = False

from dotenv import load_dotenv
from pydantic import BaseModel, Field

# Semantic Kernel
from semantic_kernel import Kernel
from semantic_kernel.connectors.ai.chat_completion_client_base import ChatCompletionClientBase
from semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior
from semantic_kernel.connectors.ai.open_ai.prompt_execution_settings.azure_chat_prompt_execution_settings import AzureChatPromptExecutionSettings
from semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion import AzureChatCompletion
from semantic_kernel.contents.chat_history import ChatHistory
from semantic_kernel.contents.streaming_chat_message_content import StreamingChatMessageContent

# OpenTelemetry
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import ConsoleSpanExporter, BatchSpanProcessor
from opentelemetry.sdk.resources import Resource


# ============================================================================
# ğŸ¯ ì¤‘ì•™ ì„¤ì • (CENTRAL CONFIGURATION)
# ============================================================================
# ğŸš¨ ëª¨ë“  ì„¤ì •ì€ ì—¬ê¸°ì„œë§Œ ë³€ê²½í•˜ì„¸ìš”!
# ============================================================================

class Settings:
    """
    í”„ë ˆì„ì›Œí¬ ì „ì—­ ì„¤ì • - ëª¨ë“  ì„¤ì •ì„ í•œ ê³³ì—ì„œ ê´€ë¦¬

    ì‚¬ìš©ë²•:
        # ëª¨ë¸ ë³€ê²½
        Settings.DEFAULT_MODEL = "gpt-4.1"

        # ì„¤ì • í™•ì¸
        print(Settings.DEFAULT_MODEL)
    """

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # LLM ëª¨ë¸ ì„¤ì •
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    DEFAULT_MODEL: str = "gpt-5.2"           # ê¸°ë³¸ ëª¨ë¸
    DEFAULT_API_VERSION: str = "2024-08-01-preview"  # API ë²„ì „
    DEFAULT_TEMPERATURE: float = 0.7         # ê¸°ë³¸ Temperature (GPT-4 ê³„ì—´ë§Œ)
    DEFAULT_MAX_TOKENS: int = 1000           # ê¸°ë³¸ ìµœëŒ€ í† í° ìˆ˜

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ì§€ì› ëª¨ë¸ ëª©ë¡
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    SUPPORTED_MODELS: list = [
        # GPT-4 ê³„ì—´
        "gpt-4", "gpt-4o", "gpt-4o-mini", "gpt-4.1", "gpt-4.1-mini", "gpt-4.1-nano",
        # GPT-5 ê³„ì—´
        "gpt-5", "gpt-5.1", "gpt-5.2",
        # o-ì‹œë¦¬ì¦ˆ (Reasoning)
        "o1", "o1-mini", "o1-preview", "o3", "o3-mini", "o4-mini"
    ]

    # Temperature ë¯¸ì§€ì› ëª¨ë¸ (ìë™ìœ¼ë¡œ temperature íŒŒë¼ë¯¸í„° ì œì™¸)
    MODELS_WITHOUT_TEMPERATURE: list = [
        "gpt-5", "gpt-5.1", "gpt-5.2",
        "o1", "o1-mini", "o1-preview", "o3", "o3-mini", "o4-mini"
    ]

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # í”„ë ˆì„ì›Œí¬ ì„¤ì •
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    CHECKPOINT_DIR: str = "./checkpoints"    # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²½ë¡œ
    ENABLE_TELEMETRY: bool = True            # OpenTelemetry í™œì„±í™”
    ENABLE_EVENTS: bool = True               # ì´ë²¤íŠ¸ ì‹œìŠ¤í…œ í™œì„±í™”
    ENABLE_STREAMING: bool = False           # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ í™œì„±í™”
    MAX_CACHE_SIZE: int = 100                # ë©”ëª¨ë¦¬ ìºì‹œ ìµœëŒ€ í¬ê¸°

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Memory ì„¤ì • (AWS AgentCore íŒ¨í„´)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ENABLE_MEMORY_HOOKS: bool = True         # Memory Hook í™œì„±í™”
    MEMORY_NAMESPACE: str = "/conversation"  # ë©”ëª¨ë¦¬ ë„¤ì„ìŠ¤í˜ì´ìŠ¤
    MAX_MEMORY_TURNS: int = 20               # ìµœëŒ€ ëŒ€í™” í„´ ìˆ˜
    SESSION_TTL_HOURS: int = 24              # ì„¸ì…˜ ë§Œë£Œ ì‹œê°„ (ì‹œê°„)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Supervisor ì„¤ì • (SRE Agent íŒ¨í„´)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    AUTO_APPROVE_SIMPLE_PLANS: bool = True   # ê°„ë‹¨í•œ ê³„íš ìë™ ìŠ¹ì¸
    MAX_SUPERVISOR_ROUNDS: int = 5           # Supervisor ìµœëŒ€ ë¼ìš´ë“œ

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ë¡œê¹… ì„¤ì •
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    LOG_LEVEL: str = "INFO"                  # ë¡œê·¸ ë ˆë²¨
    LOG_FILE: str = "agent_framework.log"    # ë¡œê·¸ íŒŒì¼ ê²½ë¡œ


# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ì „ì—­ ë³€ìˆ˜ (Settings í´ë˜ìŠ¤ ì°¸ì¡°)
DEFAULT_LLM_MODEL = Settings.DEFAULT_MODEL
DEFAULT_API_VERSION = Settings.DEFAULT_API_VERSION
SUPPORTED_MODELS = Settings.SUPPORTED_MODELS
MODELS_WITHOUT_TEMPERATURE = Settings.MODELS_WITHOUT_TEMPERATURE


# ============================================================================
# ì„¤ì • í´ë˜ìŠ¤ (Configuration Class)
# ============================================================================

@dataclass
class FrameworkConfig:
    """
    í”„ë ˆì„ì›Œí¬ ì„¤ì • - Settings í´ë˜ìŠ¤ì˜ ê°’ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©

    ì‚¬ìš©ë²•:
        # ê¸°ë³¸ ì„¤ì • ì‚¬ìš© (Settings í´ë˜ìŠ¤ ê°’ ì ìš©)
        config = FrameworkConfig()

        # ì»¤ìŠ¤í…€ ì„¤ì •
        config = FrameworkConfig(
            model="gpt-4o",
            temperature=0.5,
            checkpoint_dir="./my_checkpoints"
        )

        # í™˜ê²½ë³€ìˆ˜ì—ì„œ ìë™ ë¡œë“œ
        config = FrameworkConfig.from_env()
    """
    # LLM ì„¤ì • - Settings í´ë˜ìŠ¤ ì°¸ì¡°
    model: str = field(default_factory=lambda: Settings.DEFAULT_MODEL)
    api_version: str = field(default_factory=lambda: Settings.DEFAULT_API_VERSION)
    temperature: float = field(default_factory=lambda: Settings.DEFAULT_TEMPERATURE)
    max_tokens: int = field(default_factory=lambda: Settings.DEFAULT_MAX_TOKENS)

    # Azure ì„¤ì • (í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
    api_key: Optional[str] = None
    endpoint: Optional[str] = None
    deployment_name: Optional[str] = None

    # í”„ë ˆì„ì›Œí¬ ì„¤ì • - Settings í´ë˜ìŠ¤ ì°¸ì¡°
    checkpoint_dir: str = field(default_factory=lambda: Settings.CHECKPOINT_DIR)
    enable_telemetry: bool = field(default_factory=lambda: Settings.ENABLE_TELEMETRY)
    enable_events: bool = field(default_factory=lambda: Settings.ENABLE_EVENTS)
    enable_streaming: bool = field(default_factory=lambda: Settings.ENABLE_STREAMING)
    max_cache_size: int = field(default_factory=lambda: Settings.MAX_CACHE_SIZE)

    # Memory ì„¤ì • - Settings í´ë˜ìŠ¤ ì°¸ì¡°
    enable_memory_hooks: bool = field(default_factory=lambda: Settings.ENABLE_MEMORY_HOOKS)
    memory_namespace: str = field(default_factory=lambda: Settings.MEMORY_NAMESPACE)
    max_memory_turns: int = field(default_factory=lambda: Settings.MAX_MEMORY_TURNS)
    session_ttl_hours: int = field(default_factory=lambda: Settings.SESSION_TTL_HOURS)

    # Supervisor ì„¤ì • - Settings í´ë˜ìŠ¤ ì°¸ì¡°
    auto_approve_simple_plans: bool = field(default_factory=lambda: Settings.AUTO_APPROVE_SIMPLE_PLANS)
    max_supervisor_rounds: int = field(default_factory=lambda: Settings.MAX_SUPERVISOR_ROUNDS)

    # ë¡œê¹… ì„¤ì • - Settings í´ë˜ìŠ¤ ì°¸ì¡°
    log_level: str = field(default_factory=lambda: Settings.LOG_LEVEL)
    log_file: Optional[str] = field(default_factory=lambda: Settings.LOG_FILE)

    @classmethod
    def from_env(cls, dotenv_path: Optional[str] = None) -> 'FrameworkConfig':
        """
        í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„¤ì • ë¡œë“œ

        ì§€ì›í•˜ëŠ” í™˜ê²½ë³€ìˆ˜ (ìš°ì„ ìˆœìœ„ ìˆœì„œ):
        - API Key: AZURE_OPENAI_API_KEY
        - Endpoint: AZURE_OPENAI_ENDPOINT
        - Deployment: AZURE_OPENAI_DEPLOYMENT
        - API Version: AZURE_OPENAI_API_VERSION (ê¸°ë³¸: 2024-08-01-preview)
        """
        load_dotenv(dotenv_path)

        # API Key (AZURE_OPENAI_API_KEY ìš°ì„ )
        api_key = (
            os.getenv("AZURE_OPENAI_API_KEY")
        )

        # Endpoint (AZURE_OPENAI_ENDPOINT ìš°ì„ )
        endpoint = (
            os.getenv("AZURE_OPENAI_ENDPOINT")
        )

        # Deployment Name (AZURE_OPENAI_DEPLOYMENT ìš°ì„ ) - ê°’ì—ì„œ ë”°ì˜´í‘œ/ê³µë°± ì œê±°
        deployment_name = (
            os.getenv("AZURE_OPENAI_DEPLOYMENT")
        )

        # í™˜ê²½ë³€ìˆ˜ ê°’ì—ì„œ ë”°ì˜´í‘œì™€ ê³µë°± ì œê±° (Windows .env íŒŒì¼ ë¬¸ì œ í•´ê²°)
        if api_key:
            api_key = api_key.strip().strip('"').strip("'").strip()
        if endpoint:
            endpoint = endpoint.strip().strip('"').strip("'").strip()
        if deployment_name:
            deployment_name = deployment_name.strip().strip('"').strip("'").strip()

        return cls(
            api_key=api_key,
            endpoint=endpoint,
            deployment_name=deployment_name,
            model=os.getenv("AZURE_OPENAI_MODEL", Settings.DEFAULT_MODEL),
            api_version=os.getenv("AZURE_OPENAI_API_VERSION", Settings.DEFAULT_API_VERSION),
        )

    def validate(self) -> bool:
        """ì„¤ì • ìœ íš¨ì„± ê²€ì¦"""
        missing = []
        if not self.api_key:
            missing.append("api_key (AZURE_OPENAI_API_KEY)")
        if not self.endpoint:
            missing.append("endpoint (AZURE_OPENAI_ENDPOINT)")
        if not self.deployment_name:
            missing.append("deployment_name (AZURE_OPENAI_DEPLOYMENT)")

        if missing:
            raise ValueError(
                f"âŒ í•„ìˆ˜ ì„¤ì •ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤:\n" +
                "\n".join(f"  - {m}" for m in missing) +
                "\n\nğŸ’¡ .env íŒŒì¼ì„ ìƒì„±í•˜ê±°ë‚˜ í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”."
            )
        return True


def supports_temperature(model: str) -> bool:
    """
    ëª¨ë¸ì´ temperature íŒŒë¼ë¯¸í„°ë¥¼ ì§€ì›í•˜ëŠ”ì§€ í™•ì¸

    Args:
        model: ëª¨ë¸ ì´ë¦„ (ì˜ˆ: 'gpt-4.1', 'gpt-5', 'o1')

    Returns:
        bool: temperature ì§€ì› ì—¬ë¶€

    Note:
        GPT-5, o1, o3 ê³„ì—´ ëª¨ë¸ì€ temperatureë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    """
    model_lower = model.lower()
    for unsupported in MODELS_WITHOUT_TEMPERATURE:
        if unsupported in model_lower:
            return False
    return True


def create_execution_settings(
    model: str,
    temperature: float = 0.7,
    max_tokens: int = 1000,
    service_id: Optional[str] = None,
    **kwargs
) -> AzureChatPromptExecutionSettings:
    """
    ëª¨ë¸ì— ë”°ë¼ ì ì ˆí•œ ì‹¤í–‰ ì„¤ì • ìƒì„±

    Args:
        model: ëª¨ë¸ ì´ë¦„
        temperature: ì˜¨ë„ ì„¤ì • (ì§€ì›í•˜ëŠ” ëª¨ë¸ì—ë§Œ ì ìš©)
        max_tokens: ìµœëŒ€ í† í° ìˆ˜
        service_id: ì„œë¹„ìŠ¤ ID (ì—†ìœ¼ë©´ model ì‚¬ìš©)
        **kwargs: ì¶”ê°€ ì„¤ì •

    Returns:
        AzureChatPromptExecutionSettings ì¸ìŠ¤í„´ìŠ¤
    """
    settings_kwargs = {
        "max_tokens": max_tokens,
        "service_id": service_id or model,
        **kwargs
    }

    # Temperature ì§€ì› ëª¨ë¸ì—ë§Œ temperature ì¶”ê°€
    if supports_temperature(model):
        settings_kwargs["temperature"] = temperature
    else:
        logging.info(f"â„¹ï¸ ëª¨ë¸ '{model}'ì€(ëŠ”) temperatureë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í•´ë‹¹ íŒŒë¼ë¯¸í„°ë¥¼ ìƒëµí•©ë‹ˆë‹¤.")

    return AzureChatPromptExecutionSettings(**settings_kwargs)


# ============================================================================
# Skills ì‹œìŠ¤í…œ (Anthropic Skills íŒ¨í„´)
# ============================================================================

@dataclass
class SkillResource:
    """
    ìŠ¤í‚¬ ë²ˆë“¤ ë¦¬ì†ŒìŠ¤

    ìŠ¤í‚¬ì— í¬í•¨ë˜ëŠ” ì¶”ê°€ ë¦¬ì†ŒìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤:
    - scripts/: ì‹¤í–‰ ê°€ëŠ¥í•œ ìŠ¤í¬ë¦½íŠ¸ (Python, Bash ë“±)
    - references/: ì°¸ì¡° ë¬¸ì„œ (ë§ˆí¬ë‹¤ìš´, í…ìŠ¤íŠ¸ ë“±)
    - assets/: í…œí”Œë¦¿, ì´ë¯¸ì§€ ë“± ì¶œë ¥ìš© íŒŒì¼
    """
    resource_type: str  # 'script', 'reference', 'asset'
    name: str
    path: str
    content: Optional[str] = None
    description: Optional[str] = None


@dataclass
class Skill:
    """
    Anthropic Skills íŒ¨í„´ êµ¬í˜„

    SkillsëŠ” Claudeì˜ ëŠ¥ë ¥ì„ í™•ì¥í•˜ëŠ” ëª¨ë“ˆí™”ëœ íŒ¨í‚¤ì§€ì…ë‹ˆë‹¤.
    íŠ¹ì • ë„ë©”ì¸ì˜ ì§€ì‹, ì›Œí¬í”Œë¡œìš°, ë„êµ¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

    êµ¬ì¡°:
    ```
    skill-name/
    â”œâ”€â”€ SKILL.md (í•„ìˆ˜)
    â”‚   â”œâ”€â”€ YAML frontmatter (name, description)
    â”‚   â””â”€â”€ Markdown ì§€ì¹¨
    â””â”€â”€ Bundled Resources (ì„ íƒ)
        â”œâ”€â”€ scripts/      - ì‹¤í–‰ ì½”ë“œ
        â”œâ”€â”€ references/   - ì°¸ì¡° ë¬¸ì„œ
        â””â”€â”€ assets/       - í…œí”Œë¦¿, ì•„ì´ì½˜ ë“±
    ```

    ì‚¬ìš©ë²•:
    ```python
    # ì§ì ‘ ìƒì„±
    skill = Skill(
        name="python-expert",
        description="Python ì½”ë”© ì „ë¬¸ê°€",
        instructions="## ì—­í• \\níŒŒì´ì¬ ì „ë¬¸ê°€ë¡œì„œ...",
        triggers=["python", "ì½”ë”©"]
    )

    # íŒŒì¼ì—ì„œ ë¡œë“œ
    skill = Skill.from_file("skills/python-expert/SKILL.md")

    # ë””ë ‰í† ë¦¬ì—ì„œ ë¡œë“œ (ë¦¬ì†ŒìŠ¤ í¬í•¨)
    skill = Skill.from_directory("skills/python-expert/")
    ```
    """
    name: str
    description: str
    instructions: str = ""
    triggers: List[str] = field(default_factory=list)
    resources: List[SkillResource] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)
    enabled: bool = True
    priority: int = 0  # ë†’ì„ìˆ˜ë¡ ìš°ì„ ìˆœìœ„ ë†’ìŒ

    # Progressive Disclosure ê´€ë ¨
    always_loaded: bool = False  # Trueë©´ í•­ìƒ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨
    max_context_lines: int = 500  # SKILL.md ìµœëŒ€ ë¼ì¸ ìˆ˜

    @classmethod
    def from_file(cls, filepath: str) -> 'Skill':
        """
        SKILL.md íŒŒì¼ì—ì„œ ìŠ¤í‚¬ ë¡œë“œ

        íŒŒì¼ í˜•ì‹:
        ```markdown
        ---
        name: skill-name
        description: ìŠ¤í‚¬ ì„¤ëª…
        ---

        # ìŠ¤í‚¬ ì œëª©

        ## ì§€ì¹¨
        ...
        ```
        """
        path = Path(filepath)
        if not path.exists():
            raise FileNotFoundError(f"ìŠ¤í‚¬ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filepath}")

        content = path.read_text(encoding='utf-8')
        return cls._parse_skill_content(content, filepath)

    @classmethod
    def from_directory(cls, dirpath: str) -> 'Skill':
        """
        ìŠ¤í‚¬ ë””ë ‰í† ë¦¬ì—ì„œ ìŠ¤í‚¬ ë¡œë“œ (ë¦¬ì†ŒìŠ¤ í¬í•¨)

        ë””ë ‰í† ë¦¬ êµ¬ì¡°:
        ```
        skill-name/
        â”œâ”€â”€ SKILL.md
        â”œâ”€â”€ scripts/
        â”‚   â””â”€â”€ example.py
        â”œâ”€â”€ references/
        â”‚   â””â”€â”€ api_reference.md
        â””â”€â”€ assets/
            â””â”€â”€ template.txt
        ```
        """
        dirpath = Path(dirpath)
        skill_file = dirpath / "SKILL.md"

        if not skill_file.exists():
            raise FileNotFoundError(f"SKILL.mdë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {skill_file}")

        # ê¸°ë³¸ ìŠ¤í‚¬ ë¡œë“œ
        skill = cls.from_file(str(skill_file))

        # ë¦¬ì†ŒìŠ¤ ë¡œë“œ
        skill._load_resources(dirpath)

        return skill

    @classmethod
    def _parse_skill_content(cls, content: str, source: str = "") -> 'Skill':
        """SKILL.md ë‚´ìš© íŒŒì‹±"""
        # YAML frontmatter ì¶”ì¶œ
        frontmatter = {}
        body = content

        if content.startswith('---'):
            parts = content.split('---', 2)
            if len(parts) >= 3:
                if YAML_AVAILABLE:
                    try:
                        frontmatter = yaml.safe_load(parts[1]) or {}
                    except yaml.YAMLError:
                        frontmatter = cls._parse_simple_yaml(parts[1])
                else:
                    frontmatter = cls._parse_simple_yaml(parts[1])
                body = parts[2].strip()

        name = frontmatter.get('name', Path(source).stem if source else 'unnamed-skill')
        description = frontmatter.get('description', '')

        # triggers ì¶”ì¶œ (descriptionì—ì„œ ìë™ ì¶”ì¶œ ë˜ëŠ” ëª…ì‹œì  ì§€ì •)
        triggers = frontmatter.get('triggers', [])
        if not triggers and description:
            # descriptionì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
            triggers = cls._extract_triggers(description)

        # priority ì¶”ì¶œ (ê¸°ë³¸ê°’: 0)
        priority = frontmatter.get('priority', 0)
        if isinstance(priority, str):
            try:
                priority = int(priority)
            except ValueError:
                priority = 0

        return cls(
            name=name,
            description=description,
            instructions=body,
            triggers=triggers,
            priority=priority,  # ğŸ†• priority ë°˜ì˜
            metadata={
                'source': source,
                'license': frontmatter.get('license', ''),
                **{k: v for k, v in frontmatter.items() if k not in ['name', 'description', 'triggers', 'license', 'priority']}
            }
        )

    @staticmethod
    def _parse_simple_yaml(text: str) -> Dict[str, Any]:
        """ê°„ë‹¨í•œ YAML íŒŒì‹± (yaml ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ì„ ë•Œ)"""
        result = {}
        for line in text.strip().split('\n'):
            if ':' in line:
                key, value = line.split(':', 1)
                key = key.strip()
                value = value.strip()
                # ë”°ì˜´í‘œ ì œê±°
                if value.startswith('"') and value.endswith('"'):
                    value = value[1:-1]
                elif value.startswith("'") and value.endswith("'"):
                    value = value[1:-1]
                result[key] = value
        return result

    @staticmethod
    def _extract_triggers(description: str) -> List[str]:
        """ì„¤ëª…ì—ì„œ íŠ¸ë¦¬ê±° í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ì£¼ìš” í‚¤ì›Œë“œ íŒ¨í„´
        keywords = []
        # ê´„í˜¸ ì•ˆì˜ ë‚´ìš© ì¶”ì¶œ
        parens = re.findall(r'\(([^)]+)\)', description)
        for paren in parens:
            keywords.extend([k.strip() for k in paren.split(',')])

        # ì£¼ìš” ë‹¨ì–´ ì¶”ì¶œ (ì˜ë¬¸ì€ ì†Œë¬¸ìë¡œ)
        words = re.findall(r'\b[A-Za-zê°€-í£]{3,}\b', description)
        stop_words = {'the', 'and', 'for', 'use', 'when', 'with', 'this', 'that', 'from', 'have', 'are'}
        keywords.extend([w.lower() for w in words if w.lower() not in stop_words][:5])

        return list(set(keywords))[:10]

    def _load_resources(self, dirpath: Path):
        """ë””ë ‰í† ë¦¬ì—ì„œ ë¦¬ì†ŒìŠ¤ ë¡œë“œ"""
        # Scripts
        scripts_dir = dirpath / "scripts"
        if scripts_dir.exists():
            for script_file in scripts_dir.glob("*"):
                if script_file.is_file():
                    self.resources.append(SkillResource(
                        resource_type="script",
                        name=script_file.name,
                        path=str(script_file),
                        description=f"Script: {script_file.name}"
                    ))

        # References
        refs_dir = dirpath / "references"
        if refs_dir.exists():
            for ref_file in refs_dir.glob("*"):
                if ref_file.is_file():
                    self.resources.append(SkillResource(
                        resource_type="reference",
                        name=ref_file.name,
                        path=str(ref_file),
                        description=f"Reference: {ref_file.name}"
                    ))

        # Assets
        assets_dir = dirpath / "assets"
        if assets_dir.exists():
            for asset_file in assets_dir.glob("*"):
                if asset_file.is_file() or asset_file.is_dir():
                    self.resources.append(SkillResource(
                        resource_type="asset",
                        name=asset_file.name,
                        path=str(asset_file),
                        description=f"Asset: {asset_file.name}"
                    ))

    def get_resource(self, name: str) -> Optional[SkillResource]:
        """ì´ë¦„ìœ¼ë¡œ ë¦¬ì†ŒìŠ¤ ì°¾ê¸°"""
        for resource in self.resources:
            if resource.name == name:
                return resource
        return None

    def load_resource_content(self, resource: SkillResource) -> str:
        """ë¦¬ì†ŒìŠ¤ ë‚´ìš© ë¡œë“œ"""
        if resource.content:
            return resource.content

        path = Path(resource.path)
        if path.exists() and path.is_file():
            try:
                resource.content = path.read_text(encoding='utf-8')
                return resource.content
            except Exception as e:
                logging.warning(f"ë¦¬ì†ŒìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {resource.path} - {e}")
        return ""

    def matches(self, query: str) -> float:
        """
        ì¿¼ë¦¬ì™€ì˜ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚° (0.0 ~ 1.0)

        Progressive Disclosure: ì¿¼ë¦¬ì— ë”°ë¼ ìŠ¤í‚¬ í™œì„±í™” ì—¬ë¶€ ê²°ì •
        """
        query_lower = query.lower()
        score = 0.0

        # ì´ë¦„ ë§¤ì¹­ (ë†’ì€ ê°€ì¤‘ì¹˜)
        if self.name.lower() in query_lower:
            score += 0.5

        # íŠ¸ë¦¬ê±° ë§¤ì¹­
        for trigger in self.triggers:
            if trigger.lower() in query_lower:
                score += 0.3
                break

        # ì„¤ëª… ë§¤ì¹­
        desc_words = self.description.lower().split()
        query_words = query_lower.split()
        common_words = set(desc_words) & set(query_words)
        if common_words:
            score += min(len(common_words) * 0.1, 0.2)

        return min(score, 1.0)

    def get_prompt_section(self, include_full: bool = False) -> str:
        """
        í”„ë¡¬í”„íŠ¸ì— í¬í•¨í•  ìŠ¤í‚¬ ì„¹ì…˜ ìƒì„±

        Progressive Disclosure ì ìš©:
        - include_full=False: ë©”íƒ€ë°ì´í„°ë§Œ (name + description)
        - include_full=True: ì „ì²´ ì§€ì¹¨ í¬í•¨
        """
        if include_full:
            return f"""
## Skill: {self.name}

**Description:** {self.description}

{self.instructions}

---
"""
        else:
            return f"- **{self.name}**: {self.description}\n"

    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            "name": self.name,
            "description": self.description,
            "instructions": self.instructions,
            "triggers": self.triggers,
            "resources": [
                {"type": r.resource_type, "name": r.name, "path": r.path}
                for r in self.resources
            ],
            "metadata": self.metadata,
            "enabled": self.enabled,
            "priority": self.priority
        }


class SkillManager:
    """
    ìŠ¤í‚¬ ê´€ë¦¬ì - ìŠ¤í‚¬ ë“±ë¡, ê²€ìƒ‰, í™œì„±í™” ê´€ë¦¬

    ì£¼ìš” ê¸°ëŠ¥:
    - ìŠ¤í‚¬ ë“±ë¡ ë° í•´ì œ
    - ì¿¼ë¦¬ ê¸°ë°˜ ìŠ¤í‚¬ ë§¤ì¹­ (Progressive Disclosure)
    - ë””ë ‰í† ë¦¬ì—ì„œ ìŠ¤í‚¬ ì¼ê´„ ë¡œë“œ
    - ìŠ¤í‚¬ ìš°ì„ ìˆœìœ„ ê´€ë¦¬

    ì‚¬ìš©ë²•:
    ```python
    manager = SkillManager()

    # ìŠ¤í‚¬ ë“±ë¡
    manager.register_skill(my_skill)

    # ë””ë ‰í† ë¦¬ì—ì„œ ë¡œë“œ
    manager.load_skills_from_directory("./skills")

    # ì¿¼ë¦¬ì— ë§ëŠ” ìŠ¤í‚¬ ì°¾ê¸°
    matched_skills = manager.match_skills("Python ì½”ë“œ ì‘ì„±í•´ì¤˜")

    # í™œì„±í™”ëœ ìŠ¤í‚¬ë¡œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = manager.build_system_prompt(matched_skills)
    ```
    """

    def __init__(self, skill_dirs: Optional[List[str]] = None):
        self.skills: Dict[str, Skill] = {}
        self.skill_history: List[Dict[str, Any]] = []  # ìŠ¤í‚¬ ì‚¬ìš© ê¸°ë¡

        # ê¸°ë³¸ ìŠ¤í‚¬ ë””ë ‰í† ë¦¬ì—ì„œ ë¡œë“œ
        if skill_dirs:
            for skill_dir in skill_dirs:
                self.load_skills_from_directory(skill_dir)

    def register_skill(self, skill: Skill) -> bool:
        """ìŠ¤í‚¬ ë“±ë¡"""
        if skill.name in self.skills:
            logging.warning(f"ìŠ¤í‚¬ '{skill.name}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ë®ì–´ì”ë‹ˆë‹¤.")

        self.skills[skill.name] = skill
        logging.info(f"âœ… ìŠ¤í‚¬ ë“±ë¡: {skill.name}")
        return True

    def unregister_skill(self, name: str) -> bool:
        """ìŠ¤í‚¬ í•´ì œ"""
        if name in self.skills:
            del self.skills[name]
            logging.info(f"ğŸ—‘ï¸ ìŠ¤í‚¬ í•´ì œ: {name}")
            return True
        return False

    def get_skill(self, name: str) -> Optional[Skill]:
        """ì´ë¦„ìœ¼ë¡œ ìŠ¤í‚¬ ê°€ì ¸ì˜¤ê¸°"""
        return self.skills.get(name)

    def list_skills(self, enabled_only: bool = True) -> List[Skill]:
        """ë“±ë¡ëœ ìŠ¤í‚¬ ëª©ë¡"""
        skills = list(self.skills.values())
        if enabled_only:
            skills = [s for s in skills if s.enabled]
        return sorted(skills, key=lambda s: -s.priority)

    def load_skills_from_directory(self, dirpath: str) -> int:
        """
        ë””ë ‰í† ë¦¬ì—ì„œ ìŠ¤í‚¬ ì¼ê´„ ë¡œë“œ

        ë””ë ‰í† ë¦¬ êµ¬ì¡°:
        ```
        skills/
        â”œâ”€â”€ python-expert/
        â”‚   â””â”€â”€ SKILL.md
        â”œâ”€â”€ data-analyst/
        â”‚   â”œâ”€â”€ SKILL.md
        â”‚   â””â”€â”€ scripts/
        â””â”€â”€ ...
        ```
        """
        dirpath = Path(dirpath)
        if not dirpath.exists():
            logging.warning(f"ìŠ¤í‚¬ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {dirpath}")
            return 0

        loaded = 0
        for skill_dir in dirpath.iterdir():
            if skill_dir.is_dir():
                skill_file = skill_dir / "SKILL.md"
                if skill_file.exists():
                    try:
                        skill = Skill.from_directory(str(skill_dir))
                        self.register_skill(skill)
                        loaded += 1
                    except Exception as e:
                        logging.error(f"ìŠ¤í‚¬ ë¡œë“œ ì‹¤íŒ¨: {skill_dir} - {e}")

        logging.info(f"ğŸ“¦ {loaded}ê°œ ìŠ¤í‚¬ ë¡œë“œ ì™„ë£Œ from {dirpath}")
        return loaded

    def match_skills(
        self,
        query: str,
        threshold: float = 0.2,
        max_skills: int = 3
    ) -> List[Skill]:
        """
        ì¿¼ë¦¬ì— ë§¤ì¹­ë˜ëŠ” ìŠ¤í‚¬ ì°¾ê¸°

        Progressive Disclosure êµ¬í˜„:
        - threshold ì´ìƒì˜ ë§¤ì¹­ ì ìˆ˜ë¥¼ ê°€ì§„ ìŠ¤í‚¬ë§Œ ë°˜í™˜
        - max_skills ê°œìˆ˜ ì œí•œ
        - always_loaded ìŠ¤í‚¬ì€ í•­ìƒ í¬í•¨
        """
        matched = []

        for skill in self.list_skills():
            if skill.always_loaded:
                matched.append((skill, 1.0))
                continue

            score = skill.matches(query)
            if score >= threshold:
                matched.append((skill, score))

        # ì ìˆ˜ ë° ìš°ì„ ìˆœìœ„ë¡œ ì •ë ¬
        matched.sort(key=lambda x: (-x[1], -x[0].priority))

        result = [skill for skill, _ in matched[:max_skills]]

        # ì‚¬ìš© ê¸°ë¡
        self.skill_history.append({
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "query": query,
            "matched": [s.name for s in result]
        })

        return result

    def build_system_prompt(
        self,
        skills: List[Skill],
        base_prompt: str = "",
        include_full: bool = True
    ) -> str:
        """
        ìŠ¤í‚¬ì„ í¬í•¨í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±

        Progressive Disclosure:
        - ë§¤ì¹­ëœ ìŠ¤í‚¬ë§Œ ì „ì²´ ì§€ì¹¨ í¬í•¨
        - ë‹¤ë¥¸ ìŠ¤í‚¬ì€ ë©”íƒ€ë°ì´í„°ë§Œ í¬í•¨ (ì„ íƒì )
        """
        prompt_parts = []

        if base_prompt:
            prompt_parts.append(base_prompt)

        if skills:
            prompt_parts.append("\n# Active Skills\n")
            for skill in skills:
                prompt_parts.append(skill.get_prompt_section(include_full=include_full))

        # ì‚¬ìš© ê°€ëŠ¥í•œ ë‹¤ë¥¸ ìŠ¤í‚¬ ëª©ë¡ (Progressive Disclosure)
        other_skills = [s for s in self.list_skills() if s not in skills]
        if other_skills:
            prompt_parts.append("\n# Available Skills (activate by mentioning)\n")
            for skill in other_skills[:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                prompt_parts.append(skill.get_prompt_section(include_full=False))

        return "\n".join(prompt_parts)

    def get_usage_stats(self) -> Dict[str, Any]:
        """ìŠ¤í‚¬ ì‚¬ìš© í†µê³„"""
        stats = defaultdict(int)
        for record in self.skill_history:
            for skill_name in record.get("matched", []):
                stats[skill_name] += 1

        return {
            "total_queries": len(self.skill_history),
            "skill_usage": dict(stats),
            "registered_skills": len(self.skills),
            "enabled_skills": len([s for s in self.skills.values() if s.enabled])
        }

    def create_skill_template(self, name: str, output_dir: str) -> str:
        """
        ìƒˆ ìŠ¤í‚¬ í…œí”Œë¦¿ ìƒì„±

        init_skill.py ìŠ¤í¬ë¦½íŠ¸ì™€ ìœ ì‚¬í•œ ê¸°ëŠ¥
        """
        output_path = Path(output_dir) / name
        output_path.mkdir(parents=True, exist_ok=True)

        # SKILL.md í…œí”Œë¦¿
        skill_md = f"""---
name: {name}
description: [TODO: ì´ ìŠ¤í‚¬ì´ ë¬´ì—‡ì„ í•˜ëŠ”ì§€, ì–¸ì œ ì‚¬ìš©í•´ì•¼ í•˜ëŠ”ì§€ ì„¤ëª…í•˜ì„¸ìš”]
---

# {name.replace('-', ' ').title()}

## Overview

[TODO: 1-2ë¬¸ì¥ìœ¼ë¡œ ì´ ìŠ¤í‚¬ì´ ë¬´ì—‡ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ”ì§€ ì„¤ëª…]

## When to Use

ì´ ìŠ¤í‚¬ì€ ë‹¤ìŒê³¼ ê°™ì€ ê²½ìš°ì— ì‚¬ìš©í•©ë‹ˆë‹¤:
- [TODO: ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ 1]
- [TODO: ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ 2]

## Instructions

[TODO: AIê°€ ë”°ë¼ì•¼ í•  ì§€ì¹¨ì„ ì‘ì„±í•˜ì„¸ìš”]

## Examples

### Example 1
[TODO: ì˜ˆì‹œ ì¶”ê°€]

## Resources

- scripts/: ì‹¤í–‰ ê°€ëŠ¥í•œ ìŠ¤í¬ë¦½íŠ¸
- references/: ì°¸ì¡° ë¬¸ì„œ
- assets/: í…œí”Œë¦¿ ë° ì—ì…‹
"""

        (output_path / "SKILL.md").write_text(skill_md, encoding='utf-8')

        # ë¦¬ì†ŒìŠ¤ ë””ë ‰í† ë¦¬ ìƒì„±
        (output_path / "scripts").mkdir(exist_ok=True)
        (output_path / "references").mkdir(exist_ok=True)
        (output_path / "assets").mkdir(exist_ok=True)

        # ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸
        example_script = f'''#!/usr/bin/env python3
"""
Example script for {name}
"""

def main():
    print("Hello from {name}!")

if __name__ == "__main__":
    main()
'''
        (output_path / "scripts" / "example.py").write_text(example_script, encoding='utf-8')

        logging.info(f"âœ… ìŠ¤í‚¬ í…œí”Œë¦¿ ìƒì„±: {output_path}")
        return str(output_path)


# ê¸°ë³¸ ìŠ¤í‚¬ ë””ë ‰í† ë¦¬ ê²½ë¡œ (íŒŒì¼ ê¸°ë°˜ ë¡œë“œ)
BUILTIN_SKILLS_DIR = Path(__file__).parent / "skills"


# ============================================================================
# ìœ í‹¸ë¦¬í‹° & ì¸í”„ë¼ (New)
# ============================================================================

class StructuredLogger:
    """
    JSON í˜•íƒœì˜ êµ¬ì¡°í™”ëœ ë¡œê¹…
    """
    def __init__(self, name: str):
        self.logger = logging.getLogger(name)

    def info(self, message: str, **kwargs):
        self._log(logging.INFO, message, **kwargs)

    def error(self, message: str, **kwargs):
        self._log(logging.ERROR, message, **kwargs)

    def warning(self, message: str, **kwargs):
        self._log(logging.WARNING, message, **kwargs)

    def _log(self, level: int, message: str, **kwargs):
        log_data = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "message": message,
            **kwargs
        }
        # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” json.dumps ì‚¬ìš©, ì—¬ê¸°ì„œëŠ” ê°€ë…ì„±ì„ ìœ„í•´ í¬ë§·íŒ…
        self.logger.log(level, f"[{level}] {json.dumps(log_data, ensure_ascii=False)}")

async def retry_with_backoff(
    func: Callable,
    max_retries: int = 3,
    base_delay: float = 1.0,
    max_delay: float = 60.0,
    exponential_base: float = 2.0,
    *args,
    **kwargs
) -> Any:
    """
    ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„ ë¡œì§
    """
    retries = 0
    while True:
        try:
            return await func(*args, **kwargs)
        except Exception as e:
            retries += 1
            if retries > max_retries:
                raise e

            delay = min(base_delay * (exponential_base ** (retries - 1)), max_delay)
            logging.warning(f"âš ï¸ ì¬ì‹œë„ {retries}/{max_retries} ({delay:.2f}s í›„): {e}")
            await asyncio.sleep(delay)



# ============================================================================
# í•µì‹¬ ë°ì´í„° ëª¨ë¸
# ============================================================================

class AgentRole(str, Enum):
    """
    Agent ì—­í•  ì •ì˜

    [ìˆ˜ì •] SUPERVISOR ì¶”ê°€ - Microsoft AutoGen íŒ¨í„´
    ê¸°ì¡´: ASSISTANT, USER, SYSTEM, FUNCTION, ROUTER, ORCHESTRATOR
    ì¶”ê°€: SUPERVISOR - ì—¬ëŸ¬ ì—ì´ì „íŠ¸ë¥¼ ê°ë…í•˜ê³  ì¡°ìœ¨í•˜ëŠ” ì—­í• 
    """
    ASSISTANT = "assistant"
    USER = "user"
    SYSTEM = "system"
    FUNCTION = "function"
    ROUTER = "router"
    ORCHESTRATOR = "orchestrator"
    SUPERVISOR = "supervisor"  # ğŸ†• ì¶”ê°€


class ExecutionStatus(str, Enum):
    """
    ì‹¤í–‰ ìƒíƒœ ì •ì˜

    [ìˆ˜ì •] ìŠ¹ì¸ ê´€ë ¨ ìƒíƒœ ì¶”ê°€ - Human-in-the-loop íŒ¨í„´
    ê¸°ì¡´: PENDING, RUNNING, COMPLETED, FAILED, PAUSED, WAITING_APPROVAL
    ì¶”ê°€: APPROVED, REJECTED
    """
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    PAUSED = "paused"
    WAITING_APPROVAL = "waiting_approval"
    APPROVED = "approved"    # ğŸ†• ì¶”ê°€
    REJECTED = "rejected"    # ğŸ†• ì¶”ê°€


class ApprovalStatus(str, Enum):
    """
    ìŠ¹ì¸ ìƒíƒœ ì •ì˜

    [ì‹ ê·œ] Microsoft Agent Frameworkì˜ approval íŒ¨í„´
    - PENDING: ìŠ¹ì¸ ëŒ€ê¸° ì¤‘
    - APPROVED: ì‚¬ìš©ìê°€ ìŠ¹ì¸í•¨
    - REJECTED: ì‚¬ìš©ìê°€ ê±°ë¶€í•¨
    - AUTO_APPROVED: ìë™ ìŠ¹ì¸ë¨ (ì•ˆì „í•œ ì‘ì—…)
    """
    PENDING = "pending"
    APPROVED = "approved"
    REJECTED = "rejected"
    AUTO_APPROVED = "auto_approved"  # ğŸ†• ìë™ ìŠ¹ì¸


class Message(BaseModel):
    """
    ë©”ì‹œì§€ ëª¨ë¸

    [ìˆ˜ì •] function_call í•„ë“œ ì¶”ê°€
    - í•¨ìˆ˜ í˜¸ì¶œ ì •ë³´ë¥¼ ì €ì¥í•˜ì—¬ OpenAI Function Calling ì§€ì›
    """
    role: AgentRole
    content: str
    metadata: Dict[str, Any] = Field(default_factory=dict)
    timestamp: str = Field(default_factory=lambda: datetime.now(timezone.utc).isoformat())
    agent_name: Optional[str] = None
    function_call: Optional[Dict[str, Any]] = None  # ğŸ†• í•¨ìˆ˜ í˜¸ì¶œ ì •ë³´

    class Config:
        use_enum_values = True


class AgentState(BaseModel):
    """
    Agent ìƒíƒœ - ì²´í¬í¬ì¸íŒ… ë° ë³µì› ì§€ì›

    [ìˆ˜ì •] pending_approvals, metrics í•„ë“œ ì¶”ê°€
    - pending_approvals: ìŠ¹ì¸ ëŒ€ê¸° ì¤‘ì¸ ìš”ì²­ ëª©ë¡
    - metrics: ì‹¤í–‰ ë©”íŠ¸ë¦­ (ì‹œê°„, í† í° ë“±)
    """
    messages: List[Message] = Field(default_factory=list)
    current_node: str = "start"
    visited_nodes: List[str] = Field(default_factory=list)
    metadata: Dict[str, Any] = Field(default_factory=dict)
    session_id: str
    workflow_name: str = "default"
    execution_status: ExecutionStatus = ExecutionStatus.PENDING
    pending_approvals: List[Dict[str, Any]] = Field(default_factory=list)  # ğŸ†• ìŠ¹ì¸ ëŒ€ê¸°
    metrics: Dict[str, Any] = Field(default_factory=dict)  # ğŸ†• ë©”íŠ¸ë¦­

    def add_message(self, role: AgentRole, content: str, agent_name: Optional[str] = None,
                   function_call: Optional[Dict[str, Any]] = None):
        """ë©”ì‹œì§€ ì¶”ê°€"""
        self.messages.append(Message(
            role=role,
            content=content,
            agent_name=agent_name,
            function_call=function_call
        ))

    def get_conversation_history(self, max_messages: int = 10) -> List[Message]:
        """ìµœê·¼ ëŒ€í™” ê¸°ë¡"""
        return self.messages[-max_messages:]

    def add_pending_approval(self, approval_request: Dict[str, Any]):
        """
        ìŠ¹ì¸ ëŒ€ê¸° ìš”ì²­ ì¶”ê°€

        [ì‹ ê·œ] Human-in-the-loop íŒ¨í„´ ì§€ì›
        """
        self.pending_approvals.append(approval_request)
        self.execution_status = ExecutionStatus.WAITING_APPROVAL


class NodeResult(BaseModel):
    """
    ë…¸ë“œ ì‹¤í–‰ ê²°ê³¼

    [ìˆ˜ì •] requires_approval, approval_data í•„ë“œ ì¶”ê°€
    - ìŠ¹ì¸ì´ í•„ìš”í•œ ì‘ì—…ì¸ì§€ í‘œì‹œ
    """
    node_name: str
    output: str
    next_node: Optional[str] = None
    metadata: Dict[str, Any] = Field(default_factory=dict)
    success: bool = True
    error: Optional[str] = None
    tokens_used: int = 0
    duration_ms: float = 0.0
    requires_approval: bool = False  # ğŸ†• ìŠ¹ì¸ í•„ìš” ì—¬ë¶€
    approval_data: Optional[Dict[str, Any]] = None  # ğŸ†• ìŠ¹ì¸ ë°ì´í„°


# ============================================================================
# AIFunction - Microsoft Agent Framework íŒ¨í„´
# ============================================================================

class AIFunction(ABC):
    """
    AI Function ì¶”ìƒ í´ë˜ìŠ¤ - Microsoft Agent Framework íŒ¨í„´

    [ì‹ ê·œ] OpenAI Function Callingì„ ìœ„í•œ ì¶”ìƒ í´ë˜ìŠ¤

    ì°¸ì¡°: https://github.com/microsoft/agent-framework/blob/main/python/samples/getting_started/tools/

    ì£¼ìš” ê¸°ëŠ¥:
    - get_schema(): OpenAI Function Calling ìŠ¤í‚¤ë§ˆ ë°˜í™˜
    - invoke_with_metrics(): ë©”íŠ¸ë¦­ê³¼ í•¨ê»˜ ì‹¤í–‰
    """

    def __init__(self, name: str, description: str, parameters: Optional[Dict[str, Any]] = None):
        self.name = name
        self.description = description
        self.parameters = parameters or {}
        self.execution_count = 0
        self.total_duration_ms = 0.0

    @abstractmethod
    async def execute(self, **kwargs) -> Any:
        """í•¨ìˆ˜ ì‹¤í–‰"""
        pass

    def get_schema(self) -> Dict[str, Any]:
        """
        OpenAI Function Calling ìŠ¤í‚¤ë§ˆ

        [ì‹ ê·œ] OpenAI APIì— ì „ë‹¬í•  í•¨ìˆ˜ ìŠ¤í‚¤ë§ˆ ìƒì„±
        """
        return {
            "name": self.name,
            "description": self.description,
            "parameters": self.parameters
        }

    async def invoke_with_metrics(self, **kwargs) -> tuple[Any, float]:
        """
        ë©”íŠ¸ë¦­ê³¼ í•¨ê»˜ ì‹¤í–‰

        [ì‹ ê·œ] ì‹¤í–‰ ì‹œê°„ ì¸¡ì • ë° ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        """
        start_time = time.time()
        result = await self.execute(**kwargs)
        duration_ms = (time.time() - start_time) * 1000

        self.execution_count += 1
        self.total_duration_ms += duration_ms

        return result, duration_ms


class ApprovalRequiredAIFunction(AIFunction):
    """
    Human-in-the-loop ìŠ¹ì¸ì´ í•„ìš”í•œ í•¨ìˆ˜

    [ì‹ ê·œ] Microsoft Agent Frameworkì˜ approval íŒ¨í„´

    ì°¸ì¡°: https://github.com/microsoft/agent-framework/blob/main/python/samples/getting_started/tools/ai_tool_with_approval.py

    ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:
    - ê²°ì œ ì²˜ë¦¬
    - ë°ì´í„° ì‚­ì œ
    - ì¤‘ìš”í•œ ì„¤ì • ë³€ê²½
    - ì™¸ë¶€ API í˜¸ì¶œ

    ìë™ ìŠ¹ì¸:
    - auto_approve_threshold ì„¤ì • ì‹œ ì•ˆì „í•œ ì‘ì—…ì€ ìë™ ìŠ¹ì¸
    - ì˜ˆ: ì½ê¸° ì „ìš© ì‘ì—…, ë‚®ì€ ê¸ˆì•¡ì˜ ê²°ì œ ë“±
    """

    def __init__(self, base_function: AIFunction,
                 approval_callback: Optional[Callable] = None,
                 auto_approve_threshold: Optional[float] = None):
        super().__init__(
            name=f"{base_function.name}_approval_required",
            description=f"{base_function.description} (Requires Approval)",
            parameters=base_function.parameters
        )
        self.base_function = base_function
        self.approval_callback = approval_callback
        self.auto_approve_threshold = auto_approve_threshold

    async def execute(self, **kwargs) -> Dict[str, Any]:
        """ìŠ¹ì¸ ìš”ì²­ ìƒì„±"""
        approval_request = {
            "function_name": self.base_function.name,
            "arguments": kwargs,
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "status": ApprovalStatus.PENDING,
            "description": self.description
        }

        # [ì‹ ê·œ] ìë™ ìŠ¹ì¸ ì„ê³„ê°’ í™•ì¸
        if self.auto_approve_threshold and self._is_safe_operation(**kwargs):
            approval_request["status"] = ApprovalStatus.AUTO_APPROVED
            result = await self.base_function.execute(**kwargs)
            approval_request["result"] = result
            return approval_request

        # ìŠ¹ì¸ ì½œë°± ì‹¤í–‰
        if self.approval_callback:
            approved = await self.approval_callback(approval_request)
            if approved:
                approval_request["status"] = ApprovalStatus.APPROVED
                result = await self.base_function.execute(**kwargs)
                approval_request["result"] = result
            else:
                approval_request["status"] = ApprovalStatus.REJECTED
                approval_request["result"] = "Operation rejected by user"

        return approval_request

    def _is_safe_operation(self, **kwargs) -> bool:
        """
        ì•ˆì „í•œ ì‘ì—…ì¸ì§€ í™•ì¸ (ì˜ˆ: ì½ê¸° ì „ìš©)

        [ì‹ ê·œ] ìë™ ìŠ¹ì¸ ë¡œì§
        """
        # ì½ê¸° ì „ìš© ì‘ì—…ì€ ìë™ ìŠ¹ì¸ (ì˜ˆ: get_, read_, list_ ë¡œ ì‹œì‘)
        if self.base_function.name.startswith(("get_", "read_", "list_")):
            return True
        return False


# ============================================================================
# Memory Hook Provider íŒ¨í„´ (Amazon Bedrock AgentCore ì°¸ì¡°)
# ============================================================================

@dataclass
class ConversationMessage:
    """
    ëŒ€í™” ë©”ì‹œì§€ ëª¨ë¸ (AgentCore Memory íŒ¨í„´)

    ì°¸ì¡°: https://github.com/awslabs/amazon-bedrock-agentcore-samples
    """
    content: str
    role: str  # USER, ASSISTANT, TOOL
    timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    agent_name: Optional[str] = None
    session_id: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)


class MemoryHookProvider:
    """
    Memory Hook Provider - ìë™ ë©”ëª¨ë¦¬ ê´€ë¦¬

    ì°¸ì¡°: amazon-bedrock-agentcore-samples/memory/hooks.py

    ì£¼ìš” ê¸°ëŠ¥:
    - ëŒ€í™” ê¸°ë¡ ìë™ ì €ì¥/ë¡œë“œ
    - ì„¸ì…˜ ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬
    - ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ê¸°ë°˜ ë©”ëª¨ë¦¬ ë¶„ë¥˜

    ì‚¬ìš©ë²•:
    ```python
    memory_hook = MemoryHookProvider(
        memory_store=memory_store,
        session_id="session-123",
        actor_id="user-456"
    )

    # ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì‹œ ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ
    context = await memory_hook.on_agent_initialized(agent_name="assistant")

    # ë©”ì‹œì§€ ì¶”ê°€ ì‹œ ìë™ ì €ì¥
    await memory_hook.on_message_added(message, agent_name="assistant")
    ```
    """

    def __init__(
        self,
        memory_store: 'MemoryStore',
        session_id: str,
        actor_id: str,
        max_context_turns: int = 10,
        namespace: str = "/conversation"
    ):
        self.memory_store = memory_store
        self.session_id = session_id
        self.actor_id = actor_id
        self.max_context_turns = max_context_turns
        self.namespace = namespace
        self.conversation_history: List[ConversationMessage] = []
        self._logger = StructuredLogger("memory_hook")

    async def on_agent_initialized(self, agent_name: str) -> List[ConversationMessage]:
        """
        ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì‹œ ìµœê·¼ ëŒ€í™” ê¸°ë¡ ë¡œë“œ
        """
        try:
            key = f"{self.namespace}/{self.session_id}/history"
            data = await self.memory_store.load(key)

            if data:
                messages = data.get("messages", [])
                self.conversation_history = [
                    ConversationMessage(**msg) for msg in messages[-self.max_context_turns:]
                ]
                self._logger.info(
                    f"Loaded {len(self.conversation_history)} messages",
                    agent=agent_name,
                    session_id=self.session_id
                )

            return self.conversation_history
        except Exception as e:
            self._logger.error(f"Failed to load history: {e}")
            return []

    async def on_message_added(
        self,
        content: str,
        role: str,
        agent_name: Optional[str] = None
    ):
        """
        ë©”ì‹œì§€ ì¶”ê°€ ì‹œ ìë™ ì €ì¥
        """
        message = ConversationMessage(
            content=content,
            role=role,
            agent_name=agent_name,
            session_id=self.session_id
        )

        self.conversation_history.append(message)

        # ì €ì¥
        try:
            key = f"{self.namespace}/{self.session_id}/history"
            await self.memory_store.save(key, {
                "messages": [{
                    "content": m.content,
                    "role": m.role,
                    "timestamp": m.timestamp.isoformat(),
                    "agent_name": m.agent_name,
                    "session_id": m.session_id
                } for m in self.conversation_history[-self.max_context_turns:]],
                "actor_id": self.actor_id,
                "updated_at": datetime.now(timezone.utc).isoformat()
            })
        except Exception as e:
            self._logger.error(f"Failed to save message: {e}")

    async def get_last_k_turns(self, k: int = 5) -> List[ConversationMessage]:
        """
        ìµœê·¼ kê°œ ëŒ€í™” í„´ ì¡°íšŒ
        """
        return self.conversation_history[-k:]

    async def clear_session(self):
        """
        ì„¸ì…˜ ë°ì´í„° ì‚­ì œ
        """
        key = f"{self.namespace}/{self.session_id}/history"
        await self.memory_store.delete(key)
        self.conversation_history = []
        self._logger.info("Session cleared", session_id=self.session_id)


class MemorySessionManager:
    """
    ì„¸ì…˜ ê¸°ë°˜ ë©”ëª¨ë¦¬ ê´€ë¦¬ì (AgentCore MemorySessionManager íŒ¨í„´)

    ì°¸ì¡°: amazon-bedrock-agentcore-samples/memory/session_manager.py

    ì£¼ìš” ê¸°ëŠ¥:
    - ë‹¤ì¤‘ ì„¸ì…˜ ê´€ë¦¬
    - ì„¸ì…˜ ê°„ ì»¨í…ìŠ¤íŠ¸ ê³µìœ 
    - ìë™ ì„¸ì…˜ ì •ë¦¬
    """

    def __init__(self, memory_store: 'MemoryStore', default_ttl_hours: int = 24):
        self.memory_store = memory_store
        self.default_ttl_hours = default_ttl_hours
        self._sessions: Dict[str, MemoryHookProvider] = {}
        self._logger = StructuredLogger("session_manager")

    def get_or_create_session(
        self,
        session_id: str,
        actor_id: str,
        namespace: str = "/conversation"
    ) -> MemoryHookProvider:
        """
        ì„¸ì…˜ ì¡°íšŒ ë˜ëŠ” ìƒì„±
        """
        key = f"{actor_id}:{session_id}"

        if key not in self._sessions:
            self._sessions[key] = MemoryHookProvider(
                memory_store=self.memory_store,
                session_id=session_id,
                actor_id=actor_id,
                namespace=namespace
            )
            self._logger.info(
                "Created new session",
                session_id=session_id,
                actor_id=actor_id
            )

        return self._sessions[key]

    async def list_sessions(self, actor_id: Optional[str] = None) -> List[str]:
        """
        ì„¸ì…˜ ëª©ë¡ ì¡°íšŒ
        """
        sessions = []
        for key in self._sessions.keys():
            if actor_id is None or key.startswith(f"{actor_id}:"):
                sessions.append(key)
        return sessions

    async def cleanup_expired_sessions(self):
        """
        ë§Œë£Œëœ ì„¸ì…˜ ì •ë¦¬
        """
        # êµ¬í˜„: TTL ê¸°ë°˜ ì„¸ì…˜ ì •ë¦¬
        pass


# ============================================================================
# MCP (Model Context Protocol) í†µí•©
# ============================================================================

class MockMCPClient:
    """
    [ì‹ ê·œ] MCP í´ë¼ì´ì–¸íŠ¸ ëª¨ì˜ êµ¬í˜„ (ë°ëª¨ìš©)
    """
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.tools = {
            "calculator": {
                "name": "calculator",
                "description": "Perform basic calculations",
                "parameters": {"type": "object", "properties": {"expression": {"type": "string"}}}
            },
            "web_search": {
                "name": "web_search",
                "description": "Search the web for information",
                "parameters": {"type": "object", "properties": {"query": {"type": "string"}}}
            }
        }

    async def list_tools(self) -> List[Dict[str, Any]]:
        return list(self.tools.values())

    async def call_tool(self, name: str, arguments: Dict[str, Any]) -> Any:
        if name == "calculator":
            return f"Calculated: {arguments.get('expression')} = 42 (Mock)"
        elif name == "web_search":
            return f"Search results for '{arguments.get('query')}': [Mock Result 1, Mock Result 2]"
        return f"Tool {name} executed with {arguments}"

class MCPTool:
    """
    MCP ì„œë²„ì™€ í†µí•©í•˜ëŠ” ë„êµ¬
    """

    def __init__(self, name: str, server_config: Dict[str, Any]):
        self.name = name
        self.server_config = server_config
        self.connected = False
        self.client: Optional[MockMCPClient] = None
        self.available_tools: List[Dict[str, Any]] = []

    async def connect(self):
        """
        MCP ì„œë²„ ì—°ê²°
        """
        try:
            logging.info(f"ğŸ”Œ MCP ì„œë²„ ì—°ê²° ì‹œë„: {self.name}")
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” mcp.Client ì‚¬ìš©
            self.client = MockMCPClient(self.server_config)
            self.available_tools = await self.client.list_tools()
            self.connected = True
            logging.info(f"âœ… MCP ì„œë²„ ì—°ê²° ì„±ê³µ: {self.name}")
        except Exception as e:
            logging.error(f"âŒ MCP ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
            raise

    async def disconnect(self):
        """MCP ì„œë²„ ì—°ê²° í•´ì œ"""
        if self.connected:
            logging.info(f"ğŸ”Œ MCP ì„œë²„ ì—°ê²° í•´ì œ: {self.name}")
            self.connected = False
            self.client = None

    async def get_available_tools(self) -> List[Dict[str, Any]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡"""
        if not self.connected:
            await self.connect()
        return self.available_tools

    async def invoke_tool(self, tool_name: str, **kwargs) -> Any:
        """MCP ë„êµ¬ í˜¸ì¶œ"""
        if not self.connected:
            raise RuntimeError("MCP ì„œë²„ê°€ ì—°ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        logging.info(f"ğŸ› ï¸ MCP ë„êµ¬ í˜¸ì¶œ: {tool_name}")
        return await self.client.call_tool(tool_name, kwargs)


# ============================================================================
# íšŒë¡œ ì°¨ë‹¨ê¸° íŒ¨í„´
# ============================================================================

class CircuitBreaker:
    """
    íšŒë¡œ ì°¨ë‹¨ê¸° - ì¥ì•  ì „íŒŒ ë°©ì§€

    [ì‹ ê·œ] ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜ì˜ í•µì‹¬ íŒ¨í„´

    ìƒíƒœ ì „í™˜:
    1. CLOSED (ì •ìƒ): ëª¨ë“  ìš”ì²­ í—ˆìš©
    2. OPEN (ì°¨ë‹¨): ì‹¤íŒ¨ ì„ê³„ê°’ ë„ë‹¬, ëª¨ë“  ìš”ì²­ ì°¨ë‹¨
    3. HALF_OPEN (ë°˜ê°œë°©): íƒ€ì„ì•„ì›ƒ í›„ ì¼ë¶€ ìš”ì²­ í—ˆìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸

    ì£¼ìš” íŒŒë¼ë¯¸í„°:
    - failure_threshold: ì—°ì† ì‹¤íŒ¨ ì„ê³„ê°’ (ê¸°ë³¸ 5íšŒ)
    - timeout: OPEN ìƒíƒœ ìœ ì§€ ì‹œê°„ (ê¸°ë³¸ 60ì´ˆ)

    ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:
    - ì™¸ë¶€ API í˜¸ì¶œ
    - ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬
    - LLM API í˜¸ì¶œ
    """

    def __init__(self, failure_threshold: int = 5, timeout: float = 60.0):
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.failure_count = 0
        self.last_failure_time: Optional[float] = None
        self.state = "CLOSED"  # CLOSED, OPEN, HALF_OPEN

    async def call(self, func: Callable, *args, **kwargs):
        """
        íšŒë¡œ ì°¨ë‹¨ê¸°ë¥¼ í†µí•œ í•¨ìˆ˜ í˜¸ì¶œ

        [ì‹ ê·œ] ì¥ì•  ê²©ë¦¬ ë° ë¹ ë¥¸ ì‹¤íŒ¨
        """
        if self.state == "OPEN":
            if time.time() - self.last_failure_time > self.timeout:
                self.state = "HALF_OPEN"
                logging.info("ğŸ”„ íšŒë¡œ ì°¨ë‹¨ê¸°: HALF_OPEN ìƒíƒœ")
            else:
                raise RuntimeError("íšŒë¡œ ì°¨ë‹¨ê¸°ê°€ OPEN ìƒíƒœì…ë‹ˆë‹¤")

        try:
            result = await func(*args, **kwargs)
            if self.state == "HALF_OPEN":
                self.state = "CLOSED"
                self.failure_count = 0
                logging.info("âœ… íšŒë¡œ ì°¨ë‹¨ê¸°: CLOSED ìƒíƒœ ë³µêµ¬")
            return result
        except Exception as e:
            self.failure_count += 1
            self.last_failure_time = time.time()

            if self.failure_count >= self.failure_threshold:
                self.state = "OPEN"
                logging.error(f"âŒ íšŒë¡œ ì°¨ë‹¨ê¸°: OPEN ìƒíƒœ ({self.failure_count} ì‹¤íŒ¨)")

            raise e


# ============================================================================
# ë©”ëª¨ë¦¬ ì €ì¥ì†Œ - í–¥ìƒëœ ë²„ì „
# ============================================================================

class MemoryStore(ABC):
    """
    ë©”ëª¨ë¦¬ ì €ì¥ì†Œ ì¸í„°í˜ì´ìŠ¤

    [ìˆ˜ì •] list_keys ë©”ì„œë“œ ì¶”ê°€
    """

    @abstractmethod
    async def save(self, key: str, data: Dict) -> None:
        pass

    @abstractmethod
    async def load(self, key: str) -> Optional[Dict]:
        pass

    @abstractmethod
    async def delete(self, key: str) -> None:
        pass

    @abstractmethod
    async def list_keys(self, pattern: str = "*") -> List[str]:
        """[ì‹ ê·œ] í‚¤ ëª©ë¡ ì¡°íšŒ"""
        pass


class CachedMemoryStore(MemoryStore):
    """
    ìºì‹± ë©”ëª¨ë¦¬ ì €ì¥ì†Œ - LRU ìºì‹œ

    [ìˆ˜ì •] LRU (Least Recently Used) ìºì‹œ ì•Œê³ ë¦¬ì¦˜ ì ìš©

    ê¸°ì¡´ vs ê³ ë„í™”:
    - ê¸°ì¡´: ë‹¨ìˆœ ì ‘ê·¼ íšŸìˆ˜ ê¸°ë°˜ ìºì‹±
    - ê³ ë„í™”: LRU ì•Œê³ ë¦¬ì¦˜ + max_cache_size + access_order ì¶”ì 

    LRU ìºì‹œ ì¥ì :
    - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œí•œ (max_cache_size)
    - ìµœê·¼ ì‚¬ìš© ë°ì´í„° ìš°ì„  ìœ ì§€
    - ì˜¤ë˜ëœ ë°ì´í„° ìë™ ì œê±°
    """

    def __init__(self, max_cache_size: int = 100):
        self.data: Dict[str, Dict] = {}
        self.cache: Dict[str, Any] = {}
        self.access_count: Dict[str, int] = defaultdict(int)
        self.max_cache_size = max_cache_size  # ğŸ†• ìµœëŒ€ ìºì‹œ í¬ê¸°
        self.access_order: List[str] = []  # ğŸ†• LRU ìˆœì„œ ì¶”ì 

    async def save(self, key: str, data: Dict) -> None:
        self.data[key] = {
            'data': data,
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'version': self.data.get(key, {}).get('version', 0) + 1  # ğŸ†• ë²„ì „ ê´€ë¦¬
        }
        self.access_count[key] += 1

        # ìì£¼ ì ‘ê·¼í•˜ëŠ” ë°ì´í„°ëŠ” ìºì‹œì— ì €ì¥
        if self.access_count[key] > 3:
            self._add_to_cache(key, data)

    async def load(self, key: str) -> Optional[Dict]:
        if key in self.cache:
            self._update_access_order(key)  # ğŸ†• LRU ìˆœì„œ ì—…ë°ì´íŠ¸
            self.access_count[key] += 1
            return self.cache[key]

        if key in self.data:
            self.access_count[key] += 1
            return self.data[key]['data']
        return None

    async def delete(self, key: str) -> None:
        if key in self.data:
            del self.data[key]
        if key in self.cache:
            del self.cache[key]
            self.access_order.remove(key)  # ğŸ†• ìˆœì„œì—ì„œë„ ì œê±°

    async def list_keys(self, pattern: str = "*") -> List[str]:
        """
        í‚¤ ëª©ë¡ ë°˜í™˜ (ê°„ë‹¨í•œ íŒ¨í„´ ë§¤ì¹­)

        [ì‹ ê·œ] ì™€ì¼ë“œì¹´ë“œ íŒ¨í„´ ì§€ì›
        """
        if pattern == "*":
            return list(self.data.keys())
        # ê°„ë‹¨í•œ ì™€ì¼ë“œì¹´ë“œ ì§€ì›
        import fnmatch
        return [k for k in self.data.keys() if fnmatch.fnmatch(k, pattern)]

    def _add_to_cache(self, key: str, data: Any):
        """
        LRU ìºì‹œì— ì¶”ê°€

        [ì‹ ê·œ] LRU ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„
        """
        if len(self.cache) >= self.max_cache_size:
            # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±° (LRU)
            oldest_key = self.access_order.pop(0)
            del self.cache[oldest_key]

        self.cache[key] = data
        self._update_access_order(key)

    def _update_access_order(self, key: str):
        """
        ì ‘ê·¼ ìˆœì„œ ì—…ë°ì´íŠ¸

        [ì‹ ê·œ] LRU ìˆœì„œ ì¶”ì 
        """
        if key in self.access_order:
            self.access_order.remove(key)
        self.access_order.append(key)


# ============================================================================
# ì´ë²¤íŠ¸ ì‹œìŠ¤í…œ
# ============================================================================

class EventType(str, Enum):
    """
    ì´ë²¤íŠ¸ íƒ€ì…

    [ì‹ ê·œ] Pub-Sub íŒ¨í„´ì„ ìœ„í•œ ì´ë²¤íŠ¸ íƒ€ì… ì •ì˜

    10ê°€ì§€ ì´ë²¤íŠ¸ íƒ€ì…:
    - Agent ìƒëª…ì£¼ê¸°: STARTED, COMPLETED, FAILED
    - Node ìƒëª…ì£¼ê¸°: NODE_STARTED, NODE_COMPLETED
    - ìŠ¹ì¸ ê´€ë ¨: APPROVAL_REQUESTED, APPROVAL_GRANTED, APPROVAL_DENIED
    - ë©”ì‹œì§€: MESSAGE_RECEIVED, MESSAGE_SENT
    """
    AGENT_STARTED = "agent_started"
    AGENT_COMPLETED = "agent_completed"
    AGENT_FAILED = "agent_failed"
    NODE_STARTED = "node_started"
    NODE_COMPLETED = "node_completed"
    APPROVAL_REQUESTED = "approval_requested"
    APPROVAL_GRANTED = "approval_granted"
    APPROVAL_DENIED = "approval_denied"
    MESSAGE_RECEIVED = "message_received"
    MESSAGE_SENT = "message_sent"


class AgentEvent(BaseModel):
    """
    Agent ì´ë²¤íŠ¸

    [ì‹ ê·œ] ì´ë²¤íŠ¸ ë°ì´í„° ëª¨ë¸
    """
    event_type: EventType
    timestamp: str = Field(default_factory=lambda: datetime.now(timezone.utc).isoformat())
    agent_name: Optional[str] = None
    node_name: Optional[str] = None
    data: Dict[str, Any] = Field(default_factory=dict)


class EventBus:
    """
    ì´ë²¤íŠ¸ ë²„ìŠ¤

    [ì‹ ê·œ] Pub-Sub íŒ¨í„´ êµ¬í˜„

    ì£¼ìš” ê¸°ëŠ¥:
    - subscribe(): ì´ë²¤íŠ¸ êµ¬ë…
    - publish(): ì´ë²¤íŠ¸ ë°œí–‰
    - get_event_history(): ì´ë²¤íŠ¸ íˆìŠ¤í† ë¦¬ ì¡°íšŒ

    ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:
    - ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§
    - ì•Œë¦¼ ì „ì†¡
    - ë©”íŠ¸ë¦­ ìˆ˜ì§‘
    - ì›Œí¬í”Œë¡œìš° ì¡°ìœ¨

    ì˜ˆì‹œ:
    async def on_approval_requested(event):
        await send_slack_notification(event.data)

    event_bus.subscribe(EventType.APPROVAL_REQUESTED, on_approval_requested)
    """

    def __init__(self):
        self.subscribers: Dict[EventType, List[Callable]] = defaultdict(list)
        self.event_history: List[AgentEvent] = []

    def subscribe(self, event_type: EventType, handler: Callable):
        """ì´ë²¤íŠ¸ êµ¬ë…"""
        self.subscribers[event_type].append(handler)
        logging.info(f"ğŸ“¢ ì´ë²¤íŠ¸ êµ¬ë…: {event_type}")

    async def publish(self, event: AgentEvent):
        """ì´ë²¤íŠ¸ ë°œí–‰"""
        self.event_history.append(event)

        handlers = self.subscribers.get(event.event_type, [])
        for handler in handlers:
            try:
                if asyncio.iscoroutinefunction(handler):
                    await handler(event)
                else:
                    handler(event)
            except Exception as e:
                logging.error(f"âŒ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì˜¤ë¥˜: {e}")

    def get_event_history(self, event_type: Optional[EventType] = None,
                         limit: int = 100) -> List[AgentEvent]:
        """ì´ë²¤íŠ¸ íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
        if event_type:
            filtered = [e for e in self.event_history if e.event_type == event_type]
            return filtered[-limit:]
        return self.event_history[-limit:]


# ============================================================================
# Agent ê¸°ë³¸ í´ë˜ìŠ¤ - í–¥ìƒëœ ë²„ì „
# ============================================================================

class Agent(ABC):
    """
    Agent ê¸°ë³¸ í´ë˜ìŠ¤

    [ìˆ˜ì •] ì—¬ëŸ¬ ê¸°ëŠ¥ ì¶”ê°€
    1. enable_streaming: ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì§€ì›
    2. event_bus: ì´ë²¤íŠ¸ ë°œí–‰
    3. circuit_breaker: íšŒë¡œ ì°¨ë‹¨ê¸° í†µí•©
    4. ë©”íŠ¸ë¦­ ì¶”ì : total_executions, total_tokens, total_duration_ms
    """

    def __init__(
        self,
        name: str,
        role: AgentRole = AgentRole.ASSISTANT,
        system_prompt: str = "You are a helpful AI assistant.",
        model: str = DEFAULT_LLM_MODEL,  # ğŸ†• ì¤‘ì•™ ì„¤ì • ì‚¬ìš©
        temperature: float = 0.7,
        max_tokens: int = 1000,
        enable_streaming: bool = False,  # ğŸ†• ìŠ¤íŠ¸ë¦¬ë° ì˜µì…˜
        event_bus: Optional[EventBus] = None,  # ğŸ†• ì´ë²¤íŠ¸ ë²„ìŠ¤
        circuit_breaker: Optional[CircuitBreaker] = None,  # ğŸ†• íšŒë¡œ ì°¨ë‹¨ê¸°
        service_id: Optional[str] = None  # ğŸ†• ì„œë¹„ìŠ¤ ID (ì—†ìœ¼ë©´ model ì‚¬ìš©)
    ):
        self.name = name
        self.role = role
        self.system_prompt = system_prompt
        self.model = model
        self.service_id = service_id or model  # ğŸ†• service_id ì €ì¥
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.enable_streaming = enable_streaming
        self.event_bus = event_bus
        self.circuit_breaker = circuit_breaker or CircuitBreaker()

        # ğŸ†• ëª¨ë¸ì— ë”°ë¼ temperature ì§€ì› ì—¬ë¶€ í™•ì¸
        self.execution_settings = create_execution_settings(
            model=model,
            temperature=temperature,
            max_tokens=max_tokens,
            service_id=self.service_id
        )

        # ğŸ†• êµ¬ì¡°í™”ëœ ë¡œê±°
        self.logger = StructuredLogger(f"agent.{name}")

        # ğŸ†• ë©”íŠ¸ë¦­
        self.total_executions = 0
        self.total_tokens = 0
        self.total_duration_ms = 0.0

    @abstractmethod
    async def execute(self, state: AgentState, kernel: Kernel) -> NodeResult:
        """Agent ì‹¤í–‰"""
        pass

    async def _get_llm_response(self, kernel: Kernel, messages: List[Message],
                               use_streaming: bool = False) -> str:
        """
        LLM ì‘ë‹µ ê°€ì ¸ì˜¤ê¸°

        [ìˆ˜ì •] use_streaming íŒŒë¼ë¯¸í„° ì¶”ê°€
        """
        chat_completion = kernel.get_service(
            service_id=self.service_id,  # ğŸ†• service_id ì‚¬ìš©
            type=ChatCompletionClientBase
        )

        history = ChatHistory()
        history.add_system_message(self.system_prompt)

        for msg in messages:
            if msg.role == AgentRole.USER:
                history.add_user_message(msg.content)
            elif msg.role == AgentRole.ASSISTANT:
                history.add_assistant_message(msg.content)

        settings = self.execution_settings
        settings.function_choice_behavior = None

        # ğŸ†• ìŠ¤íŠ¸ë¦¬ë° ì§€ì›
        if use_streaming and self.enable_streaming:
            return await self._get_streaming_response(chat_completion, history, settings, kernel)
        else:
            # ğŸ†• ì¬ì‹œë„ ë¡œì§ ì ìš©
            response = await retry_with_backoff(
                chat_completion.get_chat_message_content,
                max_retries=3,
                chat_history=history,
                settings=settings,
                kernel=kernel
            )
            return str(response)

    async def _get_streaming_response(self, chat_completion, history, settings, kernel) -> str:
        """
        ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬

        [ì‹ ê·œ] ì‹¤ì‹œê°„ í† í° ë‹¨ìœ„ ì¶œë ¥

        ì¥ì :
        - ê¸´ ì‘ë‹µì˜ ê²½ìš° ì‚¬ìš©ì ê²½í—˜ í–¥ìƒ
        - ì‘ë‹µ ëŒ€ê¸° ì‹œê°„ ê°ì†Œ
        - ì‹¤ì‹œê°„ í”¼ë“œë°±
        """
        full_response = []

        async for chunk in chat_completion.get_streaming_chat_message_contents(
            chat_history=history,
            settings=settings,
            kernel=kernel
        ):
            if chunk:
                content = str(chunk)
                full_response.append(content)
                # ì‹¤ì‹œê°„ ì¶œë ¥ (ì˜µì…˜)
                print(content, end="", flush=True)

        print()  # ì¤„ë°”ê¿ˆ
        return "".join(full_response)

    async def _emit_event(self, event_type: EventType, data: Dict[str, Any]):
        """
        ì´ë²¤íŠ¸ ë°œí–‰

        [ì‹ ê·œ] EventBusë¥¼ í†µí•œ ì´ë²¤íŠ¸ ë°œí–‰
        """
        if self.event_bus:
            event = AgentEvent(
                event_type=event_type,
                agent_name=self.name,
                data=data
            )
            await self.event_bus.publish(event)


class SimpleAgent(Agent):
    """
    ë‹¨ìˆœ ëŒ€í™” Agent - í–¥ìƒëœ ë²„ì „

    [ìˆ˜ì •] ê°œì„ ì‚¬í•­:
    1. ì´ë²¤íŠ¸ ë°œí–‰ (AGENT_STARTED, AGENT_COMPLETED, AGENT_FAILED)
    2. íšŒë¡œ ì°¨ë‹¨ê¸°ë¥¼ í†µí•œ í˜¸ì¶œ
    3. ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (total_executions, total_duration_ms)
    """

    async def execute(self, state: AgentState, kernel: Kernel) -> NodeResult:
        start_time = time.time()

        # ğŸ†• ì´ë²¤íŠ¸ ë°œí–‰
        await self._emit_event(EventType.AGENT_STARTED, {"node": self.name})

        try:
            recent_messages = state.get_conversation_history(max_messages=5)

            # ğŸ†• íšŒë¡œ ì°¨ë‹¨ê¸°ë¥¼ í†µí•œ í˜¸ì¶œ
            response = await self.circuit_breaker.call(
                self._get_llm_response,
                kernel,
                recent_messages,
                self.enable_streaming
            )

            state.add_message(AgentRole.ASSISTANT, response, self.name)

            duration_ms = (time.time() - start_time) * 1000

            # ğŸ†• ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self.total_executions += 1
            self.total_duration_ms += duration_ms

            # ğŸ†• ì™„ë£Œ ì´ë²¤íŠ¸
            await self._emit_event(EventType.AGENT_COMPLETED, {
                "node": self.name,
                "duration_ms": duration_ms
            })

            return NodeResult(
                node_name=self.name,
                output=response,
                success=True,
                duration_ms=duration_ms
            )
        except Exception as e:
            logging.error(f"âŒ Agent {self.name} ì‹¤í–‰ ì‹¤íŒ¨: {e}")

            # ğŸ†• ì‹¤íŒ¨ ì´ë²¤íŠ¸
            await self._emit_event(EventType.AGENT_FAILED, {
                "node": self.name,
                "error": str(e)
            })

            return NodeResult(
                node_name=self.name,
                output="",
                success=False,
                error=str(e)
            )


class ApprovalAgent(Agent):
    """
    ìŠ¹ì¸ì´ í•„ìš”í•œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” Agent

    [ì‹ ê·œ] Human-in-the-loop íŒ¨í„´ êµ¬í˜„

    ì°¸ì¡°: https://github.com/microsoft/agent-framework/blob/main/python/samples/getting_started/tools/ai_tool_with_approval.py

    ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:
    - ë°ì´í„° ì‚­ì œ ì‘ì—…
    - ê²°ì œ ì²˜ë¦¬
    - ì¤‘ìš” ì„¤ì • ë³€ê²½
    - ì™¸ë¶€ API í˜¸ì¶œ
    """

    def __init__(self, *args, approval_function: ApprovalRequiredAIFunction, **kwargs):
        super().__init__(*args, **kwargs)
        self.approval_function = approval_function

    async def execute(self, state: AgentState, kernel: Kernel) -> NodeResult:
        start_time = time.time()

        try:
            # ì‚¬ìš©ì ì…ë ¥ì—ì„œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
            recent_messages = state.get_conversation_history(max_messages=3)
            last_message = recent_messages[-1].content if recent_messages else ""

            # ìŠ¹ì¸ ìš”ì²­ ìƒì„±
            approval_result = await self.approval_function.execute(input=last_message)

            if approval_result["status"] == ApprovalStatus.PENDING:
                # ìŠ¹ì¸ ëŒ€ê¸° ìƒíƒœ
                state.add_pending_approval(approval_result)
                await self._emit_event(EventType.APPROVAL_REQUESTED, approval_result)

                return NodeResult(
                    node_name=self.name,
                    output=f"ìŠ¹ì¸ ëŒ€ê¸° ì¤‘: {approval_result['description']}",
                    success=True,
                    requires_approval=True,
                    approval_data=approval_result,
                    duration_ms=(time.time() - start_time) * 1000
                )
            else:
                # ìŠ¹ì¸ë¨ ë˜ëŠ” ìë™ ìŠ¹ì¸
                result = approval_result.get("result", "")
                state.add_message(AgentRole.ASSISTANT, str(result), self.name)

                return NodeResult(
                    node_name=self.name,
                    output=str(result),
                    success=True,
                    duration_ms=(time.time() - start_time) * 1000
                )

        except Exception as e:
            logging.error(f"âŒ ApprovalAgent ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return NodeResult(
                node_name=self.name,
                output="",
                success=False,
                error=str(e)
            )


class RouterAgent(Agent):
    """
    ë¼ìš°íŒ… Agent - í–¥ìƒëœ ë²„ì „

    [ìˆ˜ì •] ê°œì„ ì‚¬í•­:
    1. default_route íŒŒë¼ë¯¸í„° ì¶”ê°€
    2. routing_history ì¶”ì  (ì¸í…íŠ¸ ë¶„ë¥˜ íˆìŠ¤í† ë¦¬)
    3. ë©”íƒ€ë°ì´í„°ì— confidence ì¶”ê°€
    """

    def __init__(self, *args, routes: Dict[str, str],
                 default_route: Optional[str] = None, **kwargs):
        super().__init__(*args, role=AgentRole.ROUTER, **kwargs)
        self.routes = routes
        self.default_route = default_route or list(routes.values())[0] if routes else None  # ğŸ†• ê¸°ë³¸ ê²½ë¡œ
        self.routing_history: List[Dict[str, Any]] = []  # ğŸ†• ë¼ìš°íŒ… íˆìŠ¤í† ë¦¬

    async def execute(self, state: AgentState, kernel: Kernel) -> NodeResult:
        start_time = time.time()

        try:
            recent_messages = state.get_conversation_history(max_messages=3)
            last_message = recent_messages[-1].content if recent_messages else ""

            routes_list = ', '.join(self.routes.keys())
            classification_prompt = f"""Classify the user's intent into one of these categories: {routes_list}

User message: {last_message}

Respond with ONLY the category name (one word)."""

            temp_messages = [Message(role=AgentRole.USER, content=classification_prompt)]
            intent = await self._get_llm_response(kernel, temp_messages)
            intent = intent.strip().lower()

            next_node = self.routes.get(intent, self.default_route)
            duration_ms = (time.time() - start_time) * 1000

            # ğŸ†• ë¼ìš°íŒ… íˆìŠ¤í† ë¦¬ ì €ì¥
            routing_record = {
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "message": last_message,
                "intent": intent,
                "next_node": next_node
            }
            self.routing_history.append(routing_record)

            logging.info(f"ğŸ”€ Router: '{intent}' -> '{next_node}'")

            return NodeResult(
                node_name=self.name,
                output=f"ë¼ìš°íŒ…: {next_node} (ì¸í…íŠ¸: {intent})",
                next_node=next_node,
                success=True,
                duration_ms=duration_ms,
                metadata={"intent": intent, "confidence": 0.95}  # ğŸ†• ì‹ ë¢°ë„ ì¶”ê°€
            )
        except Exception as e:
            logging.error(f"âŒ Router ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return NodeResult(
                node_name=self.name,
                output="",
                next_node=self.default_route,
                success=False,
                error=str(e)
            )


@dataclass
class InvestigationPlan:
    """
    Investigation Plan - ë©€í‹° ì—ì´ì „íŠ¸ ì¡°ì‚¬ ê³„íš

    ì°¸ì¡°: amazon-bedrock-agentcore-samples/SRE-agent/supervisor.py
    """
    steps: List[str]
    agents_sequence: List[str]
    complexity: str = "simple"  # simple, complex
    auto_execute: bool = True
    reasoning: str = ""

    def to_dict(self) -> Dict[str, Any]:
        return {
            "steps": self.steps,
            "agents_sequence": self.agents_sequence,
            "complexity": self.complexity,
            "auto_execute": self.auto_execute,
            "reasoning": self.reasoning
        }


class SupervisorAgent(Agent):
    """
    Supervisor Agent - ì—¬ëŸ¬ Agentë¥¼ ê°ë…í•˜ê³  ì¡°ìœ¨

    ê°œì„ ëœ íŒ¨í„´ (Amazon Bedrock AgentCore + Microsoft AutoGen í†µí•©):
    - Investigation Plan ê¸°ë°˜ ì²´ê³„ì  ì‹¤í–‰
    - ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ í†µí•©
    - ìƒì„¸í•œ ì‹¤í–‰ ì¶”ì  ë° ì§‘ê³„

    ì°¸ì¡°:
    - amazon-bedrock-agentcore-samples/SRE-agent/supervisor.py
    - Microsoft AutoGenì˜ GroupChat íŒ¨í„´

    ì£¼ìš” ê¸°ëŠ¥:
    1. Investigation Plan ìƒì„± ë° ì‹¤í–‰
    2. ë¼ìš´ë“œ ê¸°ë°˜ í˜‘ì—… (max_rounds)
    3. ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´ ("TERMINATE" í‚¤ì›Œë“œ)
    4. ìƒì„¸í•œ ì‹¤í–‰ ë¡œê·¸ (execution_log)
    5. ì‘ë‹µ ì§‘ê³„ (aggregate_responses)
    6. ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ í†µí•© (memory_hook)

    ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:
    - Research Agent + Writer Agent í˜‘ì—…
    - Diagnostic + Remediation + Prevention í˜‘ì—… (SRE íŒ¨í„´)
    - ë³µì¡í•œ multi-step ì‘ì—…
    """

    def __init__(
        self,
        *args,
        sub_agents: List[Agent],
        max_rounds: int = 3,
        memory_hook: Optional['MemoryHookProvider'] = None,
        auto_approve_simple: bool = True,  # ê°„ë‹¨í•œ ê³„íš ìë™ ì‹¤í–‰
        **kwargs
    ):
        super().__init__(*args, role=AgentRole.SUPERVISOR, **kwargs)
        self.sub_agents = {agent.name: agent for agent in sub_agents}
        self.max_rounds = max_rounds
        self.memory_hook = memory_hook
        self.auto_approve_simple = auto_approve_simple
        self.execution_log: List[Dict[str, Any]] = []
        self.investigation_history: List[InvestigationPlan] = []

    async def create_investigation_plan(
        self,
        state: AgentState,
        kernel: Kernel
    ) -> InvestigationPlan:
        """
        Investigation Plan ìƒì„± (SRE Agent íŒ¨í„´)

        ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ì—ì´ì „íŠ¸ ì‹¤í–‰ ìˆœì„œë¥¼ ê²°ì •í•©ë‹ˆë‹¤.
        """
        agent_names = list(self.sub_agents.keys())
        agent_descriptions = ", ".join([
            f"{name}: {agent.system_prompt[:100]}..."
            for name, agent in self.sub_agents.items()
        ])

        query = state.messages[-1].content if state.messages else ""

        planning_prompt = f"""You are a supervisor planning an investigation.

Available Agents:
{agent_descriptions}

User Query: {query}

Create a plan with:
1. Steps to execute
2. Agent sequence (from: {', '.join(agent_names)})
3. Complexity (simple if â‰¤3 steps, complex otherwise)

Respond in JSON format:
{{
  "steps": ["step1", "step2"],
  "agents_sequence": ["agent1", "agent2"],
  "complexity": "simple",
  "reasoning": "brief explanation"
}}"""

        temp_messages = [Message(role=AgentRole.USER, content=planning_prompt)]
        response = await self._get_llm_response(kernel, temp_messages)

        try:
            # JSON íŒŒì‹±
            import re
            json_match = re.search(r'\{[^{}]*\}', response, re.DOTALL)
            if json_match:
                plan_data = json.loads(json_match.group())
            else:
                plan_data = {
                    "steps": ["Execute query"],
                    "agents_sequence": [agent_names[0]] if agent_names else [],
                    "complexity": "simple",
                    "reasoning": "Default single-step plan"
                }
        except json.JSONDecodeError:
            plan_data = {
                "steps": ["Execute query"],
                "agents_sequence": [agent_names[0]] if agent_names else [],
                "complexity": "simple",
                "reasoning": "Fallback plan"
            }

        plan = InvestigationPlan(
            steps=plan_data.get("steps", []),
            agents_sequence=plan_data.get("agents_sequence", []),
            complexity=plan_data.get("complexity", "simple"),
            auto_execute=plan_data.get("complexity", "simple") == "simple" and self.auto_approve_simple,
            reasoning=plan_data.get("reasoning", "")
        )

        self.investigation_history.append(plan)
        logging.info(f"ğŸ“‹ Investigation Plan: {len(plan.steps)} steps, complexity={plan.complexity}")

        return plan

    async def aggregate_responses(
        self,
        responses: List[Dict[str, Any]],
        state: AgentState,
        kernel: Kernel
    ) -> str:
        """
        ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‘ë‹µ ì§‘ê³„ (SRE Agent íŒ¨í„´)
        """
        if not responses:
            return "No responses to aggregate."

        responses_text = "\n\n".join([
            f"[{r['agent']}]:\n{r['output']}" for r in responses
        ])

        aggregation_prompt = f"""Summarize the following agent responses into a cohesive answer:

{responses_text}

Provide a clear, unified response that synthesizes all findings."""

        temp_messages = [Message(role=AgentRole.USER, content=aggregation_prompt)]
        return await self._get_llm_response(kernel, temp_messages)

    async def execute(self, state: AgentState, kernel: Kernel) -> NodeResult:
        start_time = time.time()

        try:
            # Investigation Plan ìƒì„±
            plan = await self.create_investigation_plan(state, kernel)

            responses = []
            current_round = 0

            # Agent ì´ë¦„ ëª©ë¡
            agent_names = list(self.sub_agents.keys())
            agent_list_str = ", ".join(agent_names)

            while current_round < self.max_rounds:
                current_round += 1
                logging.info(f"ğŸ¯ Supervisor Round {current_round}/{self.max_rounds}")

                # 1. ë‹¤ìŒ ì‹¤í–‰í•  Agent ê²°ì • (LLM ì‚¬ìš©)
                history_text = "\n".join(responses[-3:]) if responses else "No history yet."

                decision_prompt = f"""
You are a Supervisor managing these agents: {agent_list_str}.
Current goal: {state.messages[-1].content if state.messages else 'Unknown'}

Recent history:
{history_text}

Decide the next step:
1. Select the next agent to act (respond with agent name).
2. If the task is complete, respond with "TERMINATE".

Respond with ONLY the agent name or "TERMINATE".
"""
                temp_messages = [Message(role=AgentRole.SYSTEM, content=decision_prompt)]
                decision = await self._get_llm_response(kernel, temp_messages)
                decision = decision.strip()

                logging.info(f"ğŸ¤” Supervisor Decision: {decision}")

                if "TERMINATE" in decision.upper():
                    logging.info("âœ… Supervisor decided to terminate.")
                    break

                # ì„ íƒëœ Agent ì‹¤í–‰
                selected_agent_name = None
                for name in agent_names:
                    if name.lower() in decision.lower():
                        selected_agent_name = name
                        break

                if not selected_agent_name:
                    # ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ì ìœ¼ë¡œ ì²« ë²ˆì§¸ ë˜ëŠ” ë¼ìš´ë“œ ë¡œë¹ˆ ë“± ëŒ€ì•ˆ í•„ìš”
                    # ì—¬ê¸°ì„œëŠ” ë¡œê¹… í›„ ê³„ì† ì§„í–‰ (í˜¹ì€ ì¢…ë£Œ)
                    logging.warning(f"âš ï¸ Unknown agent selected: {decision}. Stopping.")
                    break

                agent = self.sub_agents[selected_agent_name]
                logging.info(f"  â¤ {selected_agent_name} ì‹¤í–‰ ì¤‘...")

                result = await agent.execute(state, kernel)

                # ğŸ†• ì‹¤í–‰ ë¡œê·¸ ê¸°ë¡
                execution_record = {
                    "round": current_round,
                    "agent": selected_agent_name,
                    "output": result.output,
                    "success": result.success,
                    "duration_ms": result.duration_ms
                }
                self.execution_log.append(execution_record)

                if result.success:
                    response_text = f"[Round {current_round} - {selected_agent_name}]\n{result.output}"
                    responses.append(response_text)
                    # ìƒíƒœì— ì¤‘ê°„ ê²°ê³¼ ì¶”ê°€ (ì„ íƒ ì‚¬í•­)
                    # state.add_message(AgentRole.FUNCTION, result.output, selected_agent_name)

                # Agentê°€ ëª…ì‹œì ìœ¼ë¡œ ì¢…ë£Œ ìš”ì²­í•œ ê²½ìš°
                if "TERMINATE" in result.output.upper():
                    logging.info(f"âœ… ì¡°ê¸° ì¢…ë£Œ ìš”ì²­ by {selected_agent_name}")
                    break

            # ì‘ë‹µ ì§‘ê³„ (SRE Agent íŒ¨í„´)
            if responses and len(responses) > 1:
                aggregated = await self.aggregate_responses(
                    self.execution_log, state, kernel
                )
                final_output = aggregated
            else:
                final_output = "\n\n".join(responses)

            duration_ms = (time.time() - start_time) * 1000

            # ìµœì¢… ìš”ì•½
            summary = f"Supervisor ì‹¤í–‰ ì™„ë£Œ: {current_round}ë¼ìš´ë“œ"
            state.add_message(AgentRole.SUPERVISOR, summary, self.name)

            # Memory Hook ì €ì¥ (ìˆëŠ” ê²½ìš°)
            if self.memory_hook:
                await self.memory_hook.on_message_added(
                    content=final_output,
                    role="ASSISTANT",
                    agent_name=self.name
                )

            return NodeResult(
                node_name=self.name,
                output=final_output,
                success=True,
                duration_ms=duration_ms,
                metadata={
                    "rounds": current_round,
                    "agents": len(self.sub_agents),
                    "execution_log": self.execution_log,
                    "investigation_plan": plan.to_dict() if plan else None
                }
            )
        except Exception as e:
            logging.error(f"âŒ Supervisor ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return NodeResult(
                node_name=self.name,
                output="",
                success=False,
                error=str(e)
            )


# ============================================================================
# ê·¸ë˜í”„ ê¸°ë°˜ ì›Œí¬í”Œë¡œìš° - í–¥ìƒëœ ë²„ì „
# ============================================================================

class Node:
    """
    ì›Œí¬í”Œë¡œìš° ë…¸ë“œ

    [ìˆ˜ì •] condition_func íŒŒë¼ë¯¸í„° ì¶”ê°€
    - ì¡°ê±´ë¶€ ë¼ìš°íŒ… ì§€ì› (LangGraph íŒ¨í„´)
    """

    def __init__(self, name: str, agent: Agent,
                 edges: Optional[Dict[str, str]] = None,
                 condition_func: Optional[Callable] = None):  # ğŸ†• ì¡°ê±´ í•¨ìˆ˜
        self.name = name
        self.agent = agent
        self.edges = edges or {}
        self.condition_func = condition_func
        self.execution_count = 0  # ğŸ†• ì‹¤í–‰ íšŸìˆ˜ ì¶”ì 

    async def execute(self, state: AgentState, kernel: Kernel) -> NodeResult:
        logging.info(f"ğŸ“ ë…¸ë“œ ì‹¤í–‰: {self.name} (#{self.execution_count + 1})")

        result = await self.agent.execute(state, kernel)
        self.execution_count += 1

        # ğŸ†• ì¡°ê±´ë¶€ ë¼ìš°íŒ…
        if not result.next_node and self.edges:
            if self.condition_func:
                # ì¡°ê±´ í•¨ìˆ˜ë¡œ ë‹¤ìŒ ë…¸ë“œ ê²°ì •
                next_node = await self.condition_func(state, result)
                result.next_node = self.edges.get(next_node, self.edges.get("default"))
            else:
                result.next_node = self.edges.get("default", None)

        state.visited_nodes.append(self.name)
        return result


class Graph:
    """
    ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ - ì¡°ê±´ë¶€ ë¼ìš°íŒ… ë° ë£¨í”„ ì§€ì›

    [ìˆ˜ì •] ì—¬ëŸ¬ ê¸°ëŠ¥ ì¶”ê°€:
    1. loop_nodes: ë£¨í”„ ê°€ëŠ¥í•œ ë…¸ë“œ ì§‘í•©
    2. add_conditional_edge(): ì¡°ê±´ë¶€ ì—£ì§€ ì¶”ê°€
    3. ë¬´í•œ ë£¨í”„ ë°©ì§€ ë¡œì§
    4. ìƒì„¸í•œ ì‹¤í–‰ ë¡œê·¸
    5. get_statistics(): ê·¸ë˜í”„ í†µê³„
    """

    def __init__(self, name: str = "workflow"):
        self.name = name
        self.nodes: Dict[str, Node] = {}
        self.start_node: Optional[str] = None
        self.end_nodes: Set[str] = set()
        self.loop_nodes: Set[str] = set()  # ğŸ†• ë£¨í”„ ê°€ëŠ¥ ë…¸ë“œ

    def add_node(self, node: Node, allow_loop: bool = False):  # ğŸ†• allow_loop íŒŒë¼ë¯¸í„°
        """
        ë…¸ë“œ ì¶”ê°€

        [ìˆ˜ì •] allow_loop íŒŒë¼ë¯¸í„°ë¡œ ë£¨í”„ í—ˆìš© ì—¬ë¶€ ì§€ì •
        """
        self.nodes[node.name] = node
        if allow_loop:
            self.loop_nodes.add(node.name)
        logging.info(f"âœ… ë…¸ë“œ ì¶”ê°€: {node.name}")

    def add_edge(self, from_node: str, to_node: str, condition: str = "default"):
        if from_node not in self.nodes:
            raise ValueError(f"ë…¸ë“œ '{from_node}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        self.nodes[from_node].edges[condition] = to_node
        logging.info(f"âœ… ì—£ì§€ ì¶”ê°€: {from_node} --[{condition}]--> {to_node}")

    def add_conditional_edge(self, from_node: str, condition_func: Callable):
        """
        ì¡°ê±´ë¶€ ì—£ì§€ ì¶”ê°€

        [ì‹ ê·œ] LangGraphì˜ ì¡°ê±´ë¶€ ë¼ìš°íŒ… íŒ¨í„´

        ì‚¬ìš© ì˜ˆì‹œ:
        async def route_by_complexity(state, result):
            if "simple" in result.output.lower():
                return "simple"
            return "complex"

        graph.add_conditional_edge("analyzer", route_by_complexity)
        """
        if from_node not in self.nodes:
            raise ValueError(f"ë…¸ë“œ '{from_node}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        self.nodes[from_node].condition_func = condition_func
        logging.info(f"âœ… ì¡°ê±´ë¶€ ì—£ì§€ ì¶”ê°€: {from_node}")

    def set_start(self, node_name: str):
        self.start_node = node_name
        logging.info(f"âœ… ì‹œì‘ ë…¸ë“œ: {node_name}")

    def set_end(self, node_name: str):
        self.end_nodes.add(node_name)
        logging.info(f"âœ… ì¢…ë£Œ ë…¸ë“œ: {node_name}")

    async def execute(self, state: AgentState, kernel: Kernel,
                     max_iterations: int = 10) -> AgentState:
        """
        ê·¸ë˜í”„ ì‹¤í–‰

        [ìˆ˜ì •] ê°œì„ ì‚¬í•­:
        1. ìŠ¹ì¸ ëŒ€ê¸° ì²˜ë¦¬
        2. ë¬´í•œ ë£¨í”„ ë°©ì§€ (loop_nodes ì²´í¬)
        3. ìƒì„¸í•œ ë¡œê·¸ ì¶œë ¥
        4. ì‹¤í–‰ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        """
        if not self.start_node:
            raise ValueError("ì‹œì‘ ë…¸ë“œê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        current_node = self.start_node
        iterations = 0

        logging.info(f"\n{'='*60}")
        logging.info(f"ğŸš€ ì›Œí¬í”Œë¡œìš° ì‹œì‘: {self.name}")
        logging.info(f"{'='*60}")
        state.execution_status = ExecutionStatus.RUNNING

        while current_node and iterations < max_iterations:
            iterations += 1
            state.current_node = current_node

            logging.info(f"\nâ–¶ï¸ Iteration {iterations}: {current_node}")

            node = self.nodes.get(current_node)
            if not node:
                logging.error(f"âŒ ë…¸ë“œ '{current_node}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                state.execution_status = ExecutionStatus.FAILED
                break

            # ğŸ†• ë¬´í•œ ë£¨í”„ ë°©ì§€ (ê°™ì€ ë…¸ë“œ ì¬ë°©ë¬¸ ì²´í¬)
            if current_node in state.visited_nodes and current_node not in self.loop_nodes:
                logging.warning(f"âš ï¸ ë…¸ë“œ ì¬ë°©ë¬¸ ê°ì§€: {current_node}")

            result = await node.execute(state, kernel)
            state.metadata[f"{current_node}_result"] = result.model_dump()

            # ğŸ†• ìŠ¹ì¸ ëŒ€ê¸° ì²˜ë¦¬
            if result.requires_approval:
                logging.info(f"â¸ï¸ ìŠ¹ì¸ ëŒ€ê¸°: {current_node}")
                state.execution_status = ExecutionStatus.WAITING_APPROVAL
                return state

            if not result.success:
                logging.error(f"âŒ ë…¸ë“œ ì‹¤í–‰ ì‹¤íŒ¨: {result.error}")
                state.execution_status = ExecutionStatus.FAILED
                break

            # ì¢…ë£Œ ì¡°ê±´
            if current_node in self.end_nodes:
                logging.info(f"\n{'='*60}")
                logging.info(f"âœ… ì›Œí¬í”Œë¡œìš° ì™„ë£Œ: {self.name}")
                logging.info(f"{'='*60}")
                state.execution_status = ExecutionStatus.COMPLETED
                break

            current_node = result.next_node

            if not current_node:
                state.execution_status = ExecutionStatus.COMPLETED
                break

        if iterations >= max_iterations:
            logging.warning(f"âš ï¸ ìµœëŒ€ ë°˜ë³µ ë„ë‹¬ ({max_iterations})")
            state.execution_status = ExecutionStatus.FAILED

        # ğŸ†• ì‹¤í–‰ í†µê³„
        state.metrics["total_iterations"] = iterations
        state.metrics["visited_nodes"] = len(state.visited_nodes)
        state.metrics["workflow_name"] = self.name

        return state

    def visualize(self) -> str:
        """
        ê·¸ë˜í”„ ì‹œê°í™” (Mermaid í˜•ì‹)

        [ìˆ˜ì •] loop_nodes í‘œì‹œ ê°œì„ 
        """
        lines = []
        lines.append("```")
        lines.append("graph TD")

        # ë…¸ë“œ ì •ì˜
        for node_name, node in self.nodes.items():
            if node_name == self.start_node:
                shape = f"{node_name}([ğŸ¬ START: {node_name}])"
            elif node_name in self.end_nodes:
                shape = f"{node_name}[ğŸ END: {node_name}]"
            elif node_name in self.loop_nodes:  # ğŸ†• ë£¨í”„ ë…¸ë“œ í‘œì‹œ
                shape = f"{node_name}{{ğŸ”„ {node_name}}}"
            else:
                shape = f"{node_name}[{node_name}]"

            lines.append(f"    {shape}")

        # ì—£ì§€ ì •ì˜
        for node_name, node in self.nodes.items():
            for condition, target in node.edges.items():
                if condition == "default":
                    lines.append(f"    {node_name} --> {target}")
                else:
                    lines.append(f"    {node_name} -->|{condition}| {target}")

        lines.append("```")
        return "\n".join(lines)

    def get_statistics(self) -> Dict[str, Any]:
        """
        ê·¸ë˜í”„ í†µê³„

        [ì‹ ê·œ] ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ í†µê³„
        """
        return {
            "name": self.name,
            "total_nodes": len(self.nodes),
            "start_node": self.start_node,
            "end_nodes": list(self.end_nodes),
            "loop_nodes": list(self.loop_nodes),
            "total_edges": sum(len(node.edges) for node in self.nodes.values()),
            "node_execution_counts": {
                name: node.execution_count
                for name, node in self.nodes.items()
            }
        }


# ============================================================================
# ìƒíƒœ ê´€ë¦¬ - í–¥ìƒëœ ë²„ì „
# ============================================================================

class StateManager:
    """
    ìƒíƒœ ê´€ë¦¬ì - ë²„ì „ ê´€ë¦¬ ë° ë¡¤ë°± ì§€ì›

    [ìˆ˜ì •] ì—¬ëŸ¬ ê¸°ëŠ¥ ì¶”ê°€:
    1. ë²„ì „ ê´€ë¦¬ (state_versions)
    2. load_state(version): íŠ¹ì • ë²„ì „ ë¡œë“œ
    3. save_checkpoint(tag): íƒœê·¸ì™€ í•¨ê»˜ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    4. restore_checkpoint(tag): íŠ¹ì • íƒœê·¸ ë³µì›
    5. list_checkpoints(): ì²´í¬í¬ì¸íŠ¸ ëª©ë¡
    6. rollback(steps): ì´ì „ ìƒíƒœë¡œ ë¡¤ë°±
    """

    def __init__(self, memory_store: MemoryStore, checkpoint_dir: Optional[str] = None):
        self.memory_store = memory_store
        self.checkpoint_dir = checkpoint_dir
        self.state_versions: Dict[str, List[str]] = defaultdict(list)  # ğŸ†• ë²„ì „ ì¶”ì 

        if checkpoint_dir and not os.path.exists(checkpoint_dir):
            os.makedirs(checkpoint_dir)

    async def save_state(self, state: AgentState):
        """
        ìƒíƒœ ì €ì¥

        [ìˆ˜ì •] ë²„ì „ ì¶”ì  ì¶”ê°€
        """
        state_dict = state.model_dump()
        await self.memory_store.save(f"state:{state.session_id}", state_dict)

        # ğŸ†• ë²„ì „ ì¶”ì 
        version_key = f"state:{state.session_id}:v{len(self.state_versions[state.session_id])}"
        await self.memory_store.save(version_key, state_dict)
        self.state_versions[state.session_id].append(version_key)

    async def load_state(self, session_id: str, version: Optional[int] = None) -> Optional[AgentState]:
        """
        ìƒíƒœ ë¡œë“œ (íŠ¹ì • ë²„ì „ ì§€ì›)

        [ìˆ˜ì •] version íŒŒë¼ë¯¸í„° ì¶”ê°€
        """
        if version is not None:
            # ğŸ†• íŠ¹ì • ë²„ì „ ë¡œë“œ
            version_key = f"state:{session_id}:v{version}"
            data = await self.memory_store.load(version_key)
        else:
            # ìµœì‹  ë²„ì „ ë¡œë“œ
            data = await self.memory_store.load(f"state:{session_id}")

        if data:
            return AgentState(**data)
        return None

    async def save_checkpoint(self, state: AgentState, tag: Optional[str] = None) -> str:
        """
        ì²´í¬í¬ì¸íŠ¸ ì €ì¥

        [ìˆ˜ì •] tag íŒŒë¼ë¯¸í„° ì¶”ê°€
        """
        if not self.checkpoint_dir:
            raise ValueError("ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ ë¯¸ì„¤ì •")

        timestamp = datetime.now(timezone.utc).isoformat().replace(':', '-').replace('.', '-')
        tag_suffix = f"_{tag}" if tag else ""  # ğŸ†• íƒœê·¸ ì ‘ë¯¸ì‚¬
        checkpoint_file = os.path.join(
            self.checkpoint_dir,
            f"{state.session_id}_{timestamp}{tag_suffix}.json"
        )

        with open(checkpoint_file, 'w', encoding='utf-8') as f:
            json.dump(state.model_dump(), f, ensure_ascii=False, indent=2)

        logging.info(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_file}")
        return checkpoint_file

    async def restore_checkpoint(self, session_id: str, tag: Optional[str] = None) -> Optional[AgentState]:
        """
        ì²´í¬í¬ì¸íŠ¸ ë³µì›

        [ìˆ˜ì •] tag íŒŒë¼ë¯¸í„° ì¶”ê°€
        """
        if not self.checkpoint_dir:
            return None

        checkpoints = [
            f for f in os.listdir(self.checkpoint_dir)
            if f.startswith(session_id) and f.endswith('.json')
        ]

        # ğŸ†• íƒœê·¸ í•„í„°ë§
        if tag:
            checkpoints = [f for f in checkpoints if tag in f]

        if not checkpoints:
            return None

        latest = os.path.join(self.checkpoint_dir, sorted(checkpoints)[-1])

        with open(latest, 'r', encoding='utf-8') as f:
            data = json.load(f)

        logging.info(f"ğŸ“‚ ì²´í¬í¬ì¸íŠ¸ ë³µì›: {latest}")
        return AgentState(**data)

    async def list_checkpoints(self, session_id: str) -> List[str]:
        """
        ì²´í¬í¬ì¸íŠ¸ ëª©ë¡

        [ì‹ ê·œ] ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ ëª©ë¡ ì¡°íšŒ
        """
        if not self.checkpoint_dir or not os.path.exists(self.checkpoint_dir):
            return []

        checkpoints = [
            f for f in os.listdir(self.checkpoint_dir)
            if f.startswith(session_id) and f.endswith('.json')
        ]
        return sorted(checkpoints)

    async def rollback(self, session_id: str, steps: int = 1) -> Optional[AgentState]:
        """
        ì´ì „ ìƒíƒœë¡œ ë¡¤ë°±

        [ì‹ ê·œ] ë²„ì „ ê¸°ë°˜ ë¡¤ë°±

        ì‚¬ìš© ì˜ˆì‹œ:
        # 1ë‹¨ê³„ ì´ì „ìœ¼ë¡œ ë¡¤ë°±
        state = await state_manager.rollback(session_id, steps=1)

        # 3ë‹¨ê³„ ì´ì „ìœ¼ë¡œ ë¡¤ë°±
        state = await state_manager.rollback(session_id, steps=3)
        """
        versions = self.state_versions.get(session_id, [])
        if len(versions) < steps:
            logging.warning(f"âš ï¸ ë¡¤ë°± ë¶ˆê°€: {steps}ë‹¨ê³„ ì´ì „ ë²„ì „ ì—†ìŒ")
            return None

        target_version = len(versions) - steps - 1
        return await self.load_state(session_id, version=target_version)


# ============================================================================
# í†µí•© í”„ë ˆì„ì›Œí¬ - Enterprise Edition
# ============================================================================

class UnifiedAgentFramework:
    """
    í†µí•© Agent í”„ë ˆì„ì›Œí¬ - Enterprise Edition

    ê°„í¸í•œ ì‚¬ìš©ë²•:
    ```python
    # 1. ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²• (í™˜ê²½ë³€ìˆ˜ì—ì„œ ìë™ ë¡œë“œ)
    framework = UnifiedAgentFramework.create()

    # 2. ì„¤ì • ê°ì²´ ì‚¬ìš©
    config = FrameworkConfig.from_env()
    framework = UnifiedAgentFramework.create(config)

    # 3. ë¹ ë¥¸ ì§ˆì˜ì‘ë‹µ
    response = await framework.quick_chat("ì•ˆë…•í•˜ì„¸ìš”!")

    # 4. ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
    state = await framework.run("session-1", "simple_chat", "ì§ˆë¬¸ì…ë‹ˆë‹¤")

    # 5. Skills ê¸°ë°˜ ì—ì´ì „íŠ¸ (NEW!)
    agent = framework.create_skilled_agent("coder", skills=["python-expert"])
    ```

    ì£¼ìš” ê¸°ëŠ¥:
    - MCP ë„êµ¬ ê´€ë¦¬
    - ì´ë²¤íŠ¸ ì‹œìŠ¤í…œ (Pub-Sub)
    - ì „ì—­ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
    - ì²´í¬í¬ì¸íŠ¸ ë° ë¡¤ë°±
    - Human-in-the-loop ìŠ¹ì¸
    - Skills ì‹œìŠ¤í…œ (Anthropic íŒ¨í„´)
    """

    def __init__(
        self,
        kernel: Kernel,
        config: Optional[FrameworkConfig] = None,
        memory_store: Optional[MemoryStore] = None,
        checkpoint_dir: str = "./checkpoints",
        enable_telemetry: bool = True,
        enable_events: bool = True,
        skill_dirs: Optional[List[str]] = None,  # Skills ë””ë ‰í† ë¦¬
        load_builtin_skills: bool = True  # ê¸°ë³¸ ìŠ¤í‚¬ ë¡œë“œ
    ):
        self.kernel = kernel
        self.config = config or FrameworkConfig()
        self.memory_store = memory_store or CachedMemoryStore(max_cache_size=self.config.max_cache_size)
        self.state_manager = StateManager(self.memory_store, checkpoint_dir)
        self.graphs: Dict[str, Graph] = {}
        self.mcp_tools: Dict[str, MCPTool] = {}
        self.event_bus = EventBus() if enable_events else None

        # Skills ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.skill_manager = SkillManager(skill_dirs)
        if load_builtin_skills:
            self._load_builtin_skills()

        if enable_telemetry:
            self.tracer = trace.get_tracer(__name__)
        else:
            self.tracer = None

        self.global_metrics = {
            "total_workflows": 0,
            "total_executions": 0,
            "total_failures": 0,
            "start_time": datetime.now(timezone.utc).isoformat()
        }

    def _load_builtin_skills(self):
        """
        ê¸°ë³¸ ì œê³µ ìŠ¤í‚¬ ë¡œë“œ (SKILL.md íŒŒì¼ ê¸°ë°˜)

        skills/ ë””ë ‰í† ë¦¬ì—ì„œ SKILL.md íŒŒì¼ì„ ì½ì–´ ìŠ¤í‚¬ì„ ë¡œë“œí•©ë‹ˆë‹¤.
        """
        if BUILTIN_SKILLS_DIR.exists():
            loaded = self.skill_manager.load_skills_from_directory(str(BUILTIN_SKILLS_DIR))
            logging.info(f"ğŸ“š SKILL.md ê¸°ë°˜ ìŠ¤í‚¬ {loaded}ê°œ ë¡œë“œ ì™„ë£Œ (from {BUILTIN_SKILLS_DIR})")
        else:
            logging.warning(f"âš ï¸ ê¸°ë³¸ ìŠ¤í‚¬ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {BUILTIN_SKILLS_DIR}")
            logging.info("ğŸ’¡ 'skills' ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í•˜ê³  SKILL.md íŒŒì¼ì„ ì¶”ê°€í•˜ì„¸ìš”.")

    @classmethod
    def create(
        cls,
        config: Optional[FrameworkConfig] = None,
        skill_dirs: Optional[List[str]] = None,
        load_builtin_skills: bool = True
    ) -> 'UnifiedAgentFramework':
        """
        í”„ë ˆì„ì›Œí¬ ê°„í¸ ìƒì„± (ê¶Œì¥)

        ì‚¬ìš©ë²•:
        ```python
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ ìë™ ë¡œë“œ
        framework = UnifiedAgentFramework.create()

        # ì»¤ìŠ¤í…€ ì„¤ì • + ìŠ¤í‚¬ ë””ë ‰í† ë¦¬
        framework = UnifiedAgentFramework.create(
            skill_dirs=["./my_skills", "./team_skills"]
        )
        ```
        """
        if config is None:
            config = FrameworkConfig.from_env()

        config.validate()

        # Kernel ì´ˆê¸°í™”
        kernel = Kernel()
        chat_service = AzureChatCompletion(
            deployment_name=config.deployment_name,
            api_key=config.api_key,
            endpoint=config.endpoint,
            service_id=config.deployment_name,  # deployment_nameê³¼ ë™ì¼í•˜ê²Œ ì„¤ì •
            api_version=config.api_version
        )
        kernel.add_service(chat_service)

        return cls(
            kernel=kernel,
            config=config,
            checkpoint_dir=config.checkpoint_dir,
            enable_telemetry=config.enable_telemetry,
            enable_events=config.enable_events,
            skill_dirs=skill_dirs,
            load_builtin_skills=load_builtin_skills
        )

    async def quick_chat(self, message: str, system_prompt: str = "You are a helpful assistant.") -> str:
        """
        ë¹ ë¥¸ ì§ˆì˜ì‘ë‹µ (ì›Œí¬í”Œë¡œìš° ì—†ì´)

        ì‚¬ìš©ë²•:
        ```python
        response = await framework.quick_chat("íŒŒì´ì¬ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?")
        print(response)
        ```
        """
        # ì„ì‹œ ì›Œí¬í”Œë¡œìš°ê°€ ì—†ìœ¼ë©´ ìƒì„±
        if "_quick_chat" not in self.graphs:
            self.create_simple_workflow("_quick_chat", system_prompt)

        session_id = f"quick-{int(time.time())}"
        state = await self.run(session_id, "_quick_chat", message)

        # ë§ˆì§€ë§‰ ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ë°˜í™˜
        for msg in reversed(state.messages):
            if msg.role == AgentRole.ASSISTANT:
                return msg.content
        return ""

    def create_simple_workflow(self, name: str, system_prompt: str = "You are a helpful assistant.") -> Graph:
        """
        ê°„ë‹¨í•œ ëŒ€í™” ì›Œí¬í”Œë¡œìš° ìƒì„±

        ì‚¬ìš©ë²•:
        ```python
        workflow = framework.create_simple_workflow("my_assistant", "ë„ˆëŠ” í•œêµ­ì–´ ì„ ìƒë‹˜ì´ì•¼.")
        ```
        """
        graph = self.create_graph(name)

        agent = SimpleAgent(
            name="assistant",
            system_prompt=system_prompt,
            model=self.config.model,
            temperature=self.config.temperature,
            max_tokens=self.config.max_tokens,
            enable_streaming=self.config.enable_streaming,
            event_bus=self.event_bus,
            service_id=self.config.deployment_name  # ğŸ†• deployment_name ì‚¬ìš©
        )

        graph.add_node(Node("assistant", agent))
        graph.set_start("assistant")
        graph.set_end("assistant")

        return graph

    def create_router_workflow(
        self,
        name: str,
        routes: Dict[str, Dict[str, str]]
    ) -> Graph:
        """
        ë¼ìš°íŒ… ì›Œí¬í”Œë¡œìš° ìƒì„±

        ì‚¬ìš©ë²•:
        ```python
        workflow = framework.create_router_workflow(
            "customer_service",
            routes={
                "order": {"prompt": "ì£¼ë¬¸ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                "support": {"prompt": "ê¸°ìˆ  ì§€ì› ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                "general": {"prompt": "ì¼ë°˜ ìƒë‹´ì›ì…ë‹ˆë‹¤."}
            }
        )
        ```
        """
        graph = self.create_graph(name)

        # ë¼ìš°í„° ìƒì„±
        router = RouterAgent(
            name="router",
            system_prompt="Classify user intent accurately.",
            model=self.config.model,
            routes={k: f"{k}_agent" for k in routes.keys()},
            event_bus=self.event_bus,
            service_id=self.config.deployment_name  # ğŸ†• deployment_name ì‚¬ìš©
        )
        graph.add_node(Node("router", router))
        graph.set_start("router")

        # ê° ë¼ìš°íŠ¸ë³„ ì—ì´ì „íŠ¸ ìƒì„±
        for route_name, route_config in routes.items():
            agent = SimpleAgent(
                name=f"{route_name}_agent",
                system_prompt=route_config.get("prompt", f"You handle {route_name} inquiries."),
                model=self.config.model,
                event_bus=self.event_bus,
                service_id=self.config.deployment_name  # ğŸ†• deployment_name ì‚¬ìš©
            )
            graph.add_node(Node(f"{route_name}_agent", agent))
            graph.set_end(f"{route_name}_agent")

        return graph

    def create_skilled_agent(
        self,
        name: str,
        skills: Optional[List[str]] = None,
        base_prompt: str = "",
        auto_detect_skills: bool = True
    ) -> SimpleAgent:
        """
        Skills ê¸°ë°˜ ì—ì´ì „íŠ¸ ìƒì„±

        ì‚¬ìš©ë²•:
        ```python
        # íŠ¹ì • ìŠ¤í‚¬ ì§€ì •
        agent = framework.create_skilled_agent(
            "coder",
            skills=["python-expert", "api-developer"]
        )

        # ìë™ ìŠ¤í‚¬ ê°ì§€ (ì¿¼ë¦¬ ê¸°ë°˜)
        agent = framework.create_skilled_agent(
            "assistant",
            auto_detect_skills=True
        )
        ```
        """
        # ìŠ¤í‚¬ ê°€ì ¸ì˜¤ê¸°
        skill_objects = []
        if skills:
            for skill_name in skills:
                skill = self.skill_manager.get_skill(skill_name)
                if skill:
                    skill_objects.append(skill)
                else:
                    logging.warning(f"ìŠ¤í‚¬ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {skill_name}")

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        system_prompt = self.skill_manager.build_system_prompt(
            skill_objects,
            base_prompt=base_prompt,
            include_full=True
        )

        agent = SimpleAgent(
            name=name,
            system_prompt=system_prompt,
            model=self.config.model,
            temperature=self.config.temperature,
            max_tokens=self.config.max_tokens,
            enable_streaming=self.config.enable_streaming,
            event_bus=self.event_bus,
            service_id=self.config.deployment_name  # ğŸ†• deployment_name ì‚¬ìš©
        )

        # ìë™ ìŠ¤í‚¬ ê°ì§€ ë©”íƒ€ë°ì´í„° ì¶”ê°€
        agent._auto_detect_skills = auto_detect_skills
        agent._skill_manager = self.skill_manager

        return agent

    def create_skill_workflow(
        self,
        name: str,
        skills: List[str],
        base_prompt: str = "You are a helpful assistant."
    ) -> Graph:
        """
        Skills ê¸°ë°˜ ì›Œí¬í”Œë¡œìš° ìƒì„±

        ì‚¬ìš©ë²•:
        ```python
        workflow = framework.create_skill_workflow(
            "data_pipeline",
            skills=["python-expert", "data-analyst"],
            base_prompt="ë°ì´í„° ì²˜ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
        )
        ```
        """
        graph = self.create_graph(name)

        agent = self.create_skilled_agent(
            name="skilled_assistant",
            skills=skills,
            base_prompt=base_prompt
        )

        graph.add_node(Node("skilled_assistant", agent))
        graph.set_start("skilled_assistant")
        graph.set_end("skilled_assistant")

        return graph

    async def smart_chat(
        self,
        message: str,
        base_prompt: str = "You are a helpful assistant.",
        max_skills: int = 2
    ) -> str:
        """
        ìŠ¤ë§ˆíŠ¸ ì§ˆì˜ì‘ë‹µ - ì¿¼ë¦¬ì— ë§ëŠ” ìŠ¤í‚¬ ìë™ í™œì„±í™”

        Progressive Disclosure ì ìš©:
        - ë©”ì‹œì§€ ë¶„ì„í•˜ì—¬ ê´€ë ¨ ìŠ¤í‚¬ ìë™ ë§¤ì¹­
        - ë§¤ì¹­ëœ ìŠ¤í‚¬ì˜ ì§€ì¹¨ì„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— í¬í•¨

        ì‚¬ìš©ë²•:
        ```python
        # ìë™ìœ¼ë¡œ python-expert ìŠ¤í‚¬ì´ í™œì„±í™”ë¨
        response = await framework.smart_chat("íŒŒì´ì¬ìœ¼ë¡œ ì›¹ í¬ë¡¤ëŸ¬ ë§Œë“¤ì–´ì¤˜")
        ```
        """
        # ìŠ¤í‚¬ ë§¤ì¹­
        matched_skills = self.skill_manager.match_skills(
            message,
            threshold=0.2,
            max_skills=max_skills
        )

        if matched_skills:
            skill_names = [s.name for s in matched_skills]
            logging.info(f"ğŸ¯ ë§¤ì¹­ëœ ìŠ¤í‚¬: {', '.join(skill_names)}")

        # ë™ì  ì›Œí¬í”Œë¡œìš° ìƒì„±
        workflow_name = f"_smart_chat_{int(time.time())}"
        self.create_skill_workflow(
            workflow_name,
            skills=[s.name for s in matched_skills],
            base_prompt=base_prompt
        )

        session_id = f"smart-{int(time.time())}"
        state = await self.run(session_id, workflow_name, message)

        # ë§ˆì§€ë§‰ ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ë°˜í™˜
        for msg in reversed(state.messages):
            if msg.role == AgentRole.ASSISTANT:
                return msg.content
        return ""

    def create_graph(self, name: str) -> Graph:
        """ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ ìƒì„±"""
        graph = Graph(name)
        self.graphs[name] = graph
        logging.info(f"ğŸ¨ ê·¸ë˜í”„ ìƒì„±: {name}")
        return graph

    def register_mcp_tool(self, tool: MCPTool):
        """
        MCP ë„êµ¬ ë“±ë¡

        [ì‹ ê·œ] MCP ì„œë²„ ì—°ë™
        """
        self.mcp_tools[tool.name] = tool
        logging.info(f"ğŸ”§ MCP ë„êµ¬ ë“±ë¡: {tool.name}")

    async def run(
        self,
        session_id: str,
        workflow_name: str,
        user_message: str = "",
        restore_from_checkpoint: bool = False,
        checkpoint_tag: Optional[str] = None  # ğŸ†• íƒœê·¸ ì§€ì›
    ) -> AgentState:
        """
        ì›Œí¬í”Œë¡œìš° ì‹¤í–‰

        [ìˆ˜ì •] ê°œì„ ì‚¬í•­:
        1. checkpoint_tag íŒŒë¼ë¯¸í„° ì¶”ê°€
        2. ì‹¤í–‰ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        3. ìë™ ì²´í¬í¬ì¸íŠ¸ (ì™„ë£Œ ì‹œ)
        4. ì—ëŸ¬ í•¸ë“¤ë§ ê°•í™”
        """

        # ìƒíƒœ ë³µì›
        if restore_from_checkpoint:
            state = await self.state_manager.restore_checkpoint(session_id, tag=checkpoint_tag)
            if not state:
                logging.warning("âš ï¸ ì²´í¬í¬ì¸íŠ¸ ë³µì› ì‹¤íŒ¨, ìƒˆ ì„¸ì…˜ ì‹œì‘")
                state = None
        else:
            state = await self.state_manager.load_state(session_id)

        if not state:
            state = AgentState(session_id=session_id, workflow_name=workflow_name)
            logging.info(f"ğŸ†• ìƒˆ ì„¸ì…˜ ì‹œì‘: {session_id}")

        if user_message:
            state.add_message(AgentRole.USER, user_message)
            # ğŸ†• ì´ë²¤íŠ¸ ë°œí–‰
            if self.event_bus:
                await self.event_bus.publish(AgentEvent(
                    event_type=EventType.MESSAGE_RECEIVED,
                    data={"content": user_message}
                ))

        graph = self.graphs.get(workflow_name)
        if not graph:
            raise ValueError(f"ì›Œí¬í”Œë¡œìš° '{workflow_name}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # ì‹¤í–‰
        start_time = time.time()
        self.global_metrics["total_executions"] += 1

        try:
            if self.tracer:
                with self.tracer.start_as_current_span("workflow_execution") as span:
                    span.set_attribute("session_id", session_id)
                    span.set_attribute("workflow_name", workflow_name)
                    state = await graph.execute(state, self.kernel)
                    span.set_attribute("status", state.execution_status.value)
                    span.set_attribute("iterations", state.metrics.get("total_iterations", 0))
            else:
                state = await graph.execute(state, self.kernel)

            # ğŸ†• ì‹¤í–‰ ë©”íŠ¸ë¦­ ì €ì¥
            execution_time = (time.time() - start_time) * 1000
            state.metrics["execution_time_ms"] = execution_time
            state.metrics["success"] = state.execution_status == ExecutionStatus.COMPLETED

        except Exception as e:
            logging.error(f"âŒ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            self.global_metrics["total_failures"] += 1
            state.execution_status = ExecutionStatus.FAILED
            state.metadata["error"] = str(e)

        # ìƒíƒœ ì €ì¥
        await self.state_manager.save_state(state)

        # ğŸ†• ìë™ ì²´í¬í¬ì¸íŠ¸ (ì™„ë£Œ ì‹œ)
        if state.execution_status == ExecutionStatus.COMPLETED:
            await self.state_manager.save_checkpoint(state, tag="auto")

        return state

    async def approve_pending_request(self, session_id: str, request_id: int,
                                     approved: bool) -> AgentState:
        """
        ëŒ€ê¸° ì¤‘ì¸ ìŠ¹ì¸ ìš”ì²­ ì²˜ë¦¬

        [ì‹ ê·œ] Human-in-the-loop ìŠ¹ì¸ ì²˜ë¦¬
        """
        state = await self.state_manager.load_state(session_id)
        if not state:
            raise ValueError(f"ì„¸ì…˜ '{session_id}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        if request_id >= len(state.pending_approvals):
            raise ValueError(f"ìŠ¹ì¸ ìš”ì²­ #{request_id}ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

        approval = state.pending_approvals[request_id]
        approval["status"] = ApprovalStatus.APPROVED if approved else ApprovalStatus.REJECTED
        approval["approved_at"] = datetime.now(timezone.utc).isoformat()

        if approved:
            # ìŠ¹ì¸ë¨ - ì›Œí¬í”Œë¡œìš° ê³„ì† ì‹¤í–‰
            state.execution_status = ExecutionStatus.RUNNING
            if self.event_bus:
                await self.event_bus.publish(AgentEvent(
                    event_type=EventType.APPROVAL_GRANTED,
                    data=approval
                ))
        else:
            # ê±°ë¶€ë¨
            state.execution_status = ExecutionStatus.FAILED
            if self.event_bus:
                await self.event_bus.publish(AgentEvent(
                    event_type=EventType.APPROVAL_DENIED,
                    data=approval
                ))

        await self.state_manager.save_state(state)
        return state

    def visualize_workflow(self, workflow_name: str) -> str:
        """ì›Œí¬í”Œë¡œìš° ì‹œê°í™”"""
        graph = self.graphs.get(workflow_name)
        if not graph:
            return f"âŒ ì›Œí¬í”Œë¡œìš° '{workflow_name}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        return graph.visualize()

    def get_workflow_stats(self, workflow_name: str) -> Dict[str, Any]:
        """
        ì›Œí¬í”Œë¡œìš° í†µê³„

        [ì‹ ê·œ] ê·¸ë˜í”„ ì‹¤í–‰ í†µê³„
        """
        graph = self.graphs.get(workflow_name)
        if not graph:
            return {}
        return graph.get_statistics()

    def get_global_metrics(self) -> Dict[str, Any]:
        """
        ì „ì—­ ë©”íŠ¸ë¦­

        [ì‹ ê·œ] í”„ë ˆì„ì›Œí¬ ì „ì²´ ë©”íŠ¸ë¦­
        """
        return {
            **self.global_metrics,
            "total_workflows": len(self.graphs),
            "total_mcp_tools": len(self.mcp_tools),
            "uptime_seconds": (
                datetime.now(timezone.utc) -
                datetime.fromisoformat(self.global_metrics["start_time"])
            ).total_seconds()
        }

    async def cleanup(self):
        """
        ë¦¬ì†ŒìŠ¤ ì •ë¦¬

        [ì‹ ê·œ] í”„ë ˆì„ì›Œí¬ ì¢…ë£Œ ì‹œ ë¦¬ì†ŒìŠ¤ í•´ì œ
        """
        logging.info("ğŸ§¹ í”„ë ˆì„ì›Œí¬ ì •ë¦¬ ì‹œì‘")

        # MCP ë„êµ¬ ì—°ê²° í•´ì œ
        for tool in self.mcp_tools.values():
            await tool.disconnect()

        logging.info("âœ… í”„ë ˆì„ì›Œí¬ ì •ë¦¬ ì™„ë£Œ")


# ============================================================================
# OpenTelemetry ì„¤ì •
# ============================================================================

def setup_telemetry(service_name: str = "UnifiedAgentFramework",
                   enable_console: bool = False):
    """OpenTelemetry ì„¤ì •"""
    try:
        resource = Resource.create({"service.name": service_name})
        provider = TracerProvider(resource=resource)

        if enable_console:
            processor = BatchSpanProcessor(ConsoleSpanExporter())
            provider.add_span_processor(processor)

        trace.set_tracer_provider(provider)
        logging.info(f"âœ… OpenTelemetry ì„¤ì •: {service_name}")
    except Exception as e:
        logging.warning(f"âš ï¸ OpenTelemetry ì„¤ì • ì‹¤íŒ¨: {e}")


# ============================================================================
# ë°ëª¨ í•¨ìˆ˜ë“¤ - í•™ìŠµìš© 4ê°€ì§€ ë°ëª¨
# ============================================================================

async def demo_simple_chat(framework: UnifiedAgentFramework):
    """
    ë°ëª¨ 1: ë‹¨ìˆœ ëŒ€í™” Agent
    """
    print("\n" + "="*60)
    print("ğŸ“š ë°ëª¨ 1: ë‹¨ìˆœ ëŒ€í™” Agent")
    print("="*60)

    # ê°„í¸ ë©”ì„œë“œ ì‚¬ìš©
    framework.create_simple_workflow(
        "simple_chat",
        "You are a helpful AI assistant. Answer questions clearly and concisely."
    )

    print("\nì›Œí¬í”Œë¡œìš° êµ¬ì¡°:")
    print(framework.visualize_workflow("simple_chat"))


async def demo_routing_workflow(framework: UnifiedAgentFramework):
    """
    ë°ëª¨ 2: ë¼ìš°íŒ… ì›Œí¬í”Œë¡œìš° (ì¸í…íŠ¸ ê¸°ë°˜)
    """
    print("\n" + "="*60)
    print("ğŸ“š ë°ëª¨ 2: ì¸í…íŠ¸ ê¸°ë°˜ ë¼ìš°íŒ…")
    print("="*60)

    # ê°„í¸ ë©”ì„œë“œ ì‚¬ìš©
    framework.create_router_workflow(
        "routing_workflow",
        routes={
            "order": {"prompt": "You are an order specialist. Help with ordering and purchases."},
            "support": {"prompt": "You are a support specialist. Help troubleshoot and resolve issues."},
            "general": {"prompt": "You are a general assistant. Answer various questions."}
        }
    )

    print("\nì›Œí¬í”Œë¡œìš° êµ¬ì¡°:")
    print(framework.visualize_workflow("routing_workflow"))


async def demo_supervisor_workflow(framework: UnifiedAgentFramework):
    """
    ë°ëª¨ 3: Supervisor íŒ¨í„´ (ë©€í‹° ì—ì´ì „íŠ¸ í˜‘ì—…)
    """
    print("\n" + "="*60)
    print("ğŸ“š ë°ëª¨ 3: Supervisor Multi-Agent í˜‘ì—…")
    print("="*60)

    graph = framework.create_graph("supervisor_workflow")
    config = framework.config

    # Sub-agents
    research_agent = SimpleAgent(
        name="researcher",
        system_prompt="You are a research specialist. Gather and analyze information.",
        model=config.model,
        event_bus=framework.event_bus,
        service_id=config.deployment_name  # ğŸ†• deployment_name ì‚¬ìš©
    )

    writer_agent = SimpleAgent(
        name="writer",
        system_prompt="You are a content writer. Create clear, engaging content.",
        model=config.model,
        event_bus=framework.event_bus,
        service_id=config.deployment_name  # ğŸ†• deployment_name ì‚¬ìš©
    )

    # Supervisor
    supervisor = SupervisorAgent(
        name="supervisor",
        system_prompt="Coordinate research and writing tasks.",
        model=config.model,
        sub_agents=[research_agent, writer_agent],
        max_rounds=2,
        event_bus=framework.event_bus,
        service_id=config.deployment_name  # ğŸ†• deployment_name ì‚¬ìš©
    )

    graph.add_node(Node("supervisor", supervisor))
    graph.set_start("supervisor")
    graph.set_end("supervisor")

    print("\nì›Œí¬í”Œë¡œìš° êµ¬ì¡°:")
    print(framework.visualize_workflow("supervisor_workflow"))


async def demo_conditional_workflow(framework: UnifiedAgentFramework):
    """
    ë°ëª¨ 4: ì¡°ê±´ë¶€ ë¼ìš°íŒ… (ë³µì¡ë„ ê¸°ë°˜ ë¶„ê¸°)
    """
    print("\n" + "="*60)
    print("ğŸ“š ë°ëª¨ 4: ì¡°ê±´ë¶€ ë¼ìš°íŒ… ë° ë£¨í”„")
    print("="*60)

    graph = framework.create_graph("conditional_workflow")
    config = framework.config

    # Agents
    analyzer = SimpleAgent(
        name="analyzer",
        system_prompt="Analyze the complexity of the user's question. Respond with SIMPLE or COMPLEX.",
        model=config.model,
        event_bus=framework.event_bus,
        service_id=config.deployment_name  # ğŸ†• deployment_name ì‚¬ìš©
    )

    simple_handler = SimpleAgent(
        name="simple_handler",
        system_prompt="Answer simple questions directly and briefly.",
        model=config.model,
        event_bus=framework.event_bus,
        service_id=config.deployment_name  # ğŸ†• deployment_name ì‚¬ìš©
    )

    complex_handler = SimpleAgent(
        name="complex_handler",
        system_prompt="Provide detailed, comprehensive answers to complex questions.",
        model=config.model,
        max_tokens=2000,
        event_bus=framework.event_bus,
        service_id=config.deployment_name  # ğŸ†• deployment_name ì‚¬ìš©
    )

    # ì¡°ê±´ë¶€ ë¼ìš°íŒ… í•¨ìˆ˜
    async def route_by_complexity(state: AgentState, result: NodeResult) -> str:
        """ë³µì¡ë„ì— ë”°ë¼ ë¼ìš°íŒ…"""
        output_lower = result.output.lower()
        return "simple" if "simple" in output_lower else "complex"

    # Build Graph
    analyzer_node = Node(
        "analyzer",
        analyzer,
        edges={"simple": "simple_handler", "complex": "complex_handler"}
    )
    analyzer_node.condition_func = route_by_complexity

    graph.add_node(analyzer_node)
    graph.add_node(Node("simple_handler", simple_handler))
    graph.add_node(Node("complex_handler", complex_handler))

    graph.set_start("analyzer")
    graph.set_end("simple_handler")
    graph.set_end("complex_handler")

    print("\nì›Œí¬í”Œë¡œìš° êµ¬ì¡°:")
    print(framework.visualize_workflow("conditional_workflow"))


# ============================================================================
# ë©”ì¸ í•¨ìˆ˜ - í–¥ìƒëœ CLI
# ============================================================================

async def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - ì¸í„°ë™í‹°ë¸Œ ë°ëª¨

    ì‹¤í–‰ ë°©ë²•:
        python Semantic-agent_framework.py

    í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜ (.env íŒŒì¼):
        AZURE_OPENAI_API_KEY=your-api-key
        AZURE_OPENAI_ENDPOINT =https://your-endpoint.openai.azure.com/
        AZURE_OPENAI_DEPLOYMENT=your-deployment-name
    """
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler("agent_framework.log", encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    logging.getLogger("httpx").setLevel(logging.WARNING)
    logging.getLogger("semantic_kernel").setLevel(logging.WARNING)

    # OpenTelemetry ì„¤ì •
    setup_telemetry("UnifiedAgentFramework-Enterprise", enable_console=False)

    print("\n" + "="*60)
    print("ğŸš€ Unified Agent Framework - Enterprise Edition")
    print("="*60)

    try:
        # í”„ë ˆì„ì›Œí¬ ê°„í¸ ìƒì„±
        framework = UnifiedAgentFramework.create()
        config = framework.config

        print(f"âœ… ì—”ë“œí¬ì¸íŠ¸: {config.endpoint}")
        print(f"âœ… ëª¨ë¸: {config.deployment_name}")
        print("="*60)

    except ValueError as e:
        print(str(e))
        print("\nğŸ’¡ .env íŒŒì¼ ì˜ˆì‹œ:")
        print("OPEN_AI_KEY_5=your-api-key")
        print("OPEN_AI_ENDPOINT_5=https://your-endpoint.openai.azure.com/")
        print("AZURE_OPENAI_DEPLOYMENT=your-deployment-name")
        return

    # ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ ë“±ë¡
    if framework.event_bus:
        async def log_event(event: AgentEvent):
            logging.info(f"ğŸ“¢ ì´ë²¤íŠ¸: {event.event_type.value} - {event.agent_name or 'System'}")

        framework.event_bus.subscribe(EventType.AGENT_STARTED, log_event)
        framework.event_bus.subscribe(EventType.AGENT_COMPLETED, log_event)
        framework.event_bus.subscribe(EventType.APPROVAL_REQUESTED, log_event)

    # ë°ëª¨ ì›Œí¬í”Œë¡œìš° ìƒì„±
    await demo_simple_chat(framework)
    await demo_routing_workflow(framework)
    await demo_supervisor_workflow(framework)
    await demo_conditional_workflow(framework)

    # ì¸í„°ë™í‹°ë¸Œ ì„¸ì…˜
    print("\n" + "="*60)
    print("ğŸ’¬ ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ")
    print("="*60)
    print("ëª…ë ¹ì–´:")
    print("  exit          - ì¢…ë£Œ")
    print("  quick         - ë¹ ë¥¸ ì§ˆì˜ì‘ë‹µ (ì˜ˆ: quick ì•ˆë…•í•˜ì„¸ìš”)")
    print("  smart         - ìŠ¤í‚¬ ìë™ ê°ì§€ ì§ˆì˜ì‘ë‹µ (ì˜ˆ: smart íŒŒì´ì¬ ì½”ë“œ ì‘ì„±)")
    print("  model         - ëª¨ë¸ ë³€ê²½ (ì˜ˆ: model gpt-5, model list)")
    print("  skills        - ìŠ¤í‚¬ ê´€ë¦¬ (ì˜ˆ: skills list, skills info python-expert)")
    print("  switch        - ì›Œí¬í”Œë¡œìš° ì „í™˜ (ì˜ˆ: switch routing_workflow)")
    print("  list          - ì‚¬ìš© ê°€ëŠ¥í•œ ì›Œí¬í”Œë¡œìš° ëª©ë¡")
    print("  visualize     - í˜„ì¬ ì›Œí¬í”Œë¡œìš° ì‹œê°í™”")
    print("  stats         - ì›Œí¬í”Œë¡œìš° í†µê³„")
    print("  metrics       - ì „ì—­ ë©”íŠ¸ë¦­")
    print("  events        - ì´ë²¤íŠ¸ íˆìŠ¤í† ë¦¬")
    print("  checkpoint    - ì²´í¬í¬ì¸íŠ¸ ì €ì¥")
    print("  restore       - ì²´í¬í¬ì¸íŠ¸ ë³µì›")
    print("  rollback      - ì´ì „ ìƒíƒœë¡œ ë¡¤ë°±")
    print("="*60 + "\n")

    session_id = f"session-{int(time.time())}"
    current_workflow = "simple_chat"

    try:
        while True:
            try:
                user_input = input(f"\n[{current_workflow}] User > ").strip()
            except EOFError:
                break

            if not user_input:
                continue

            # ëª…ë ¹ì–´ ì²˜ë¦¬
            cmd = user_input.lower().split()[0] if user_input else ""
            args = user_input.split()[1:] if len(user_input.split()) > 1 else []

            if cmd == "exit":
                print("\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤...")
                break

            elif cmd == "model":
                # ëª¨ë¸ ë³€ê²½
                subcmd = args[0].lower() if args else "info"

                if subcmd == "list":
                    print("\nğŸ“‹ ì§€ì›í•˜ëŠ” ëª¨ë¸ ëª©ë¡:")
                    print("\n  [GPT-4 ê³„ì—´]")
                    for m in ["gpt-4", "gpt-4o", "gpt-4o-mini", "gpt-4.1", "gpt-4.1-mini", "gpt-4.1-nano"]:
                        marker = "ğŸ‘‰" if m == framework.config.model else "  "
                        temp_info = "(temp âœ“)" if supports_temperature(m) else "(temp âœ—)"
                        print(f"  {marker} {m} {temp_info}")
                    print("\n  [GPT-5 ê³„ì—´] - NEW!")
                    for m in ["gpt-5", "gpt-5.1", "gpt-5.2"]:
                        marker = "ğŸ‘‰" if m == framework.config.model else "  "
                        temp_info = "(temp âœ“)" if supports_temperature(m) else "(temp âœ—)"
                        print(f"  {marker} {m} {temp_info}")
                    print("\n  [o-ì‹œë¦¬ì¦ˆ (Reasoning)]")
                    for m in ["o1", "o1-mini", "o1-preview", "o3", "o3-mini", "o4-mini"]:
                        marker = "ğŸ‘‰" if m == framework.config.model else "  "
                        temp_info = "(temp âœ“)" if supports_temperature(m) else "(temp âœ—)"
                        print(f"  {marker} {m} {temp_info}")
                    print("\n  â€» (temp âœ—) = temperature íŒŒë¼ë¯¸í„° ë¯¸ì§€ì›")

                elif subcmd == "info":
                    print(f"\nğŸ“Š í˜„ì¬ ëª¨ë¸ ì •ë³´:")
                    print(f"   ëª¨ë¸: {framework.config.model}")
                    print(f"   ë°°í¬ëª…: {framework.config.deployment_name}")
                    print(f"   Temperature ì§€ì›: {'ì˜ˆ' if supports_temperature(framework.config.model) else 'ì•„ë‹ˆì˜¤'}")
                    print(f"   Temperature: {framework.config.temperature}")
                    print(f"   Max Tokens: {framework.config.max_tokens}")

                elif subcmd in SUPPORTED_MODELS:
                    old_model = framework.config.model
                    framework.config.model = subcmd
                    framework.config.deployment_name = subcmd

                    # ì»¤ë„ ì¬ìƒì„±
                    framework.kernel = framework._create_kernel()

                    temp_info = "" if supports_temperature(subcmd) else " (temperature ë¯¸ì§€ì›)"
                    print(f"\nâœ… ëª¨ë¸ ë³€ê²½: {old_model} â†’ {subcmd}{temp_info}")

                    # ì›Œí¬í”Œë¡œìš° ì¬ìƒì„± (ìƒˆ ëª¨ë¸ ì ìš©)
                    await demo_simple_chat(framework)
                    print(f"   ì›Œí¬í”Œë¡œìš° ì—…ë°ì´íŠ¸ ì™„ë£Œ")

                else:
                    print(f"\nâŒ ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸: {subcmd}")
                    print("   'model list'ë¡œ ì§€ì›í•˜ëŠ” ëª¨ë¸ì„ í™•ì¸í•˜ì„¸ìš”.")
                continue

            elif cmd == "quick":
                # ë¹ ë¥¸ ì§ˆì˜ì‘ë‹µ
                message = " ".join(args) if args else input("ì§ˆë¬¸: ")
                print("\nâ³ ì²˜ë¦¬ ì¤‘...")
                response = await framework.quick_chat(message)
                print(f"\n[AI] > {response}")
                continue

            elif cmd == "checkpoint":
                tag = args[0] if args else None
                state = await framework.state_manager.load_state(session_id)
                if state:
                    checkpoint_file = await framework.state_manager.save_checkpoint(state, tag=tag)
                    print(f"âœ… ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {checkpoint_file}")
                else:
                    print("âŒ ì €ì¥í•  ìƒíƒœê°€ ì—†ìŠµë‹ˆë‹¤")
                continue

            elif cmd == "restore":
                tag = args[0] if args else None
                state = await framework.state_manager.restore_checkpoint(session_id, tag=tag)
                if state:
                    print(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë³µì› ì™„ë£Œ")
                else:
                    print("âŒ ë³µì›í•  ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")
                continue

            elif cmd == "rollback":
                steps = int(args[0]) if args else 1
                state = await framework.state_manager.rollback(session_id, steps=steps)
                if state:
                    print(f"âœ… {steps}ë‹¨ê³„ ë¡¤ë°± ì™„ë£Œ")
                else:
                    print("âŒ ë¡¤ë°± ì‹¤íŒ¨")
                continue

            elif cmd == "visualize":
                print("\n" + framework.visualize_workflow(current_workflow))
                continue

            elif cmd == "switch":
                if args:
                    workflow_name = args[0]
                    if workflow_name in framework.graphs:
                        current_workflow = workflow_name
                        print(f"âœ… ì›Œí¬í”Œë¡œìš° ì „í™˜: {workflow_name}")
                    else:
                        print(f"âŒ ì›Œí¬í”Œë¡œìš° '{workflow_name}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                        print(f"   ì‚¬ìš© ê°€ëŠ¥: {', '.join(framework.graphs.keys())}")
                else:
                    print("âŒ ì›Œí¬í”Œë¡œìš° ì´ë¦„ì„ ì§€ì •í•˜ì„¸ìš” (ì˜ˆ: switch simple_chat)")
                continue

            elif cmd == "stats":
                stats = framework.get_workflow_stats(current_workflow)
                print("\nğŸ“Š ì›Œí¬í”Œë¡œìš° í†µê³„:")
                print(json.dumps(stats, indent=2, ensure_ascii=False))
                continue

            elif cmd == "metrics":
                metrics = framework.get_global_metrics()
                print("\nğŸ“ˆ ì „ì—­ ë©”íŠ¸ë¦­:")
                print(json.dumps(metrics, indent=2, ensure_ascii=False))
                continue

            elif cmd == "events":
                event_type = args[0] if args else None

                if framework.event_bus:
                    if event_type:
                        try:
                            et = EventType(event_type)
                            events = framework.event_bus.get_event_history(event_type=et, limit=10)
                        except ValueError:
                            print(f"âŒ ì˜ëª»ëœ ì´ë²¤íŠ¸ íƒ€ì…: {event_type}")
                            print(f"   ê°€ëŠ¥í•œ íƒ€ì…: {', '.join(e.value for e in EventType)}")
                            continue
                    else:
                        events = framework.event_bus.get_event_history(limit=10)

                    print(f"\nğŸ“œ ìµœê·¼ ì´ë²¤íŠ¸ ({len(events)}ê°œ):")
                    for event in events:
                        print(f"  - {event.timestamp}: {event.event_type.value} ({event.agent_name or 'System'})")
                else:
                    print("âŒ ì´ë²¤íŠ¸ ì‹œìŠ¤í…œì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
                continue

            elif cmd == "list":
                print("\nğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ì›Œí¬í”Œë¡œìš°:")
                for name in framework.graphs.keys():
                    marker = "ğŸ‘‰" if name == current_workflow else "  "
                    print(f"{marker} {name}")
                continue

            elif cmd == "skills":
                # ìŠ¤í‚¬ ê´€ë ¨ ëª…ë ¹ì–´
                subcmd = args[0] if args else "list"

                if subcmd == "list":
                    print("\nğŸ“š ë“±ë¡ëœ ìŠ¤í‚¬:")
                    for skill in framework.skill_manager.list_skills():
                        status = "âœ…" if skill.enabled else "âŒ"
                        print(f"  {status} {skill.name}: {skill.description[:50]}...")

                elif subcmd == "info" and len(args) > 1:
                    skill_name = args[1]
                    skill = framework.skill_manager.get_skill(skill_name)
                    if skill:
                        print(f"\nğŸ“– ìŠ¤í‚¬: {skill.name}")
                        print(f"   ì„¤ëª…: {skill.description}")
                        print(f"   íŠ¸ë¦¬ê±°: {', '.join(skill.triggers)}")
                        print(f"   ë¦¬ì†ŒìŠ¤: {len(skill.resources)}ê°œ")
                        print(f"   ìš°ì„ ìˆœìœ„: {skill.priority}")
                    else:
                        print(f"âŒ ìŠ¤í‚¬ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {skill_name}")

                elif subcmd == "stats":
                    stats = framework.skill_manager.get_usage_stats()
                    print("\nğŸ“Š ìŠ¤í‚¬ ì‚¬ìš© í†µê³„:")
                    print(json.dumps(stats, indent=2, ensure_ascii=False))

                elif subcmd == "create" and len(args) > 1:
                    skill_name = args[1]
                    output_dir = args[2] if len(args) > 2 else "./skills"
                    path = framework.skill_manager.create_skill_template(skill_name, output_dir)
                    print(f"âœ… ìŠ¤í‚¬ í…œí”Œë¦¿ ìƒì„±: {path}")

                elif subcmd == "load" and len(args) > 1:
                    skill_dir = args[1]
                    count = framework.skill_manager.load_skills_from_directory(skill_dir)
                    print(f"âœ… {count}ê°œ ìŠ¤í‚¬ ë¡œë“œ ì™„ë£Œ")

                else:
                    print("\nğŸ’¡ ìŠ¤í‚¬ ëª…ë ¹ì–´:")
                    print("  skills list           - ë“±ë¡ëœ ìŠ¤í‚¬ ëª©ë¡")
                    print("  skills info <name>    - ìŠ¤í‚¬ ìƒì„¸ ì •ë³´")
                    print("  skills stats          - ìŠ¤í‚¬ ì‚¬ìš© í†µê³„")
                    print("  skills create <name>  - ìƒˆ ìŠ¤í‚¬ í…œí”Œë¦¿ ìƒì„±")
                    print("  skills load <dir>     - ë””ë ‰í† ë¦¬ì—ì„œ ìŠ¤í‚¬ ë¡œë“œ")
                continue

            elif cmd == "smart":
                # ìŠ¤ë§ˆíŠ¸ ì§ˆì˜ì‘ë‹µ (ìŠ¤í‚¬ ìë™ ê°ì§€)
                message = " ".join(args) if args else input("ì§ˆë¬¸: ")
                print("\nâ³ ìŠ¤í‚¬ ë§¤ì¹­ ë° ì²˜ë¦¬ ì¤‘...")
                response = await framework.smart_chat(message)
                print(f"\n[AI] > {response}")
                continue

            elif cmd == "help":
                print("\nğŸ’¡ ë„ì›€ë§:")
                print("  ì¼ë°˜ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ë©´ í˜„ì¬ ì›Œí¬í”Œë¡œìš°ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.")
                print("  'quick ì§ˆë¬¸' í˜•ì‹ìœ¼ë¡œ ë¹ ë¥¸ ì§ˆì˜ì‘ë‹µì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
                print("  'smart ì§ˆë¬¸' í˜•ì‹ìœ¼ë¡œ ìŠ¤í‚¬ ìë™ ê°ì§€ ì§ˆì˜ì‘ë‹µì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
                print("  'skills' ëª…ë ¹ì–´ë¡œ ìŠ¤í‚¬ì„ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                continue

            # ì¼ë°˜ ë©”ì‹œì§€ ì²˜ë¦¬
            try:
                print("\nâ³ ì²˜ë¦¬ ì¤‘...")
                state = await framework.run(
                    session_id=session_id,
                    workflow_name=current_workflow,
                    user_message=user_input
                )

                # ì‘ë‹µ ì¶œë ¥
                if state.messages:
                    last_message = state.messages[-1]
                    print(f"\n[{last_message.agent_name or 'AI'}] > {last_message.content}")

                # ìƒíƒœ ì •ë³´
                print(f"\nğŸ“ ìƒíƒœ: {state.execution_status.value}")
                print(f"ğŸ“Š ë…¸ë“œ: {state.current_node}")
                print(f"ğŸ“ˆ ë°©ë¬¸: {' â†’ '.join(state.visited_nodes[-5:])}")

                if state.metrics:
                    exec_time = state.metrics.get('execution_time_ms', 0)
                    iterations = state.metrics.get('total_iterations', 0)
                    print(f"â±ï¸ ì‹¤í–‰ ì‹œê°„: {exec_time:.2f}ms ({iterations} iterations)")

                # ìŠ¹ì¸ ëŒ€ê¸° ì²˜ë¦¬
                if state.execution_status == ExecutionStatus.WAITING_APPROVAL:
                    print("\nâ¸ï¸ ìŠ¹ì¸ ëŒ€ê¸° ì¤‘:")
                    for i, approval in enumerate(state.pending_approvals):
                        print(f"  [{i}] {approval.get('description', 'N/A')}")
                        print(f"      Arguments: {approval.get('arguments', {})}")

                    approve_input = input("\nìŠ¹ì¸í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
                    approved = approve_input == 'y'

                    state = await framework.approve_pending_request(
                        session_id,
                        request_id=0,
                        approved=approved
                    )
                    print(f"\n{'âœ… ìŠ¹ì¸ë¨' if approved else 'âŒ ê±°ë¶€ë¨'}")

            except Exception as e:
                logging.error(f"âŒ ì‹¤í–‰ ì˜¤ë¥˜: {e}", exc_info=True)
                print(f"\nâŒ ì˜¤ë¥˜: {e}")

    finally:
        # ì •ë¦¬
        await framework.cleanup()
        print("\nâœ… í”„ë ˆì„ì›Œí¬ ì¢…ë£Œ ì™„ë£Œ")


# ============================================================================
# ê°„í¸ ì‚¬ìš© í•¨ìˆ˜ (ëª¨ë“ˆë¡œ import ì‹œ í™œìš©)
# ============================================================================

async def quick_run(message: str, system_prompt: str = "You are a helpful assistant.") -> str:
    """
    ê°€ì¥ ê°„ë‹¨í•œ ì‚¬ìš©ë²• - í•œ ì¤„ë¡œ AI ì‘ë‹µ ë°›ê¸°

    ì‚¬ìš©ë²•:
    ```python
    import asyncio
    from Semantic_agent_framework import quick_run

    response = asyncio.run(quick_run("íŒŒì´ì¬ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?"))
    print(response)
    ```
    """
    framework = UnifiedAgentFramework.create()
    return await framework.quick_chat(message, system_prompt)


def create_framework(
    model: str = None,  # Noneì´ë©´ DEFAULT_LLM_MODEL ì‚¬ìš©
    temperature: float = 0.7,
    **kwargs
) -> UnifiedAgentFramework:
    """
    í”„ë ˆì„ì›Œí¬ ê°„í¸ ìƒì„±

    ì‚¬ìš©ë²•:
    ```python
    from Semantic_agent_framework import create_framework

    framework = create_framework(model="gpt-4o", temperature=0.5)
    ```
    """
    config = FrameworkConfig.from_env()
    if model is not None:
        config.model = model
    config.temperature = temperature

    for key, value in kwargs.items():
        if hasattr(config, key):
            setattr(config, key, value)

    return UnifiedAgentFramework.create(config)


if __name__ == "__main__":
    asyncio.run(main())
