#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Prompt Caching ì‹œìŠ¤í…œ - LLM ë¹„ìš© ì ˆê°

================================================================================
ğŸ“‹ ì—­í• : í”„ë¡¬í”„íŠ¸ ìºì‹±ì„ í†µí•œ LLM API ë¹„ìš© ì ˆê°
ğŸ“… ë²„ì „: 3.4.0 (2026ë…„ 2ì›”)
ğŸ“¦ ì˜ê°: Anthropic Prompt Caching, OpenAI Predicted Outputs
================================================================================

ğŸ¯ ì£¼ìš” ê¸°ëŠ¥:
    - í•´ì‹œ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ìºì‹±
    - TTL(Time-To-Live) ë§Œë£Œ ê´€ë¦¬
    - LRU(Least Recently Used) í‡´ê±° ì •ì±…
    - ìºì‹œ íˆíŠ¸ìœ¨ í†µê³„
    - ì‹œë§¨í‹± ìœ ì‚¬ë„ ê¸°ë°˜ ê·¼ì ‘ ë§¤ì¹­ (ì„ íƒì )
    - ë©”ëª¨ë¦¬/ë””ìŠ¤í¬ 2ê³„ì¸µ ìºì‹œ

ğŸ“Œ ë¹„ìš© ì ˆê° íš¨ê³¼:
    - ë°˜ë³µ í”„ë¡¬í”„íŠ¸ ìºì‹±ìœ¼ë¡œ 90% ë¹„ìš© ì ˆê° ê°€ëŠ¥
    - Anthropic: ìºì‹œëœ í† í° 90% í• ì¸
    - OpenAI: Predicted Outputs 50% ì ˆê°

ğŸ“Œ ì‚¬ìš© ì˜ˆì‹œ:
    >>> from unified_agent import PromptCache, CacheConfig
    >>>
    >>> cache = PromptCache(CacheConfig(
    ...     max_size_mb=100,
    ...     ttl_seconds=3600,
    ...     enable_semantic_match=True
    ... ))
    >>>
    >>> # ìºì‹œ ì¡°íšŒ
    >>> cached = await cache.get(prompt, model="gpt-5.2")
    >>> if cached:
    ...     return cached.response
    >>>
    >>> # ì‘ë‹µ ì €ì¥
    >>> await cache.set(prompt, response, model="gpt-5.2", tokens=1000)
"""

from __future__ import annotations

import asyncio
import hashlib
import json
import logging
import pickle
import threading
import time
from abc import ABC, abstractmethod
from collections import OrderedDict
from dataclasses import dataclass, field
from datetime import datetime, timezone, timedelta
from enum import Enum
from pathlib import Path
from typing import Any, Callable, Generic, TypeVar

from .utils import StructuredLogger

__all__ = [
    # ì„¤ì •
    "CacheConfig",
    "CacheEntry",
    "CacheStats",
    # ìºì‹œ ë°±ì—”ë“œ
    "CacheBackend",
    "MemoryCacheBackend",
    "DiskCacheBackend",
    "TwoLevelCacheBackend",
    # ë©”ì¸ ìºì‹œ
    "PromptCache",
    # ìœ í‹¸ë¦¬í‹°
    "compute_prompt_hash",
    "estimate_tokens",
]

# ============================================================================
# ì„¤ì • ë° ëª¨ë¸
# ============================================================================

class CacheEvictionPolicy(str, Enum):
    """ìºì‹œ í‡´ê±° ì •ì±…"""
    LRU = "lru"           # Least Recently Used
    LFU = "lfu"           # Least Frequently Used
    TTL = "ttl"           # Time-To-Live only
    FIFO = "fifo"         # First In First Out

@dataclass(frozen=True, slots=True)
class CacheConfig:
    """
    í”„ë¡¬í”„íŠ¸ ìºì‹œ ì„¤ì •
    
    Args:
        max_size_mb: ìµœëŒ€ ìºì‹œ í¬ê¸° (MB)
        max_entries: ìµœëŒ€ ì—”íŠ¸ë¦¬ ìˆ˜
        ttl_seconds: ê¸°ë³¸ TTL (ì´ˆ)
        eviction_policy: í‡´ê±° ì •ì±…
        enable_semantic_match: ì‹œë§¨í‹± ìœ ì‚¬ë„ ë§¤ì¹­ í™œì„±í™”
        semantic_threshold: ì‹œë§¨í‹± ìœ ì‚¬ë„ ì„ê³„ê°’ (0.0 ~ 1.0)
        disk_cache_path: ë””ìŠ¤í¬ ìºì‹œ ê²½ë¡œ (Noneì´ë©´ ë©”ëª¨ë¦¬ë§Œ)
        enable_compression: ì••ì¶• í™œì„±í™”
    """
    max_size_mb: int = 100
    max_entries: int = 10000
    ttl_seconds: int = 3600  # 1ì‹œê°„
    eviction_policy: CacheEvictionPolicy = CacheEvictionPolicy.LRU
    enable_semantic_match: bool = False
    semantic_threshold: float = 0.95
    disk_cache_path: str | None = None
    enable_compression: bool = True

@dataclass(slots=True)
class CacheEntry:
    """
    ìºì‹œ ì—”íŠ¸ë¦¬
    
    Args:
        key: ìºì‹œ í‚¤ (í•´ì‹œ)
        prompt: ì›ë³¸ í”„ë¡¬í”„íŠ¸
        response: ìºì‹œëœ ì‘ë‹µ
        model: ëª¨ë¸ ì´ë¦„
        created_at: ìƒì„± ì‹œê°„
        expires_at: ë§Œë£Œ ì‹œê°„
        hit_count: íˆíŠ¸ íšŸìˆ˜
        tokens_saved: ì ˆê°ëœ í† í° ìˆ˜
        metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°
    """
    key: str
    prompt: str
    response: str
    model: str
    created_at: datetime
    expires_at: datetime
    hit_count: int = 0
    tokens_saved: int = 0
    metadata: dict[str, Any] = field(default_factory=dict)
    
    @property
    def is_expired(self) -> bool:
        """ë§Œë£Œ ì—¬ë¶€"""
        return datetime.now(timezone.utc) > self.expires_at
    
    @property
    def age_seconds(self) -> float:
        """ìºì‹œ ê²½ê³¼ ì‹œê°„ (ì´ˆ)"""
        return (datetime.now(timezone.utc) - self.created_at).total_seconds()
    
    def to_dict(self) -> dict[str, Any]:
        return {
            "key": self.key,
            "prompt_preview": self.prompt[:100] + "..." if len(self.prompt) > 100 else self.prompt,
            "model": self.model,
            "created_at": self.created_at.isoformat(),
            "expires_at": self.expires_at.isoformat(),
            "hit_count": self.hit_count,
            "tokens_saved": self.tokens_saved,
        }

@dataclass(slots=True)
class CacheStats:
    """ìºì‹œ í†µê³„"""
    total_requests: int = 0
    cache_hits: int = 0
    cache_misses: int = 0
    total_tokens_saved: int = 0
    total_cost_saved_usd: float = 0.0
    current_entries: int = 0
    current_size_mb: float = 0.0
    
    @property
    def hit_rate(self) -> float:
        """ìºì‹œ íˆíŠ¸ìœ¨"""
        if self.total_requests == 0:
            return 0.0
        return self.cache_hits / self.total_requests
    
    def to_dict(self) -> dict[str, Any]:
        return {
            "total_requests": self.total_requests,
            "cache_hits": self.cache_hits,
            "cache_misses": self.cache_misses,
            "hit_rate": f"{self.hit_rate:.2%}",
            "total_tokens_saved": self.total_tokens_saved,
            "total_cost_saved_usd": f"${self.total_cost_saved_usd:.4f}",
            "current_entries": self.current_entries,
            "current_size_mb": f"{self.current_size_mb:.2f}MB",
        }

# ============================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================================================

def compute_prompt_hash(
    prompt: str,
    model: str,
    system_prompt: str | None = None,
    temperature: float = 0.0,
) -> str:
    """
    í”„ë¡¬í”„íŠ¸ í•´ì‹œ ê³„ì‚°
    
    ë™ì¼í•œ í”„ë¡¬í”„íŠ¸+ëª¨ë¸+ì„¤ì •ì— ëŒ€í•´ ë™ì¼í•œ í•´ì‹œ ìƒì„±
    
    Args:
        prompt: ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
        model: ëª¨ë¸ ì´ë¦„
        system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        temperature: ì˜¨ë„ ì„¤ì •
        
    Returns:
        SHA256 í•´ì‹œ (16ì)
    """
    content = json.dumps({
        "prompt": prompt,
        "model": model,
        "system_prompt": system_prompt or "",
        "temperature": temperature,
    }, sort_keys=True, ensure_ascii=False)
    
    return hashlib.sha256(content.encode('utf-8')).hexdigest()[:16]

def estimate_tokens(text: str) -> int:
    """
    í† í° ìˆ˜ ì¶”ì • (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
    
    ì‹¤ì œë¡œëŠ” tiktoken ì‚¬ìš© ê¶Œì¥
    
    Args:
        text: í…ìŠ¤íŠ¸
        
    Returns:
        ì¶”ì • í† í° ìˆ˜
    """
    # ì˜ì–´: ~4ì = 1í† í°, í•œêµ­ì–´: ~2ì = 1í† í°
    # í‰ê· ì ìœ¼ë¡œ 3ì = 1í† í°ìœ¼ë¡œ ê³„ì‚°
    return max(1, len(text) // 3)

# ============================================================================
# ìºì‹œ ë°±ì—”ë“œ (ì¶”ìƒ í´ë˜ìŠ¤)
# ============================================================================

class CacheBackend(ABC):
    """ìºì‹œ ë°±ì—”ë“œ ì¶”ìƒ í´ë˜ìŠ¤"""
    
    @abstractmethod
    async def get(self, key: str) -> CacheEntry | None:
        """ìºì‹œ ì¡°íšŒ"""
        pass
    
    @abstractmethod
    async def set(self, entry: CacheEntry) -> None:
        """ìºì‹œ ì €ì¥"""
        pass
    
    @abstractmethod
    async def delete(self, key: str) -> bool:
        """ìºì‹œ ì‚­ì œ"""
        pass
    
    @abstractmethod
    async def clear(self) -> None:
        """ì „ì²´ ì‚­ì œ"""
        pass
    
    @abstractmethod
    async def size(self) -> int:
        """í˜„ì¬ ì—”íŠ¸ë¦¬ ìˆ˜"""
        pass
    
    @abstractmethod
    async def keys(self) -> list[str]:
        """ëª¨ë“  í‚¤ ì¡°íšŒ"""
        pass

# ============================================================================
# ë©”ëª¨ë¦¬ ìºì‹œ ë°±ì—”ë“œ (LRU)
# ============================================================================

class MemoryCacheBackend(CacheBackend):
    """
    ë©”ëª¨ë¦¬ ê¸°ë°˜ ìºì‹œ ë°±ì—”ë“œ (LRU)
    
    OrderedDictë¥¼ ì‚¬ìš©í•œ LRU êµ¬í˜„
    """
    
    def __init__(self, config: CacheConfig):
        self.config = config
        self._cache: OrderedDict[str, CacheEntry] = OrderedDict()
        self._lock = asyncio.Lock()
        self._logger = StructuredLogger("memory_cache")
    
    async def get(self, key: str) -> CacheEntry | None:
        async with self._lock:
            if key not in self._cache:
                return None
            
            entry = self._cache[key]
            
            # ë§Œë£Œ ì²´í¬
            if entry.is_expired:
                del self._cache[key]
                return None
            
            # LRU: ìµœê·¼ ì‚¬ìš©ìœ¼ë¡œ ì´ë™
            self._cache.move_to_end(key)
            entry.hit_count += 1
            
            return entry
    
    async def set(self, entry: CacheEntry) -> None:
        async with self._lock:
            # ìµœëŒ€ ì—”íŠ¸ë¦¬ ìˆ˜ ì²´í¬
            while len(self._cache) >= self.config.max_entries:
                # LRU: ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
                oldest_key = next(iter(self._cache))
                del self._cache[oldest_key]
                self._logger.debug("Evicted oldest entry", key=oldest_key)
            
            self._cache[entry.key] = entry
            self._cache.move_to_end(entry.key)
    
    async def delete(self, key: str) -> bool:
        async with self._lock:
            if key in self._cache:
                del self._cache[key]
                return True
            return False
    
    async def clear(self) -> None:
        async with self._lock:
            self._cache.clear()
    
    async def size(self) -> int:
        return len(self._cache)
    
    async def keys(self) -> list[str]:
        return list(self._cache.keys())
    
    async def cleanup_expired(self) -> int:
        """ë§Œë£Œëœ ì—”íŠ¸ë¦¬ ì •ë¦¬"""
        async with self._lock:
            expired_keys = [
                key for key, entry in self._cache.items()
                if entry.is_expired
            ]
            for key in expired_keys:
                del self._cache[key]
            return len(expired_keys)

# ============================================================================
# ë””ìŠ¤í¬ ìºì‹œ ë°±ì—”ë“œ
# ============================================================================

class DiskCacheBackend(CacheBackend):
    """
    ë””ìŠ¤í¬ ê¸°ë°˜ ìºì‹œ ë°±ì—”ë“œ
    
    íŒŒì¼ ì‹œìŠ¤í…œì„ ì‚¬ìš©í•œ ì˜ì† ìºì‹œ
    """
    
    def __init__(self, config: CacheConfig):
        self.config = config
        self._cache_dir = Path(config.disk_cache_path or "~/.agent_cache").expanduser()
        self._cache_dir.mkdir(parents=True, exist_ok=True)
        self._lock = asyncio.Lock()
        self._logger = StructuredLogger("disk_cache")
    
    def _get_file_path(self, key: str) -> Path:
        return self._cache_dir / f"{key}.cache"
    
    async def get(self, key: str) -> CacheEntry | None:
        file_path = self._get_file_path(key)
        
        if not file_path.exists():
            return None
        
        try:
            async with self._lock:
                with open(file_path, 'rb') as f:
                    entry = pickle.load(f)
                
                if entry.is_expired:
                    file_path.unlink()
                    return None
                
                entry.hit_count += 1
                with open(file_path, 'wb') as f:
                    pickle.dump(entry, f)
                
                return entry
        except Exception as e:
            self._logger.error("Failed to read cache", key=key, error=str(e))
            return None
    
    async def set(self, entry: CacheEntry) -> None:
        file_path = self._get_file_path(entry.key)
        
        try:
            async with self._lock:
                with open(file_path, 'wb') as f:
                    pickle.dump(entry, f)
        except Exception as e:
            self._logger.error("Failed to write cache", key=entry.key, error=str(e))
    
    async def delete(self, key: str) -> bool:
        file_path = self._get_file_path(key)
        
        if file_path.exists():
            file_path.unlink()
            return True
        return False
    
    async def clear(self) -> None:
        async with self._lock:
            for file_path in self._cache_dir.glob("*.cache"):
                file_path.unlink()
    
    async def size(self) -> int:
        return len(list(self._cache_dir.glob("*.cache")))
    
    async def keys(self) -> list[str]:
        return [f.stem for f in self._cache_dir.glob("*.cache")]

# ============================================================================
# 2ê³„ì¸µ ìºì‹œ ë°±ì—”ë“œ
# ============================================================================

class TwoLevelCacheBackend(CacheBackend):
    """
    2ê³„ì¸µ ìºì‹œ ë°±ì—”ë“œ (ë©”ëª¨ë¦¬ + ë””ìŠ¤í¬)
    
    L1: ë©”ëª¨ë¦¬ (ë¹ ë¦„, ì œí•œì )
    L2: ë””ìŠ¤í¬ (ëŠë¦¼, ëŒ€ìš©ëŸ‰)
    """
    
    def __init__(self, config: CacheConfig):
        self.config = config
        self._l1 = MemoryCacheBackend(CacheConfig(
            max_entries=min(1000, config.max_entries // 10),
            ttl_seconds=config.ttl_seconds,
        ))
        self._l2 = DiskCacheBackend(config)
        self._logger = StructuredLogger("two_level_cache")
    
    async def get(self, key: str) -> CacheEntry | None:
        # L1ì—ì„œ ë¨¼ì € ì¡°íšŒ
        entry = await self._l1.get(key)
        if entry:
            return entry
        
        # L2ì—ì„œ ì¡°íšŒ
        entry = await self._l2.get(key)
        if entry:
            # L1ì— ìŠ¹ê²©
            await self._l1.set(entry)
            return entry
        
        return None
    
    async def set(self, entry: CacheEntry) -> None:
        # ì–‘ìª½ì— ì €ì¥
        await self._l1.set(entry)
        await self._l2.set(entry)
    
    async def delete(self, key: str) -> bool:
        l1_result = await self._l1.delete(key)
        l2_result = await self._l2.delete(key)
        return l1_result or l2_result
    
    async def clear(self) -> None:
        await self._l1.clear()
        await self._l2.clear()
    
    async def size(self) -> int:
        return await self._l2.size()  # L2ê°€ ì „ì²´ í¬ê¸°
    
    async def keys(self) -> list[str]:
        return await self._l2.keys()

# ============================================================================
# ë©”ì¸ í”„ë¡¬í”„íŠ¸ ìºì‹œ
# ============================================================================

# ëª¨ë¸ë³„ í† í° ê°€ê²© (USD per 1K tokens, 2026ë…„ ê¸°ì¤€)
MODEL_PRICING = {
    "gpt-5.2": {"input": 0.005, "output": 0.015, "cached": 0.0005},
    "gpt-5.1": {"input": 0.004, "output": 0.012, "cached": 0.0004},
    "gpt-4o": {"input": 0.003, "output": 0.01, "cached": 0.0003},
    "claude-opus-4.5": {"input": 0.015, "output": 0.075, "cached": 0.0015},
    "claude-sonnet-4": {"input": 0.003, "output": 0.015, "cached": 0.0003},
    "o3": {"input": 0.015, "output": 0.060, "cached": 0.0015},
    "default": {"input": 0.003, "output": 0.01, "cached": 0.0003},
}

class PromptCache:
    """
    í”„ë¡¬í”„íŠ¸ ìºì‹œ ì‹œìŠ¤í…œ
    
    LLM API í˜¸ì¶œ ë¹„ìš©ì„ ì ˆê°í•˜ê¸° ìœ„í•œ ìºì‹± ì‹œìŠ¤í…œ
    
    ì£¼ìš” ê¸°ëŠ¥:
    1. í•´ì‹œ ê¸°ë°˜ ì •í™• ë§¤ì¹­
    2. TTL ë§Œë£Œ ê´€ë¦¬
    3. LRU í‡´ê±° ì •ì±…
    4. ìºì‹œ íˆíŠ¸ìœ¨ í†µê³„
    5. ë¹„ìš© ì ˆê° ì¶”ì 
    
    ì‚¬ìš© ì˜ˆì‹œ:
        >>> cache = PromptCache(CacheConfig(ttl_seconds=3600))
        >>> await cache.initialize()
        >>>
        >>> # ìºì‹œ ì¡°íšŒ
        >>> result = await cache.get(
        ...     prompt="Hello, world!",
        ...     model="gpt-5.2"
        ... )
        >>>
        >>> if result:
        ...     print(f"Cache hit! Response: {result.response}")
        ... else:
        ...     response = await llm.chat(prompt)
        ...     await cache.set(prompt, response, model="gpt-5.2")
    """
    
    def __init__(self, config: CacheConfig | None = None):
        self.config = config or CacheConfig()
        self._stats = CacheStats()
        self._logger = StructuredLogger("prompt_cache")
        
        # ë°±ì—”ë“œ ì„ íƒ
        if self.config.disk_cache_path:
            self._backend = TwoLevelCacheBackend(self.config)
        else:
            self._backend = MemoryCacheBackend(self.config)
        
        # ì‹œë§¨í‹± ë§¤ì¹­ìš© ì„ë² ë”© ìºì‹œ (ì„ íƒì )
        self._embedding_cache: dict[str, list[float]] = {}
        self._embedding_func: Callable[[str], list[float]] | None = None
        
        # ë°±ê·¸ë¼ìš´ë“œ ì •ë¦¬ íƒœìŠ¤í¬
        self._cleanup_task: asyncio.Task | None = None
        self._running = False
    
    async def initialize(self):
        """ìºì‹œ ì´ˆê¸°í™” ë° ë°±ê·¸ë¼ìš´ë“œ ì •ë¦¬ ì‹œì‘"""
        self._running = True
        self._cleanup_task = asyncio.create_task(self._cleanup_loop())
        self._logger.info("PromptCache initialized", config=self.config)
    
    async def close(self):
        """ìºì‹œ ì¢…ë£Œ"""
        self._running = False
        if self._cleanup_task:
            self._cleanup_task.cancel()
            try:
                await self._cleanup_task
            except asyncio.CancelledError:
                pass
    
    async def _cleanup_loop(self):
        """ë°±ê·¸ë¼ìš´ë“œ ì •ë¦¬ ë£¨í”„"""
        while self._running:
            try:
                await asyncio.sleep(60)  # 1ë¶„ë§ˆë‹¤
                if isinstance(self._backend, MemoryCacheBackend):
                    cleaned = await self._backend.cleanup_expired()
                    if cleaned > 0:
                        self._logger.debug("Cleaned expired entries", count=cleaned)
            except asyncio.CancelledError:
                break
            except Exception as e:
                self._logger.error("Cleanup error", error=str(e))
    
    def set_embedding_function(self, func: Callable[[str], list[float]]):
        """ì‹œë§¨í‹± ë§¤ì¹­ìš© ì„ë² ë”© í•¨ìˆ˜ ì„¤ì •"""
        self._embedding_func = func
    
    async def get(
        self,
        prompt: str,
        model: str,
        system_prompt: str | None = None,
        temperature: float = 0.0,
    ) -> CacheEntry | None:
        """
        ìºì‹œ ì¡°íšŒ
        
        Args:
            prompt: ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
            model: ëª¨ë¸ ì´ë¦„
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            temperature: ì˜¨ë„ ì„¤ì •
            
        Returns:
            ìºì‹œëœ ì—”íŠ¸ë¦¬ ë˜ëŠ” None
        """
        self._stats.total_requests += 1
        
        # í•´ì‹œ í‚¤ ìƒì„±
        key = compute_prompt_hash(prompt, model, system_prompt, temperature)
        
        # ì •í™• ë§¤ì¹­ ì¡°íšŒ
        entry = await self._backend.get(key)
        
        if entry:
            self._stats.cache_hits += 1
            tokens = estimate_tokens(prompt)
            self._stats.total_tokens_saved += tokens
            
            # ë¹„ìš© ì ˆê° ê³„ì‚°
            pricing = MODEL_PRICING.get(model, MODEL_PRICING["default"])
            saved = (pricing["input"] - pricing["cached"]) * tokens / 1000
            self._stats.total_cost_saved_usd += saved
            entry.tokens_saved += tokens
            
            self._logger.debug("Cache hit", key=key, model=model)
            return entry
        
        # ì‹œë§¨í‹± ë§¤ì¹­ (ì„ íƒì )
        if self.config.enable_semantic_match and self._embedding_func:
            entry = await self._semantic_search(prompt, model)
            if entry:
                self._stats.cache_hits += 1
                return entry
        
        self._stats.cache_misses += 1
        return None
    
    async def set(
        self,
        prompt: str,
        response: str,
        model: str,
        system_prompt: str | None = None,
        temperature: float = 0.0,
        ttl_seconds: int | None = None,
        metadata: dict[str, Any] | None = None,
    ) -> CacheEntry:
        """
        ìºì‹œ ì €ì¥
        
        Args:
            prompt: ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
            response: LLM ì‘ë‹µ
            model: ëª¨ë¸ ì´ë¦„
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            temperature: ì˜¨ë„ ì„¤ì •
            ttl_seconds: TTL (ì´ˆ)
            metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°
            
        Returns:
            ìƒì„±ëœ ìºì‹œ ì—”íŠ¸ë¦¬
        """
        key = compute_prompt_hash(prompt, model, system_prompt, temperature)
        now = datetime.now(timezone.utc)
        ttl = ttl_seconds or self.config.ttl_seconds
        
        entry = CacheEntry(
            key=key,
            prompt=prompt,
            response=response,
            model=model,
            created_at=now,
            expires_at=now + timedelta(seconds=ttl),
            metadata=metadata or {},
        )
        
        await self._backend.set(entry)
        self._stats.current_entries = await self._backend.size()
        
        self._logger.debug("Cache set", key=key, model=model, ttl=ttl)
        return entry
    
    async def _semantic_search(
        self,
        prompt: str,
        model: str,
    ) -> CacheEntry | None:
        """ì‹œë§¨í‹± ìœ ì‚¬ë„ ê²€ìƒ‰ (ê·¼ì ‘ ë§¤ì¹­)"""
        if not self._embedding_func:
            return None
        
        try:
            query_embedding = self._embedding_func(prompt)
            
            best_entry = None
            best_score = 0.0
            
            for key in await self._backend.keys():
                entry = await self._backend.get(key)
                if not entry or entry.model != model:
                    continue
                
                # ìºì‹œëœ ì„ë² ë”© ì¡°íšŒ ë˜ëŠ” ìƒì„±
                if entry.key not in self._embedding_cache:
                    self._embedding_cache[entry.key] = self._embedding_func(entry.prompt)
                
                cached_embedding = self._embedding_cache[entry.key]
                
                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                score = self._cosine_similarity(query_embedding, cached_embedding)
                
                if score > best_score and score >= self.config.semantic_threshold:
                    best_score = score
                    best_entry = entry
            
            if best_entry:
                self._logger.debug(
                    "Semantic match found",
                    score=f"{best_score:.3f}",
                    key=best_entry.key
                )
            
            return best_entry
        except Exception as e:
            self._logger.error("Semantic search failed", error=str(e))
            return None
    
    @staticmethod
    def _cosine_similarity(a: list[float], b: list[float]) -> float:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        dot_product = sum(x * y for x, y in zip(a, b))
        norm_a = sum(x * x for x in a) ** 0.5
        norm_b = sum(x * x for x in b) ** 0.5
        
        if norm_a == 0 or norm_b == 0:
            return 0.0
        
        return dot_product / (norm_a * norm_b)
    
    async def invalidate(
        self,
        prompt: str | None = None,
        model: str | None = None,
        key: str | None = None,
    ) -> bool:
        """
        ìºì‹œ ë¬´íš¨í™”
        
        Args:
            prompt: í”„ë¡¬í”„íŠ¸ (key ê³„ì‚°ìš©)
            model: ëª¨ë¸ ì´ë¦„
            key: ì§ì ‘ í‚¤ ì§€ì •
            
        Returns:
            ì‚­ì œ ì„±ê³µ ì—¬ë¶€
        """
        if key:
            return await self._backend.delete(key)
        elif prompt and model:
            key = compute_prompt_hash(prompt, model)
            return await self._backend.delete(key)
        return False
    
    async def clear(self):
        """ì „ì²´ ìºì‹œ ì‚­ì œ"""
        await self._backend.clear()
        self._stats = CacheStats()
        self._embedding_cache.clear()
        self._logger.info("Cache cleared")
    
    def get_stats(self) -> CacheStats:
        """ìºì‹œ í†µê³„ ì¡°íšŒ"""
        return self._stats
    
    async def get_all_entries(self) -> list[CacheEntry]:
        """ëª¨ë“  ìºì‹œ ì—”íŠ¸ë¦¬ ì¡°íšŒ"""
        entries = []
        for key in await self._backend.keys():
            entry = await self._backend.get(key)
            if entry:
                entries.append(entry)
        return entries

# ============================================================================
# ë°ì½”ë ˆì´í„°
# ============================================================================

def cached_prompt(
    cache: PromptCache,
    model: str,
    ttl_seconds: int | None = None,
):
    """
    í”„ë¡¬í”„íŠ¸ ìºì‹± ë°ì½”ë ˆì´í„°
    
    ì‚¬ìš© ì˜ˆì‹œ:
        >>> @cached_prompt(cache, model="gpt-5.2")
        >>> async def chat(prompt: str) -> str:
        ...     return await llm.chat(prompt)
    """
    def decorator(func):
        async def wrapper(prompt: str, *args, **kwargs):
            # ìºì‹œ ì¡°íšŒ
            cached = await cache.get(prompt, model)
            if cached:
                return cached.response
            
            # ì›ë³¸ í•¨ìˆ˜ ì‹¤í–‰
            response = await func(prompt, *args, **kwargs)
            
            # ìºì‹œ ì €ì¥
            await cache.set(prompt, response, model, ttl_seconds=ttl_seconds)
            
            return response
        return wrapper
    return decorator
