#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Unified Agent Framework - ÏòÅÏÜç Î©îÎ™®Î¶¨ ÏãúÏä§ÌÖú (Persistent Memory Module)

================================================================================
üìÅ ÌååÏùº ÏúÑÏπò: unified_agent/persistent_memory.py
üìã Ïó≠Ìï†: Clawdbot Ïä§ÌÉÄÏùº 2Í≥ÑÏ∏µ ÏòÅÏÜç Î©îÎ™®Î¶¨ + ÌïòÏù¥Î∏åÎ¶¨Îìú Í≤ÄÏÉâ
üìÖ ÏµúÏ¢Ö ÏóÖÎç∞Ïù¥Ìä∏: 2026ÎÖÑ 2Ïõî
================================================================================

üéØ Ï£ºÏöî Íµ¨ÏÑ± ÏöîÏÜå:

    üìå 2Í≥ÑÏ∏µ Î©îÎ™®Î¶¨ ÏãúÏä§ÌÖú:
        - Layer 1: Daily Logs (memory/YYYY-MM-DD.md) - ÏùºÎ≥Ñ Í∏∞Î°ù
        - Layer 2: Long-term Memory (MEMORY.md) - Ïû•Í∏∞ Í∏∞Ïñµ

    üìå Î©îÎ™®Î¶¨ ÎèÑÍµ¨:
        - memory_search: ÏãúÎß®Ìã± + ÌÇ§ÏõåÎìú ÌïòÏù¥Î∏åÎ¶¨Îìú Í≤ÄÏÉâ
        - memory_get: ÌäπÏ†ï ÎùºÏù∏ Î≤îÏúÑ ÏùΩÍ∏∞
        - memory_write: Î©îÎ™®Î¶¨ ÌååÏùºÏóê Í∏∞Î°ù

    üìå Ïù∏Îç±Ïã± ÏãúÏä§ÌÖú:
        - Ï≤≠ÌÇπ (400 tokens, 80 overlap)
        - ÏûÑÎ≤†Îî© (OpenAI text-embedding-3-small)
        - SQLite + FTS5 (Ï†ÑÎ¨∏ Í≤ÄÏÉâ)

üîß ÌïµÏã¨ Í∏∞Îä•:
    - ÌïòÏù¥Î∏åÎ¶¨Îìú Í≤ÄÏÉâ: Vector (70%) + BM25 (30%)
    - ÏûêÎèô Ïù∏Îç±Ïã±: ÌååÏùº Î≥ÄÍ≤Ω Ïãú ÏûêÎèô Ïû¨Ïù∏Îç±Ïã±
    - Multi-Agent Î©îÎ™®Î¶¨ Í≤©Î¶¨
    - Bootstrap Files Ìå®ÌÑ¥ (AGENTS.md, SOUL.md, USER.md)

üìå Ï∞∏Í≥†:
    - Clawdbot Memory System: https://manthanguptaa.in/posts/clawdbot_memory/
    - sqlite-vec: https://github.com/asg017/sqlite-vec
"""

import os
import re
import json
import sqlite3
import hashlib
import logging
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from datetime import datetime, timezone, date
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple, Callable
from enum import Enum

from .utils import StructuredLogger

__all__ = [
    # Î©îÎ™®Î¶¨ ÏãúÏä§ÌÖú
    "PersistentMemory",
    "MemoryConfig",
    "MemoryLayer",
    # Í≤ÄÏÉâ Í≤∞Í≥º
    "MemorySearchResult",
    "MemoryChunk",
    # ÎèÑÍµ¨
    "MemorySearchTool",
    "MemoryGetTool",
    "MemoryWriteTool",
    # Bootstrap Files
    "BootstrapFileManager",
    "BootstrapFileType",
    # Ïù∏Îç±ÏÑú
    "MemoryIndexer",
]


# ============================================================================
# Enums & Constants
# ============================================================================

class MemoryLayer(Enum):
    """Î©îÎ™®Î¶¨ Í≥ÑÏ∏µ"""
    DAILY_LOG = "daily_log"        # Layer 1: ÏùºÎ≥Ñ Í∏∞Î°ù (memory/YYYY-MM-DD.md)
    LONG_TERM = "long_term"        # Layer 2: Ïû•Í∏∞ Í∏∞Ïñµ (MEMORY.md)
    BOOTSTRAP = "bootstrap"        # Bootstrap ÌååÏùº (AGENTS.md, SOUL.md Îì±)


class BootstrapFileType(Enum):
    """Bootstrap ÌååÏùº Ïú†Ìòï (Clawdbot Ìå®ÌÑ¥)"""
    AGENTS = "AGENTS.md"     # ÏóêÏù¥Ï†ÑÌä∏ ÏßÄÏãúÏÇ¨Ìï≠, Î©îÎ™®Î¶¨ Í∞ÄÏù¥ÎìúÎùºÏù∏
    SOUL = "SOUL.md"         # ÏÑ±Í≤©Í≥º ÌÜ§
    USER = "USER.md"         # ÏÇ¨Ïö©Ïûê Ï†ïÎ≥¥
    TOOLS = "TOOLS.md"       # Ïô∏Î∂Ä ÎèÑÍµ¨ ÏÇ¨Ïö© Í∞ÄÏù¥Îìú
    MEMORY = "MEMORY.md"     # Ïû•Í∏∞ Í∏∞Ïñµ


# ============================================================================
# Configuration
# ============================================================================

@dataclass
class MemoryConfig:
    """
    ÏòÅÏÜç Î©îÎ™®Î¶¨ ÏÑ§Ï†ï
    
    Args:
        workspace_dir: Î©îÎ™®Î¶¨ ÏõåÌÅ¨Ïä§ÌéòÏù¥Ïä§ ÎîîÎ†âÌÜ†Î¶¨
        state_dir: SQLite Ïù∏Îç±Ïä§ Ï†ÄÏû• ÎîîÎ†âÌÜ†Î¶¨
        chunk_size: Ï≤≠ÌÅ¨ ÌÅ¨Í∏∞ (ÌÜ†ÌÅ∞)
        chunk_overlap: Ï≤≠ÌÅ¨ Ïò§Î≤ÑÎû© (ÌÜ†ÌÅ∞)
        vector_weight: ÌïòÏù¥Î∏åÎ¶¨Îìú Í≤ÄÏÉâÏóêÏÑú Î≤°ÌÑ∞ Í∞ÄÏ§ëÏπò (0.0 ~ 1.0)
        min_search_score: ÏµúÏÜå Í≤ÄÏÉâ Ï†êÏàò ÏûÑÍ≥ÑÍ∞í
        max_search_results: ÏµúÎåÄ Í≤ÄÏÉâ Í≤∞Í≥º Ïàò
        embedding_model: ÏûÑÎ≤†Îî© Î™®Îç∏Î™Ö
    """
    workspace_dir: str = field(default_factory=lambda: os.path.expanduser("~/agent_memory"))
    state_dir: str = field(default_factory=lambda: os.path.expanduser("~/.agent_memory"))
    chunk_size: int = 400          # ~400 tokens per chunk
    chunk_overlap: int = 80        # 80 token overlap
    vector_weight: float = 0.7     # 70% vector, 30% BM25
    min_search_score: float = 0.35
    max_search_results: int = 10
    embedding_model: str = "text-embedding-3-small"
    embedding_dimensions: int = 1536


# ============================================================================
# Data Models
# ============================================================================

@dataclass
class MemoryChunk:
    """Î©îÎ™®Î¶¨ Ï≤≠ÌÅ¨ (Ïù∏Îç±Ïã± Îã®ÏúÑ)"""
    id: str
    path: str
    start_line: int
    end_line: int
    text: str
    content_hash: str
    layer: MemoryLayer
    created_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))


@dataclass
class MemorySearchResult:
    """Î©îÎ™®Î¶¨ Í≤ÄÏÉâ Í≤∞Í≥º"""
    path: str
    start_line: int
    end_line: int
    score: float
    snippet: str
    layer: MemoryLayer
    vector_score: float = 0.0
    text_score: float = 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "path": self.path,
            "startLine": self.start_line,
            "endLine": self.end_line,
            "score": round(self.score, 3),
            "snippet": self.snippet,
            "source": self.layer.value,
            "vectorScore": round(self.vector_score, 3),
            "textScore": round(self.text_score, 3),
        }


# ============================================================================
# Memory Indexer (SQLite + FTS5)
# ============================================================================

class MemoryIndexer:
    """
    Î©îÎ™®Î¶¨ Ïù∏Îç±ÏÑú - SQLite + FTS5 Í∏∞Î∞ò ÌïòÏù¥Î∏åÎ¶¨Îìú Í≤ÄÏÉâ
    
    SQLite ÌÖåÏù¥Î∏î:
        - chunks: Ï≤≠ÌÅ¨ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ (id, path, start_line, end_line, text, hash)
        - chunks_fts: FTS5 Ï†ÑÎ¨∏ Í≤ÄÏÉâ Ïù∏Îç±Ïä§
        - embeddings: ÏûÑÎ≤†Îî© Î≤°ÌÑ∞ Ï∫êÏãú (hash -> vector)
    """
    
    def __init__(
        self,
        db_path: str,
        embedding_func: Optional[Callable[[str], List[float]]] = None,
        config: Optional[MemoryConfig] = None
    ):
        self.db_path = db_path
        self.config = config or MemoryConfig()
        self._embedding_func = embedding_func
        self._logger = StructuredLogger("memory_indexer")
        self._conn: Optional[sqlite3.Connection] = None
        self._init_database()
    
    def _init_database(self):
        """Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ï¥àÍ∏∞Ìôî"""
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        self._conn = sqlite3.connect(self.db_path, check_same_thread=False)
        self._conn.row_factory = sqlite3.Row
        
        # Ï≤≠ÌÅ¨ ÌÖåÏù¥Î∏î
        self._conn.execute("""
            CREATE TABLE IF NOT EXISTS chunks (
                id TEXT PRIMARY KEY,
                path TEXT NOT NULL,
                start_line INTEGER NOT NULL,
                end_line INTEGER NOT NULL,
                text TEXT NOT NULL,
                content_hash TEXT NOT NULL,
                layer TEXT NOT NULL,
                created_at TEXT NOT NULL
            )
        """)
        
        # FTS5 Ï†ÑÎ¨∏ Í≤ÄÏÉâ ÌÖåÏù¥Î∏î
        self._conn.execute("""
            CREATE VIRTUAL TABLE IF NOT EXISTS chunks_fts USING fts5(
                text,
                content=chunks,
                content_rowid=rowid
            )
        """)
        
        # ÏûÑÎ≤†Îî© Ï∫êÏãú ÌÖåÏù¥Î∏î
        self._conn.execute("""
            CREATE TABLE IF NOT EXISTS embeddings (
                content_hash TEXT PRIMARY KEY,
                vector BLOB NOT NULL,
                model TEXT NOT NULL,
                created_at TEXT NOT NULL
            )
        """)
        
        # Ïù∏Îç±Ïä§
        self._conn.execute("CREATE INDEX IF NOT EXISTS idx_chunks_path ON chunks(path)")
        self._conn.execute("CREATE INDEX IF NOT EXISTS idx_chunks_hash ON chunks(content_hash)")
        
        self._conn.commit()
        self._logger.info("Database initialized", db_path=self.db_path)
    
    def set_embedding_function(self, func: Callable[[str], List[float]]):
        """ÏûÑÎ≤†Îî© Ìï®Ïàò ÏÑ§Ï†ï"""
        self._embedding_func = func
    
    def _compute_hash(self, text: str) -> str:
        """ÌÖçÏä§Ìä∏ Ìï¥Ïãú Í≥ÑÏÇ∞"""
        return hashlib.sha256(text.encode('utf-8')).hexdigest()[:16]
    
    def _chunk_text(self, text: str, path: str, layer: MemoryLayer) -> List[MemoryChunk]:
        """ÌÖçÏä§Ìä∏Î•º Ï≤≠ÌÅ¨Î°ú Î∂ÑÌï†"""
        lines = text.split('\n')
        chunks = []
        
        # Í∞ÑÎã®Ìïú ÎùºÏù∏ Í∏∞Î∞ò Ï≤≠ÌÇπ (ÌÜ†ÌÅ∞ Ï∂îÏ†ï: ~4 chars/token)
        chars_per_chunk = self.config.chunk_size * 4
        overlap_chars = self.config.chunk_overlap * 4
        
        current_text = ""
        current_start = 1
        
        for i, line in enumerate(lines, start=1):
            current_text += line + "\n"
            
            if len(current_text) >= chars_per_chunk:
                chunk_id = f"{self._compute_hash(path)}_{current_start}_{i}"
                chunks.append(MemoryChunk(
                    id=chunk_id,
                    path=path,
                    start_line=current_start,
                    end_line=i,
                    text=current_text.strip(),
                    content_hash=self._compute_hash(current_text),
                    layer=layer
                ))
                
                # Ïò§Î≤ÑÎû©ÏùÑ ÏúÑÌï¥ ÏùºÎ∂Ä Î≥¥Ï°¥
                overlap_text = current_text[-overlap_chars:] if len(current_text) > overlap_chars else ""
                current_text = overlap_text
                current_start = max(1, i - len(overlap_text.split('\n')) + 1)
        
        # ÎßàÏßÄÎßâ Ï≤≠ÌÅ¨
        if current_text.strip():
            chunk_id = f"{self._compute_hash(path)}_{current_start}_{len(lines)}"
            chunks.append(MemoryChunk(
                id=chunk_id,
                path=path,
                start_line=current_start,
                end_line=len(lines),
                text=current_text.strip(),
                content_hash=self._compute_hash(current_text),
                layer=layer
            ))
        
        return chunks
    
    async def index_file(self, file_path: str, layer: MemoryLayer) -> int:
        """ÌååÏùº Ïù∏Îç±Ïã±"""
        if not os.path.exists(file_path):
            self._logger.warning("File not found", path=file_path)
            return 0
        
        with open(file_path, 'r', encoding='utf-8') as f:
            text = f.read()
        
        # Í∏∞Ï°¥ Ï≤≠ÌÅ¨ ÏÇ≠Ï†ú
        self._conn.execute("DELETE FROM chunks WHERE path = ?", (file_path,))
        
        # ÏÉà Ï≤≠ÌÅ¨ ÏÉùÏÑ± Î∞è Ï†ÄÏû•
        chunks = self._chunk_text(text, file_path, layer)
        
        for chunk in chunks:
            self._conn.execute("""
                INSERT OR REPLACE INTO chunks (id, path, start_line, end_line, text, content_hash, layer, created_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                chunk.id, chunk.path, chunk.start_line, chunk.end_line,
                chunk.text, chunk.content_hash, chunk.layer.value,
                chunk.created_at.isoformat()
            ))
        
        # FTS Ïù∏Îç±Ïä§ Ïû¨Íµ¨Ï∂ï
        self._conn.execute("INSERT INTO chunks_fts(chunks_fts) VALUES('rebuild')")
        self._conn.commit()
        
        self._logger.info("File indexed", path=file_path, chunks=len(chunks))
        return len(chunks)
    
    async def search_bm25(self, query: str, limit: int = 10) -> List[Tuple[str, float]]:
        """BM25 ÌÇ§ÏõåÎìú Í≤ÄÏÉâ"""
        cursor = self._conn.execute("""
            SELECT chunks.id, bm25(chunks_fts) as score
            FROM chunks_fts
            JOIN chunks ON chunks.rowid = chunks_fts.rowid
            WHERE chunks_fts MATCH ?
            ORDER BY score
            LIMIT ?
        """, (query, limit))
        
        results = []
        for row in cursor.fetchall():
            # BM25 Ï†êÏàò Ï†ïÍ∑úÌôî (ÏùåÏàò -> ÏñëÏàò)
            normalized_score = 1.0 / (1.0 + abs(row['score']))
            results.append((row['id'], normalized_score))
        
        return results
    
    async def search_vector(self, query: str, limit: int = 10) -> List[Tuple[str, float]]:
        """Î≤°ÌÑ∞ ÏãúÎß®Ìã± Í≤ÄÏÉâ"""
        if not self._embedding_func:
            self._logger.debug("Embedding function not set, skipping vector search")
            return []
        
        try:
            query_embedding = self._embedding_func(query)
        except Exception as e:
            self._logger.error(f"Embedding failed: {e}")
            return []
        
        # Î™®Îì† Ï≤≠ÌÅ¨Ïùò ÏûÑÎ≤†Îî©Í≥º ÏΩîÏÇ¨Ïù∏ Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞
        cursor = self._conn.execute("""
            SELECT c.id, c.content_hash, e.vector
            FROM chunks c
            LEFT JOIN embeddings e ON c.content_hash = e.content_hash
        """)
        
        results = []
        for row in cursor.fetchall():
            if row['vector']:
                stored_embedding = json.loads(row['vector'])
                similarity = self._cosine_similarity(query_embedding, stored_embedding)
                results.append((row['id'], similarity))
        
        # Ï†êÏàòÏàú Ï†ïÎ†¨
        results.sort(key=lambda x: x[1], reverse=True)
        return results[:limit]
    
    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """ÏΩîÏÇ¨Ïù∏ Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞"""
        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        norm1 = sum(a * a for a in vec1) ** 0.5
        norm2 = sum(b * b for b in vec2) ** 0.5
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        return dot_product / (norm1 * norm2)
    
    async def hybrid_search(
        self,
        query: str,
        limit: int = 10,
        min_score: float = 0.35
    ) -> List[MemorySearchResult]:
        """
        ÌïòÏù¥Î∏åÎ¶¨Îìú Í≤ÄÏÉâ (Vector 70% + BM25 30%)
        """
        vector_results = await self.search_vector(query, limit * 2)
        bm25_results = await self.search_bm25(query, limit * 2)
        
        # Ï†êÏàò Í≤∞Ìï©
        combined_scores: Dict[str, Dict[str, float]] = {}
        
        for chunk_id, score in vector_results:
            combined_scores[chunk_id] = {'vector': score, 'text': 0.0}
        
        for chunk_id, score in bm25_results:
            if chunk_id in combined_scores:
                combined_scores[chunk_id]['text'] = score
            else:
                combined_scores[chunk_id] = {'vector': 0.0, 'text': score}
        
        # Í∞ÄÏ§ë ÌèâÍ∑† Í≥ÑÏÇ∞
        final_scores = []
        for chunk_id, scores in combined_scores.items():
            final_score = (
                self.config.vector_weight * scores['vector'] +
                (1 - self.config.vector_weight) * scores['text']
            )
            if final_score >= min_score:
                final_scores.append((chunk_id, final_score, scores['vector'], scores['text']))
        
        # Ï†ïÎ†¨ Î∞è ÏÉÅÏúÑ Í≤∞Í≥º ÏÑ†ÌÉù
        final_scores.sort(key=lambda x: x[1], reverse=True)
        
        # Í≤∞Í≥º Ï°∞Ìöå
        results = []
        for chunk_id, final_score, vector_score, text_score in final_scores[:limit]:
            cursor = self._conn.execute(
                "SELECT * FROM chunks WHERE id = ?", (chunk_id,)
            )
            row = cursor.fetchone()
            if row:
                results.append(MemorySearchResult(
                    path=row['path'],
                    start_line=row['start_line'],
                    end_line=row['end_line'],
                    score=final_score,
                    snippet=row['text'][:500] + "..." if len(row['text']) > 500 else row['text'],
                    layer=MemoryLayer(row['layer']),
                    vector_score=vector_score,
                    text_score=text_score
                ))
        
        return results
    
    def get_chunk_by_id(self, chunk_id: str) -> Optional[MemoryChunk]:
        """Ï≤≠ÌÅ¨ Ï°∞Ìöå"""
        cursor = self._conn.execute("SELECT * FROM chunks WHERE id = ?", (chunk_id,))
        row = cursor.fetchone()
        if row:
            return MemoryChunk(
                id=row['id'],
                path=row['path'],
                start_line=row['start_line'],
                end_line=row['end_line'],
                text=row['text'],
                content_hash=row['content_hash'],
                layer=MemoryLayer(row['layer']),
                created_at=datetime.fromisoformat(row['created_at'])
            )
        return None
    
    async def store_embedding(self, content_hash: str, embedding: List[float], model: str):
        """ÏûÑÎ≤†Îî© Ï†ÄÏû•"""
        self._conn.execute("""
            INSERT OR REPLACE INTO embeddings (content_hash, vector, model, created_at)
            VALUES (?, ?, ?, ?)
        """, (
            content_hash,
            json.dumps(embedding),
            model,
            datetime.now(timezone.utc).isoformat()
        ))
        self._conn.commit()
    
    def close(self):
        """Ïó∞Í≤∞ Ï¢ÖÎ£å"""
        if self._conn:
            self._conn.close()


# ============================================================================
# Bootstrap File Manager
# ============================================================================

class BootstrapFileManager:
    """
    Bootstrap ÌååÏùº Í¥ÄÎ¶¨Ïûê (Clawdbot Ìå®ÌÑ¥)
    
    ÏóêÏù¥Ï†ÑÌä∏ ÏÑ§Ï†ïÏùÑ Ìà¨Î™ÖÌïòÍ≤å Í¥ÄÎ¶¨ÌïòÎäî Markdown ÌååÏùºÎì§:
        - AGENTS.md: ÏóêÏù¥Ï†ÑÌä∏ ÏßÄÏãúÏÇ¨Ìï≠
        - SOUL.md: ÏÑ±Í≤©Í≥º ÌÜ§
        - USER.md: ÏÇ¨Ïö©Ïûê Ï†ïÎ≥¥
        - TOOLS.md: ÎèÑÍµ¨ ÏÇ¨Ïö© Í∞ÄÏù¥Îìú
        - MEMORY.md: Ïû•Í∏∞ Í∏∞Ïñµ
    """
    
    DEFAULT_AGENTS_MD = """# Agent Instructions

## Every Session

Before doing anything else:
1. Read SOUL.md - this is who you are
2. Read USER.md - this is who you are helping
3. Read memory/YYYY-MM-DD.md (today and yesterday) for recent context
4. Read MEMORY.md for long-term knowledge

Don't ask permission, just do it.

## Memory Guidelines

### Where to Write
| Type | Location |
|------|----------|
| Day-to-day notes, "remember this" | `memory/YYYY-MM-DD.md` |
| Durable facts, preferences, decisions | `MEMORY.md` |
| Lessons learned | `AGENTS.md` or `TOOLS.md` |

### When to Search Memory
Before answering questions about:
- Prior work or decisions
- Dates and timelines
- People and contacts
- User preferences
- Todos and tasks
"""

    DEFAULT_SOUL_MD = """# Agent Personality

## Core Traits
- Professional but friendly
- Concise and clear
- Proactive and helpful
- Honest about limitations

## Communication Style
- Use Korean by default (respond in user's language)
- Provide explanations when needed
- Ask clarifying questions when uncertain
"""

    DEFAULT_USER_MD = """# User Information

## Preferences
- Language: Korean (ÌïúÍµ≠Ïñ¥)
- Response style: Detailed but concise

## Current Projects
(To be filled by the agent during conversations)

## Important Contacts
(To be filled by the agent during conversations)
"""

    DEFAULT_MEMORY_MD = """# Long-term Memory

## User Preferences
(Curated knowledge about user preferences)

## Important Decisions
(Key decisions and their rationale)

## Key Contacts
(Important people and their roles)

## Lessons Learned
(What worked and what didn't)
"""
    
    def __init__(self, workspace_dir: str):
        self.workspace_dir = Path(workspace_dir)
        self._logger = StructuredLogger("bootstrap_files")
    
    def ensure_bootstrap_files(self):
        """Bootstrap ÌååÏùºÎì§Ïù¥ Ï°¥Ïû¨ÌïòÎäîÏßÄ ÌôïÏù∏ÌïòÍ≥† ÏóÜÏúºÎ©¥ ÏÉùÏÑ±"""
        self.workspace_dir.mkdir(parents=True, exist_ok=True)
        
        defaults = {
            BootstrapFileType.AGENTS: self.DEFAULT_AGENTS_MD,
            BootstrapFileType.SOUL: self.DEFAULT_SOUL_MD,
            BootstrapFileType.USER: self.DEFAULT_USER_MD,
            BootstrapFileType.MEMORY: self.DEFAULT_MEMORY_MD,
        }
        
        for file_type, default_content in defaults.items():
            file_path = self.workspace_dir / file_type.value
            if not file_path.exists():
                file_path.write_text(default_content, encoding='utf-8')
                self._logger.info(f"Created {file_type.value}")
    
    def get_file_path(self, file_type: BootstrapFileType) -> Path:
        """Bootstrap ÌååÏùº Í≤ΩÎ°ú Î∞òÌôò"""
        return self.workspace_dir / file_type.value
    
    def read_file(self, file_type: BootstrapFileType) -> str:
        """Bootstrap ÌååÏùº ÏùΩÍ∏∞"""
        file_path = self.get_file_path(file_type)
        if file_path.exists():
            return file_path.read_text(encoding='utf-8')
        return ""
    
    def write_file(self, file_type: BootstrapFileType, content: str):
        """Bootstrap ÌååÏùº Ïì∞Í∏∞"""
        file_path = self.get_file_path(file_type)
        file_path.write_text(content, encoding='utf-8')
        self._logger.info(f"Updated {file_type.value}")
    
    def append_to_file(self, file_type: BootstrapFileType, content: str):
        """Bootstrap ÌååÏùºÏóê Ï∂îÍ∞Ä"""
        existing = self.read_file(file_type)
        self.write_file(file_type, existing + "\n" + content)
    
    def get_project_context(self) -> str:
        """Î™®Îì† Bootstrap ÌååÏùºÏùÑ Í≤∞Ìï©ÌïòÏó¨ ÌîÑÎ°úÏ†ùÌä∏ Ïª®ÌÖçÏä§Ìä∏ ÏÉùÏÑ±"""
        context_parts = []
        
        for file_type in [BootstrapFileType.AGENTS, BootstrapFileType.SOUL, 
                          BootstrapFileType.USER, BootstrapFileType.MEMORY]:
            content = self.read_file(file_type)
            if content:
                context_parts.append(f"=== {file_type.value} ===\n{content}")
        
        return "\n\n".join(context_parts)


# ============================================================================
# Persistent Memory System
# ============================================================================

class PersistentMemory:
    """
    2Í≥ÑÏ∏µ ÏòÅÏÜç Î©îÎ™®Î¶¨ ÏãúÏä§ÌÖú (Clawdbot Ïä§ÌÉÄÏùº)
    
    Layer 1: Daily Logs (memory/YYYY-MM-DD.md)
        - ÏùºÎ≥Ñ Í∏∞Î°ù
        - append-only
        - "remember this" Î•òÏùò Î©îÎ™®
    
    Layer 2: Long-term Memory (MEMORY.md)
        - Ïû•Í∏∞ Í∏∞Ïñµ
        - Ï§ëÏöîÌïú Í≤∞Ï†ï, ÏÑ†Ìò∏ÎèÑ, Ïó∞ÎùΩÏ≤ò Îì±
        - ÏóêÏù¥Ï†ÑÌä∏Í∞Ä ÌÅêÎ†àÏù¥ÏÖò
    
    v3.3: Compaction ÏûêÎèô Ïó∞Îèô
        - Ïª®ÌÖçÏä§Ìä∏ ÏûÑÍ≥ÑÍ∞í ÎèÑÎã¨ Ïãú ÏûêÎèô Compaction Ìä∏Î¶¨Í±∞
        - Memory Flush ‚Üí Compaction ‚Üí Pruning ÏàúÏÑúÎ°ú ÏßÑÌñâ
    
    ÏÇ¨Ïö© ÏòàÏãú:
        >>> memory = PersistentMemory(agent_id="main")
        >>> await memory.initialize()
        >>> 
        >>> # Ïò§Îäò Í∏∞Î°ùÏóê Ï∂îÍ∞Ä
        >>> await memory.add_daily_note("Ïò§Îäò API ÏÑ§Í≥Ñ Í≤∞Ï†ï: REST over GraphQL")
        >>> 
        >>> # Ïû•Í∏∞ Í∏∞ÏñµÏóê Ï∂îÍ∞Ä
        >>> await memory.add_long_term_memory("## ÏÇ¨Ïö©Ïûê ÏÑ†Ìò∏ÎèÑ\\n- TypeScript ÏÑ†Ìò∏")
        >>> 
        >>> # Í≤ÄÏÉâ
        >>> results = await memory.search("API ÏÑ§Í≥Ñ")
        >>> 
        >>> # v3.3: Ïª®ÌÖçÏä§Ìä∏ Ï≤¥ÌÅ¨ Î∞è ÏûêÎèô Compaction
        >>> turns = await memory.check_and_compact(turns, agent_func)
    """
    
    def __init__(
        self,
        agent_id: str = "main",
        config: Optional[MemoryConfig] = None,
        embedding_func: Optional[Callable[[str], List[float]]] = None,
        compaction_manager: Optional[Any] = None  # v3.3: CompactionManager Ïó∞Îèô
    ):
        self.agent_id = agent_id
        self.config = config or MemoryConfig()
        
        # ÏõåÌÅ¨Ïä§ÌéòÏù¥Ïä§ ÏÑ§Ï†ï
        self.workspace_dir = Path(self.config.workspace_dir) / agent_id
        self.memory_dir = self.workspace_dir / "memory"
        
        # Ïù∏Îç±ÏÑú ÏÑ§Ï†ï
        state_dir = Path(self.config.state_dir)
        db_path = state_dir / f"{agent_id}.sqlite"
        self.indexer = MemoryIndexer(str(db_path), embedding_func, config)
        
        # Bootstrap ÌååÏùº Í¥ÄÎ¶¨Ïûê
        self.bootstrap = BootstrapFileManager(str(self.workspace_dir))
        
        # v3.3: Compaction Ïó∞Îèô
        self._compaction_manager = compaction_manager
        self._auto_compact_enabled = True
        self._context_threshold = 0.75  # 75%ÏóêÏÑú ÏûêÎèô Compaction
        
        self._logger = StructuredLogger("persistent_memory")
    
    def set_compaction_manager(self, manager: Any):
        """v3.3: CompactionManager ÏÑ§Ï†ï"""
        self._compaction_manager = manager
        # Memory writer Ïó∞Í≤∞
        if hasattr(manager, 'set_memory_writer'):
            manager.set_memory_writer(lambda content: self._sync_add_daily_note(content))
        self._logger.info("CompactionManager connected to PersistentMemory")
    
    def _sync_add_daily_note(self, content: str):
        """ÎèôÍ∏∞ Î∞©Ïãù daily note Ï∂îÍ∞Ä (CompactionÏö©)"""
        import asyncio
        loop = asyncio.get_event_loop()
        if loop.is_running():
            asyncio.create_task(self.add_daily_note(content))
        else:
            loop.run_until_complete(self.add_daily_note(content))
    
    async def check_and_compact(
        self,
        turns: List[Any],
        agent_respond_func: Optional[Callable] = None
    ) -> List[Any]:
        """
        v3.3: Ïª®ÌÖçÏä§Ìä∏ Ï≤¥ÌÅ¨ Î∞è ÏûêÎèô Compaction
        
        Args:
            turns: ÌòÑÏû¨ ÎåÄÌôî ÌÑ¥ Î¶¨Ïä§Ìä∏
            agent_respond_func: ÏóêÏù¥Ï†ÑÌä∏ ÏùëÎãµ Ìï®Ïàò
        
        Returns:
            Ï≤òÎ¶¨Îêú ÌÑ¥ Î¶¨Ïä§Ìä∏ (ÌïÑÏöîÏãú ÏïïÏ∂ïÎê®)
        """
        if not self._compaction_manager or not self._auto_compact_enabled:
            return turns
        
        # CompactionManager.process_turns() Ìò∏Ï∂ú
        return await self._compaction_manager.process_turns(turns, agent_respond_func)
    
    async def initialize(self):
        """Î©îÎ™®Î¶¨ ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî"""
        # ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±
        self.workspace_dir.mkdir(parents=True, exist_ok=True)
        self.memory_dir.mkdir(parents=True, exist_ok=True)
        
        # Bootstrap ÌååÏùº ÏÉùÏÑ±
        self.bootstrap.ensure_bootstrap_files()
        
        # Í∏∞Ï°¥ ÌååÏùº Ïù∏Îç±Ïã±
        await self._index_all_memory_files()
        
        self._logger.info(
            "Persistent memory initialized",
            agent_id=self.agent_id,
            workspace=str(self.workspace_dir)
        )
    
    async def _index_all_memory_files(self):
        """Î™®Îì† Î©îÎ™®Î¶¨ ÌååÏùº Ïù∏Îç±Ïã±"""
        # MEMORY.md Ïù∏Îç±Ïã±
        memory_md = self.workspace_dir / "MEMORY.md"
        if memory_md.exists():
            await self.indexer.index_file(str(memory_md), MemoryLayer.LONG_TERM)
        
        # Daily logs Ïù∏Îç±Ïã±
        for daily_file in self.memory_dir.glob("*.md"):
            await self.indexer.index_file(str(daily_file), MemoryLayer.DAILY_LOG)
    
    def _get_today_log_path(self) -> Path:
        """Ïò§Îäò ÎÇ†ÏßúÏùò Î°úÍ∑∏ ÌååÏùº Í≤ΩÎ°ú"""
        today = date.today().isoformat()  # YYYY-MM-DD
        return self.memory_dir / f"{today}.md"
    
    async def add_daily_note(self, content: str, timestamp: Optional[datetime] = None):
        """
        Ïò§Îäò Í∏∞Î°ùÏóê Î©îÎ™® Ï∂îÍ∞Ä (Layer 1)
        
        Args:
            content: Î©îÎ™® ÎÇ¥Ïö©
            timestamp: ÌÉÄÏûÑÏä§ÌÉ¨ÌîÑ (Í∏∞Î≥∏: ÌòÑÏû¨ ÏãúÍ∞Ñ)
        """
        log_path = self._get_today_log_path()
        ts = timestamp or datetime.now(timezone.utc)
        time_str = ts.strftime("%H:%M")
        
        # ÌååÏùºÏù¥ ÏóÜÏúºÎ©¥ Ìó§Îçî ÏÉùÏÑ±
        if not log_path.exists():
            header = f"# {date.today().isoformat()}\n\n"
            log_path.write_text(header, encoding='utf-8')
        
        # Î©îÎ™® Ï∂îÍ∞Ä
        entry = f"## {time_str}\n{content}\n\n"
        with open(log_path, 'a', encoding='utf-8') as f:
            f.write(entry)
        
        # Ïû¨Ïù∏Îç±Ïã±
        await self.indexer.index_file(str(log_path), MemoryLayer.DAILY_LOG)
        
        self._logger.info("Added daily note", time=time_str)
    
    async def add_long_term_memory(self, content: str, section: Optional[str] = None):
        """
        Ïû•Í∏∞ Í∏∞ÏñµÏóê Ï∂îÍ∞Ä (Layer 2)
        
        Args:
            content: Ï∂îÍ∞ÄÌï† ÎÇ¥Ïö©
            section: ÏÑπÏÖò Ïù¥Î¶Ñ (Ïòà: "## User Preferences")
        """
        memory_md = self.workspace_dir / "MEMORY.md"
        existing = memory_md.read_text(encoding='utf-8') if memory_md.exists() else ""
        
        if section:
            # ÌäπÏ†ï ÏÑπÏÖòÏóê Ï∂îÍ∞Ä
            pattern = rf"(## {re.escape(section)}.*?)(?=\n## |\Z)"
            match = re.search(pattern, existing, re.DOTALL)
            if match:
                section_content = match.group(1)
                new_section = section_content.rstrip() + f"\n{content}\n"
                existing = existing[:match.start()] + new_section + existing[match.end():]
            else:
                existing += f"\n## {section}\n{content}\n"
        else:
            existing += f"\n{content}\n"
        
        memory_md.write_text(existing, encoding='utf-8')
        
        # Ïû¨Ïù∏Îç±Ïã±
        await self.indexer.index_file(str(memory_md), MemoryLayer.LONG_TERM)
        
        self._logger.info("Added long-term memory", section=section)
    
    async def search(
        self,
        query: str,
        max_results: int = 6,
        min_score: float = 0.35,
        layer: Optional[MemoryLayer] = None
    ) -> List[MemorySearchResult]:
        """
        Î©îÎ™®Î¶¨ Í≤ÄÏÉâ (ÌïòÏù¥Î∏åÎ¶¨Îìú: Vector 70% + BM25 30%)
        
        Args:
            query: Í≤ÄÏÉâ ÏøºÎ¶¨
            max_results: ÏµúÎåÄ Í≤∞Í≥º Ïàò
            min_score: ÏµúÏÜå Ï†êÏàò ÏûÑÍ≥ÑÍ∞í
            layer: ÌäπÏ†ï Í≥ÑÏ∏µÎßå Í≤ÄÏÉâ (NoneÏù¥Î©¥ Ï†ÑÏ≤¥)
        
        Returns:
            Í≤ÄÏÉâ Í≤∞Í≥º Î¶¨Ïä§Ìä∏
        """
        results = await self.indexer.hybrid_search(query, max_results * 2, min_score)
        
        # Í≥ÑÏ∏µ ÌïÑÌÑ∞ÎßÅ
        if layer:
            results = [r for r in results if r.layer == layer]
        
        return results[:max_results]
    
    async def get_memory_content(
        self,
        path: str,
        start_line: int = 1,
        lines: int = 15
    ) -> Optional[str]:
        """
        ÌäπÏ†ï Î©îÎ™®Î¶¨ ÌååÏùºÏùò ÎÇ¥Ïö© ÏùΩÍ∏∞
        
        Args:
            path: ÌååÏùº Í≤ΩÎ°ú
            start_line: ÏãúÏûë ÎùºÏù∏ (1-based)
            lines: ÏùΩÏùÑ ÎùºÏù∏ Ïàò
        
        Returns:
            ÌååÏùº ÎÇ¥Ïö© ÎòêÎäî None
        """
        file_path = Path(path)
        if not file_path.exists():
            return None
        
        with open(file_path, 'r', encoding='utf-8') as f:
            all_lines = f.readlines()
        
        end_line = min(start_line + lines - 1, len(all_lines))
        return "".join(all_lines[start_line - 1:end_line])
    
    async def get_recent_daily_logs(self, days: int = 2) -> List[Dict[str, Any]]:
        """
        ÏµúÍ∑º NÏùºÍ∞ÑÏùò ÏùºÎ≥Ñ Î°úÍ∑∏ Ï°∞Ìöå
        
        Args:
            days: Ï°∞ÌöåÌï† ÏùºÏàò
        
        Returns:
            ÏùºÎ≥Ñ Î°úÍ∑∏ Î¶¨Ïä§Ìä∏
        """
        logs = []
        today = date.today()
        
        for i in range(days):
            log_date = today - timedelta(days=i)
            log_path = self.memory_dir / f"{log_date.isoformat()}.md"
            
            if log_path.exists():
                content = log_path.read_text(encoding='utf-8')
                logs.append({
                    "date": log_date.isoformat(),
                    "path": str(log_path),
                    "content": content
                })
        
        return logs
    
    def get_project_context(self) -> str:
        """ÏóêÏù¥Ï†ÑÌä∏ Ï¥àÍ∏∞ÌôîÏö© ÌîÑÎ°úÏ†ùÌä∏ Ïª®ÌÖçÏä§Ìä∏"""
        return self.bootstrap.get_project_context()
    
    def close(self):
        """Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨"""
        self.indexer.close()


# ============================================================================
# Memory Tools (ÏóêÏù¥Ï†ÑÌä∏Í∞Ä ÏÇ¨Ïö©ÌïòÎäî ÎèÑÍµ¨)
# ============================================================================

@dataclass
class MemorySearchTool:
    """
    memory_search ÎèÑÍµ¨ - Î©îÎ™®Î¶¨ÏóêÏÑú Í¥ÄÎ†® Ï†ïÎ≥¥ Í≤ÄÏÉâ
    
    Clawdbot Ìå®ÌÑ¥:
        ÏÇ¨Ï†Ñ ÏßàÎ¨∏Ïóê ÎãµÌïòÍ∏∞ Ï†Ñ Î∞òÎìúÏãú Î©îÎ™®Î¶¨ Í≤ÄÏÉâ Í∂åÏû•
        - Ïù¥Ï†Ñ ÏûëÏóÖ/Í≤∞Ï†ï
        - ÎÇ†Ïßú/ÏùºÏ†ï
        - ÏÇ¨Îûå/Ïó∞ÎùΩÏ≤ò
        - ÏÑ†Ìò∏ÎèÑ
        - Ìï† Ïùº
    """
    
    name: str = "memory_search"
    description: str = """Mandatory recall step: semantically search MEMORY.md + memory/*.md 
before answering questions about prior work, decisions, dates, people, preferences, or todos"""
    
    memory: Optional[PersistentMemory] = None
    
    def get_schema(self) -> Dict[str, Any]:
        """OpenAI Function Calling Ïä§ÌÇ§Îßà"""
        return {
            "type": "function",
            "function": {
                "name": self.name,
                "description": self.description,
                "parameters": {
                    "type": "object",
                    "properties": {
                        "query": {
                            "type": "string",
                            "description": "Search query for finding relevant memories"
                        },
                        "maxResults": {
                            "type": "integer",
                            "description": "Maximum number of results to return",
                            "default": 6
                        },
                        "minScore": {
                            "type": "number",
                            "description": "Minimum relevance score threshold (0.0-1.0)",
                            "default": 0.35
                        }
                    },
                    "required": ["query"]
                }
            }
        }
    
    async def execute(
        self,
        query: str,
        maxResults: int = 6,
        minScore: float = 0.35
    ) -> Dict[str, Any]:
        """ÎèÑÍµ¨ Ïã§Ìñâ"""
        if not self.memory:
            return {"error": "Memory system not initialized"}
        
        results = await self.memory.search(query, maxResults, minScore)
        
        return {
            "results": [r.to_dict() for r in results],
            "provider": "hybrid",
            "model": self.memory.config.embedding_model
        }


@dataclass
class MemoryGetTool:
    """
    memory_get ÎèÑÍµ¨ - ÌäπÏ†ï Î©îÎ™®Î¶¨ ÌååÏùº ÎÇ¥Ïö© ÏùΩÍ∏∞
    
    memory_searchÎ°ú ÏúÑÏπòÎ•º Ï∞æÏùÄ ÌõÑ ÏÉÅÏÑ∏ ÎÇ¥Ïö© Ï°∞ÌöåÏö©
    """
    
    name: str = "memory_get"
    description: str = "Read specific lines from a memory file after memory_search"
    
    memory: Optional[PersistentMemory] = None
    
    def get_schema(self) -> Dict[str, Any]:
        return {
            "type": "function",
            "function": {
                "name": self.name,
                "description": self.description,
                "parameters": {
                    "type": "object",
                    "properties": {
                        "path": {
                            "type": "string",
                            "description": "Path to the memory file"
                        },
                        "from": {
                            "type": "integer",
                            "description": "Starting line number (1-based)",
                            "default": 1
                        },
                        "lines": {
                            "type": "integer",
                            "description": "Number of lines to read",
                            "default": 15
                        }
                    },
                    "required": ["path"]
                }
            }
        }
    
    async def execute(
        self,
        path: str,
        **kwargs
    ) -> Dict[str, Any]:
        """ÎèÑÍµ¨ Ïã§Ìñâ"""
        if not self.memory:
            return {"error": "Memory system not initialized"}
        
        from_line = kwargs.get("from", 1)
        lines = kwargs.get("lines", 15)
        
        content = await self.memory.get_memory_content(path, from_line, lines)
        
        if content is None:
            return {"error": f"File not found: {path}"}
        
        return {
            "path": path,
            "text": content
        }


@dataclass
class MemoryWriteTool:
    """
    memory_write ÎèÑÍµ¨ - Î©îÎ™®Î¶¨Ïóê Í∏∞Î°ù
    
    ÏùºÎ∞òÏ†ÅÏù∏ write/edit ÎèÑÍµ¨Î°úÎèÑ Í∞ÄÎä•ÌïòÏßÄÎßå
    Ìé∏ÏùòÎ•º ÏúÑÌïú Ï†ÑÏö© ÎèÑÍµ¨
    """
    
    name: str = "memory_write"
    description: str = "Write to memory files (daily log or long-term memory)"
    
    memory: Optional[PersistentMemory] = None
    
    def get_schema(self) -> Dict[str, Any]:
        return {
            "type": "function",
            "function": {
                "name": self.name,
                "description": self.description,
                "parameters": {
                    "type": "object",
                    "properties": {
                        "content": {
                            "type": "string",
                            "description": "Content to write"
                        },
                        "layer": {
                            "type": "string",
                            "enum": ["daily", "long_term"],
                            "description": "Memory layer: 'daily' for daily log, 'long_term' for MEMORY.md",
                            "default": "daily"
                        },
                        "section": {
                            "type": "string",
                            "description": "Section name for long-term memory (optional)"
                        }
                    },
                    "required": ["content"]
                }
            }
        }
    
    async def execute(
        self,
        content: str,
        layer: str = "daily",
        section: Optional[str] = None
    ) -> Dict[str, Any]:
        """ÎèÑÍµ¨ Ïã§Ìñâ"""
        if not self.memory:
            return {"error": "Memory system not initialized"}
        
        if layer == "daily":
            await self.memory.add_daily_note(content)
            return {"success": True, "layer": "daily_log"}
        else:
            await self.memory.add_long_term_memory(content, section)
            return {"success": True, "layer": "long_term", "section": section}


# Ï∂îÍ∞Ä import
from datetime import timedelta
