#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Unified Agent Framework - í‰ê°€ ëª¨ë“ˆ (Evaluation Module)

================================================================================
ğŸ“ íŒŒì¼ ìœ„ì¹˜: unified_agent/evaluation.py
ğŸ“‹ ì—­í• : ì—ì´ì „íŠ¸ í’ˆì§ˆ ì¸¡ì •, PDCA í‰ê°€, LLM-as-Judge, Check-Act Iteration
ğŸ“… ìµœì¢… ì—…ë°ì´íŠ¸: 2026ë…„ 2ì›” 4ì¼
ğŸ“¦ ë²„ì „: v3.5.0
âœ… í…ŒìŠ¤íŠ¸: test_new_modules.py, test_v35_scenarios.py
ğŸ”— ì°¸ì¡°: bkit-claude-code PDCA ë°©ë²•ë¡ 
================================================================================

ğŸ¯ ì£¼ìš” êµ¬ì„± ìš”ì†Œ:
    1. PDCAEvaluator - PDCA(Plan-Do-Check-Act) ì‚¬ì´í´ í‰ê°€
    2. LLMJudge - LLM ê¸°ë°˜ í’ˆì§ˆ í‰ê°€ (LLM-as-Judge)
    3. CheckActIterator - Check-Act ë°˜ë³µ ìµœì í™” (Evaluator-Optimizer íŒ¨í„´)
    4. AgentBenchmark - ì—ì´ì „íŠ¸ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸
    5. QualityMetrics - í’ˆì§ˆ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ë¶„ì„
    6. GapAnalyzer - ê³„íš vs ì‹¤ì œ ê°­ ë¶„ì„

ğŸ”§ 2026ë…„ 2ì›” ê¸°ëŠ¥ (bkit ì˜ê°):
    - PDCA ë°©ë²•ë¡  ê¸°ë°˜ ì²´ê³„ì  í‰ê°€
    - Evaluator-Optimizer íŒ¨í„´ (ìë™ ê°œì„  ë£¨í”„)
    - Check-Act Iteration (90% ì„ê³„ê°’, ìµœëŒ€ 5íšŒ)
    - LLM-as-Judge ë‹¤ì°¨ì› í‰ê°€
    - ê°­ ë¶„ì„ ë° ìë™ ìˆ˜ì • ì œì•ˆ

ğŸ“Œ ì‚¬ìš© ì˜ˆì‹œ:
    >>> from unified_agent.evaluation import (
    ...     PDCAEvaluator, LLMJudge, CheckActIterator,
    ...     EvaluationConfig, QualityMetrics
    ... )
    >>>
    >>> # PDCA í‰ê°€
    >>> evaluator = PDCAEvaluator()
    >>> result = await evaluator.evaluate_cycle(
    ...     plan=plan_doc,
    ...     implementation=code,
    ...     expected_outcome=spec
    ... )
    >>>
    >>> # Check-Act ìë™ ê°œì„  ë£¨í”„
    >>> iterator = CheckActIterator(
    ...     evaluator=LLMJudge(),
    ...     optimizer=optimizer_agent,
    ...     threshold=0.9,      # 90% ëª©í‘œ
    ...     max_iterations=5    # ìµœëŒ€ 5íšŒ
    ... )
    >>> final_result = await iterator.iterate(initial_output)

âš ï¸ ì£¼ì˜ì‚¬í•­:
    - LLM-as-JudgeëŠ” í‰ê°€ ëª¨ë¸ì´ ëŒ€ìƒ ëª¨ë¸ë³´ë‹¤ ê°•ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.
    - Check-Act Iterationì€ í† í° ì‚¬ìš©ëŸ‰ì´ ì¦ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - 90% ì„ê³„ê°’ì€ ë„ë©”ì¸ì— ë”°ë¼ ì¡°ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.

ğŸ”— ê´€ë ¨ ë¬¸ì„œ:
    - bkit PDCA: https://github.com/popup-studio-ai/bkit-claude-code
    - Anthropic Evaluator-Optimizer: https://www.anthropic.com/research
"""

import asyncio
import json
import logging
import statistics
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from datetime import datetime, timezone
from enum import Enum
from typing import (
    Any, Callable, Dict, Generic, List, Optional, 
    Protocol, Tuple, TypeVar, Union
)

__all__ = [
    # Enums
    "PDCAPhase",
    "EvaluationDimension",
    "QualityLevel",
    "GapSeverity",
    # Config
    "EvaluationConfig",
    "JudgeConfig",
    "IterationConfig",
    # Results
    "EvaluationResult",
    "JudgeVerdict",
    "GapAnalysisResult",
    "IterationResult",
    "BenchmarkResult",
    "QualityReport",
    # Core Components
    "PDCAEvaluator",
    "LLMJudge",
    "CheckActIterator",
    "GapAnalyzer",
    "AgentBenchmark",
    "QualityMetrics",
    # Protocols
    "Evaluator",
    "Optimizer",
]


T = TypeVar("T")


# ============================================================================
# Enums
# ============================================================================

class PDCAPhase(str, Enum):
    """PDCA ì‚¬ì´í´ ë‹¨ê³„"""
    PLAN = "plan"         # ê³„íš: ëª©í‘œ ë° í”„ë¡œì„¸ìŠ¤ ì •ì˜
    DO = "do"             # ì‹¤í–‰: ê³„íš ì‹¤í–‰
    CHECK = "check"       # ì ê²€: ê²°ê³¼ í‰ê°€
    ACT = "act"           # ì¡°ì¹˜: ê°œì„  ì ìš©


class EvaluationDimension(str, Enum):
    """í‰ê°€ ì°¨ì›"""
    TASK_COMPLETION = "task_completion"       # ì‘ì—… ì™„ë£Œë„
    FACTUAL_ACCURACY = "factual_accuracy"     # ì‚¬ì‹¤ ì •í™•ë„
    RESPONSE_QUALITY = "response_quality"     # ì‘ë‹µ í’ˆì§ˆ
    CODE_QUALITY = "code_quality"             # ì½”ë“œ í’ˆì§ˆ
    TOOL_USAGE = "tool_usage"                 # ë„êµ¬ ì‚¬ìš© íš¨ìœ¨
    INSTRUCTION_FOLLOWING = "instruction_following"  # ì§€ì‹œ ì¤€ìˆ˜
    CREATIVITY = "creativity"                 # ì°½ì˜ì„±
    EFFICIENCY = "efficiency"                 # íš¨ìœ¨ì„±
    SAFETY = "safety"                         # ì•ˆì „ì„±


class QualityLevel(str, Enum):
    """í’ˆì§ˆ ìˆ˜ì¤€"""
    EXCELLENT = "excellent"    # 90%+
    GOOD = "good"              # 70-89%
    ACCEPTABLE = "acceptable"  # 50-69%
    POOR = "poor"              # 30-49%
    FAIL = "fail"              # 0-29%


class GapSeverity(str, Enum):
    """ê°­ ì‹¬ê°ë„"""
    CRITICAL = "critical"      # ì¹˜ëª…ì : ì¦‰ì‹œ ìˆ˜ì • í•„ìš”
    MAJOR = "major"            # ì£¼ìš”: ë¹ ë¥¸ ìˆ˜ì • í•„ìš”
    MINOR = "minor"            # ê²½ë¯¸: ê°œì„  ê¶Œì¥
    TRIVIAL = "trivial"        # ì‚¬ì†Œ: ì„ íƒì  ê°œì„ 


# ============================================================================
# Protocols
# ============================================================================

class Evaluator(Protocol):
    """í‰ê°€ì í”„ë¡œí† ì½œ"""
    async def evaluate(self, output: str, context: Dict[str, Any]) -> "EvaluationResult":
        ...


class Optimizer(Protocol):
    """ìµœì í™”ì í”„ë¡œí† ì½œ"""
    async def optimize(self, output: str, feedback: str) -> str:
        ...


# ============================================================================
# Data Classes - Config
# ============================================================================

@dataclass
class EvaluationConfig:
    """
    í‰ê°€ ì„¤ì •
    
    Attributes:
        dimensions: í‰ê°€ ì°¨ì› ëª©ë¡
        weights: ì°¨ì›ë³„ ê°€ì¤‘ì¹˜
        threshold: í†µê³¼ ì„ê³„ê°’ (0.0 ~ 1.0)
        model: í‰ê°€ì— ì‚¬ìš©í•  LLM ëª¨ë¸
        detailed_feedback: ìƒì„¸ í”¼ë“œë°± ìƒì„± ì—¬ë¶€
    """
    dimensions: List[EvaluationDimension] = field(default_factory=lambda: [
        EvaluationDimension.TASK_COMPLETION,
        EvaluationDimension.FACTUAL_ACCURACY,
        EvaluationDimension.RESPONSE_QUALITY,
    ])
    weights: Dict[EvaluationDimension, float] = field(default_factory=dict)
    threshold: float = 0.7  # 70% ê¸°ë³¸ ì„ê³„ê°’
    model: str = "gpt-5.2"
    detailed_feedback: bool = True
    
    def __post_init__(self):
        # ê°€ì¤‘ì¹˜ ê¸°ë³¸ê°’ ì„¤ì •
        if not self.weights:
            self.weights = {dim: 1.0 / len(self.dimensions) for dim in self.dimensions}


@dataclass
class JudgeConfig:
    """
    LLM Judge ì„¤ì •
    
    Attributes:
        judge_model: íŒë‹¨ì— ì‚¬ìš©í•  ëª¨ë¸
        reference_model: ì°¸ì¡° ëª¨ë¸ (ë¹„êµ í‰ê°€ìš©)
        rubric: í‰ê°€ ë£¨ë¸Œë¦­ (ì ìˆ˜ ê¸°ì¤€)
        temperature: ì¼ê´€ì„±ì„ ìœ„í•´ ë‚®ì€ ê°’ ê¶Œì¥
        multi_judge: ë‹¤ì¤‘ íŒì‚¬ ì•™ìƒë¸” ì‚¬ìš©
    """
    judge_model: str = "gpt-5.2"
    reference_model: Optional[str] = None
    rubric: Optional[str] = None
    temperature: float = 0.1
    multi_judge: bool = False
    num_judges: int = 3


@dataclass
class IterationConfig:
    """
    Check-Act Iteration ì„¤ì • (bkit ìŠ¤íƒ€ì¼)
    
    Attributes:
        threshold: ëª©í‘œ í’ˆì§ˆ ì„ê³„ê°’ (ê¸°ë³¸: 90%)
        max_iterations: ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ (ê¸°ë³¸: 5íšŒ)
        improvement_threshold: ê°œì„  ì—†ìŒ íŒë‹¨ ì„ê³„ê°’
        early_stop: ëª©í‘œ ë‹¬ì„± ì‹œ ì¡°ê¸° ì¢…ë£Œ
        verbose: ìƒì„¸ ë¡œê·¸ ì¶œë ¥
    """
    threshold: float = 0.9           # 90% ëª©í‘œ (bkit ê¸°ì¤€)
    max_iterations: int = 5          # ìµœëŒ€ 5íšŒ (bkit ê¸°ì¤€)
    improvement_threshold: float = 0.01  # 1% ë¯¸ë§Œ ê°œì„  ì‹œ ì¢…ë£Œ
    early_stop: bool = True
    verbose: bool = True


# ============================================================================
# Data Classes - Results
# ============================================================================

@dataclass
class EvaluationResult:
    """
    í‰ê°€ ê²°ê³¼
    
    Attributes:
        overall_score: ì¢…í•© ì ìˆ˜ (0.0 ~ 1.0)
        dimension_scores: ì°¨ì›ë³„ ì ìˆ˜
        quality_level: í’ˆì§ˆ ìˆ˜ì¤€
        passed: ì„ê³„ê°’ í†µê³¼ ì—¬ë¶€
        feedback: í”¼ë“œë°± ë©”ì‹œì§€
        suggestions: ê°œì„  ì œì•ˆ
        metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°
    """
    overall_score: float
    dimension_scores: Dict[EvaluationDimension, float] = field(default_factory=dict)
    quality_level: QualityLevel = QualityLevel.ACCEPTABLE
    passed: bool = False
    feedback: str = ""
    suggestions: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)
    timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    
    def __post_init__(self):
        # í’ˆì§ˆ ìˆ˜ì¤€ ìë™ ê³„ì‚°
        if self.overall_score >= 0.9:
            self.quality_level = QualityLevel.EXCELLENT
        elif self.overall_score >= 0.7:
            self.quality_level = QualityLevel.GOOD
        elif self.overall_score >= 0.5:
            self.quality_level = QualityLevel.ACCEPTABLE
        elif self.overall_score >= 0.3:
            self.quality_level = QualityLevel.POOR
        else:
            self.quality_level = QualityLevel.FAIL


@dataclass
class JudgeVerdict:
    """
    LLM Judge íŒê²°
    
    Attributes:
        score: ì ìˆ˜ (0-10 ë˜ëŠ” 0-100)
        reasoning: íŒë‹¨ ê·¼ê±°
        strengths: ê°•ì 
        weaknesses: ì•½ì 
        comparison: ë¹„êµ ê²°ê³¼ (A/B í…ŒìŠ¤íŠ¸ ì‹œ)
    """
    score: float
    reasoning: str = ""
    strengths: List[str] = field(default_factory=list)
    weaknesses: List[str] = field(default_factory=list)
    comparison: Optional[str] = None  # "A > B", "A < B", "A = B"
    confidence: float = 1.0


@dataclass
class GapAnalysisResult:
    """
    ê°­ ë¶„ì„ ê²°ê³¼ (bkit ìŠ¤íƒ€ì¼)
    
    Attributes:
        match_rate: ì¼ì¹˜ìœ¨ (0.0 ~ 1.0)
        gaps: ë°œê²¬ëœ ê°­ ëª©ë¡
        missing_features: ëˆ„ë½ëœ ê¸°ëŠ¥
        extra_features: ì¶”ê°€ëœ ê¸°ëŠ¥ (ë²”ìœ„ ì´ˆê³¼)
        severity_summary: ì‹¬ê°ë„ë³„ ìš”ì•½
    """
    match_rate: float
    gaps: List[Dict[str, Any]] = field(default_factory=list)
    missing_features: List[str] = field(default_factory=list)
    extra_features: List[str] = field(default_factory=list)
    severity_summary: Dict[GapSeverity, int] = field(default_factory=dict)
    recommendations: List[str] = field(default_factory=list)


@dataclass
class IterationResult:
    """
    Check-Act Iteration ê²°ê³¼
    
    Attributes:
        final_output: ìµœì¢… ì¶œë ¥
        iterations: ë°˜ë³µ íšŸìˆ˜
        score_history: ì ìˆ˜ ì´ë ¥
        converged: ìˆ˜ë ´ ì—¬ë¶€
        improvement: ì´ ê°œì„ ìœ¨
    """
    final_output: str
    iterations: int
    score_history: List[float] = field(default_factory=list)
    feedback_history: List[str] = field(default_factory=list)
    converged: bool = False
    improvement: float = 0.0
    final_score: float = 0.0


@dataclass
class BenchmarkResult:
    """
    ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
    
    Attributes:
        agent_name: ì—ì´ì „íŠ¸ ì´ë¦„
        test_suite: í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì´ë¦„
        total_tests: ì´ í…ŒìŠ¤íŠ¸ ìˆ˜
        passed: í†µê³¼ ìˆ˜
        failed: ì‹¤íŒ¨ ìˆ˜
        scores: í…ŒìŠ¤íŠ¸ë³„ ì ìˆ˜
        avg_score: í‰ê·  ì ìˆ˜
        percentile: ë°±ë¶„ìœ„
    """
    agent_name: str
    test_suite: str
    total_tests: int
    passed: int = 0
    failed: int = 0
    scores: List[float] = field(default_factory=list)
    avg_score: float = 0.0
    percentile: float = 0.0
    details: List[Dict[str, Any]] = field(default_factory=list)
    
    def __post_init__(self):
        if self.scores:
            self.avg_score = statistics.mean(self.scores)


@dataclass
class QualityReport:
    """
    í’ˆì§ˆ ë¦¬í¬íŠ¸
    
    Attributes:
        summary: ìš”ì•½
        overall_score: ì¢…í•© ì ìˆ˜
        dimension_breakdown: ì°¨ì›ë³„ ë¶„ì„
        trends: íŠ¸ë Œë“œ (ì‹œê³„ì—´)
        recommendations: ê¶Œì¥ ì‚¬í•­
    """
    summary: str
    overall_score: float
    dimension_breakdown: Dict[str, float] = field(default_factory=dict)
    trends: List[Dict[str, Any]] = field(default_factory=list)
    recommendations: List[str] = field(default_factory=list)
    generated_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))


# ============================================================================
# PDCA Evaluator
# ============================================================================

class PDCAEvaluator:
    """
    PDCA ì‚¬ì´í´ í‰ê°€ì
    
    ================================================================================
    ğŸ“‹ ì—­í• : PDCA (Plan-Do-Check-Act) ë°©ë²•ë¡  ê¸°ë°˜ ì²´ê³„ì  í‰ê°€
    ğŸ“… ìµœì¢… ì—…ë°ì´íŠ¸: 2026ë…„ 2ì›” (bkit ì˜ê°)
    ================================================================================
    
    ğŸ¯ PDCA ì‚¬ì´í´:
        Plan  â†’ ëª©í‘œ ë° í”„ë¡œì„¸ìŠ¤ ì •ì˜ (ì„¤ê³„ ë¬¸ì„œ)
        Do    â†’ ê³„íš ì‹¤í–‰ (êµ¬í˜„)
        Check â†’ ê²°ê³¼ í‰ê°€ (ê°­ ë¶„ì„)
        Act   â†’ ê°œì„  ì ìš© (ìˆ˜ì • ë°˜ë³µ)
    
    ğŸ“Œ ì‚¬ìš© ì˜ˆì‹œ:
        >>> evaluator = PDCAEvaluator()
        >>> 
        >>> # ì „ì²´ ì‚¬ì´í´ í‰ê°€
        >>> result = await evaluator.evaluate_cycle(
        ...     plan="ì„¤ê³„ ë¬¸ì„œ ë‚´ìš©",
        ...     implementation="êµ¬í˜„ëœ ì½”ë“œ",
        ...     expected_outcome="ì˜ˆìƒ ê²°ê³¼"
        ... )
        >>> print(f"ì¼ì¹˜ìœ¨: {result.match_rate:.1%}")
        >>>
        >>> # ê°œë³„ ë‹¨ê³„ í‰ê°€
        >>> plan_result = await evaluator.evaluate_plan(plan_doc)
        >>> do_result = await evaluator.evaluate_do(implementation, plan_doc)
    """
    
    def __init__(
        self,
        config: Optional[EvaluationConfig] = None,
        llm_client: Optional[Any] = None
    ):
        self.config = config or EvaluationConfig()
        self.llm_client = llm_client
        self.logger = logging.getLogger(__name__)
        self.gap_analyzer = GapAnalyzer()
    
    async def evaluate_cycle(
        self,
        plan: str,
        implementation: str,
        expected_outcome: Optional[str] = None,
        actual_outcome: Optional[str] = None
    ) -> GapAnalysisResult:
        """
        ì „ì²´ PDCA ì‚¬ì´í´ í‰ê°€
        
        Args:
            plan: ê³„íš/ì„¤ê³„ ë¬¸ì„œ
            implementation: ì‹¤ì œ êµ¬í˜„
            expected_outcome: ì˜ˆìƒ ê²°ê³¼
            actual_outcome: ì‹¤ì œ ê²°ê³¼
        
        Returns:
            GapAnalysisResult: ê°­ ë¶„ì„ ê²°ê³¼
        """
        self.logger.info("PDCA ì‚¬ì´í´ í‰ê°€ ì‹œì‘")
        
        # ê°­ ë¶„ì„
        gap_result = await self.gap_analyzer.analyze(
            plan=plan,
            implementation=implementation,
            expected=expected_outcome,
            actual=actual_outcome
        )
        
        self.logger.info(f"PDCA í‰ê°€ ì™„ë£Œ: ì¼ì¹˜ìœ¨ {gap_result.match_rate:.1%}")
        return gap_result
    
    async def evaluate_plan(self, plan: str) -> EvaluationResult:
        """
        Plan ë‹¨ê³„ í‰ê°€ - ê³„íšì˜ ì™„ì „ì„± ë° ëª…í™•ì„± í‰ê°€
        
        Args:
            plan: ê³„íš/ì„¤ê³„ ë¬¸ì„œ
        
        Returns:
            EvaluationResult: í‰ê°€ ê²°ê³¼
        """
        criteria = {
            "completeness": "ëª¨ë“  í•„ìˆ˜ ìš”ì†Œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ê°€?",
            "clarity": "ìš”êµ¬ì‚¬í•­ì´ ëª…í™•í•˜ê²Œ ì •ì˜ë˜ì–´ ìˆëŠ”ê°€?",
            "feasibility": "ì‹¤í˜„ ê°€ëŠ¥í•œ ê³„íšì¸ê°€?",
            "measurability": "ì„±ê³µ ê¸°ì¤€ì´ ì¸¡ì • ê°€ëŠ¥í•œê°€?",
        }
        
        scores = {}
        feedback_parts = []
        
        for criterion, question in criteria.items():
            # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹± í‰ê°€ (LLM ì—†ì´)
            score = self._evaluate_criterion(plan, criterion)
            scores[criterion] = score
            
            if score < 0.7:
                feedback_parts.append(f"- {criterion}: ê°œì„  í•„ìš” ({question})")
        
        overall_score = sum(scores.values()) / len(scores)
        
        return EvaluationResult(
            overall_score=overall_score,
            passed=overall_score >= self.config.threshold,
            feedback="\n".join(feedback_parts) if feedback_parts else "ê³„íšì´ ì–‘í˜¸í•©ë‹ˆë‹¤.",
            metadata={"phase": PDCAPhase.PLAN.value, "criteria_scores": scores}
        )
    
    async def evaluate_do(
        self,
        implementation: str,
        plan: str
    ) -> EvaluationResult:
        """
        Do ë‹¨ê³„ í‰ê°€ - êµ¬í˜„ì˜ ê³„íš ì¤€ìˆ˜ë„ í‰ê°€
        
        Args:
            implementation: êµ¬í˜„ ê²°ê³¼
            plan: ì›ë³¸ ê³„íš
        
        Returns:
            EvaluationResult: í‰ê°€ ê²°ê³¼
        """
        # ê°­ ë¶„ì„
        gap_result = await self.gap_analyzer.analyze(
            plan=plan,
            implementation=implementation
        )
        
        suggestions = []
        if gap_result.missing_features:
            suggestions.append(f"ëˆ„ë½ëœ ê¸°ëŠ¥: {', '.join(gap_result.missing_features[:5])}")
        if gap_result.gaps:
            suggestions.extend([g.get("recommendation", "") for g in gap_result.gaps[:3]])
        
        return EvaluationResult(
            overall_score=gap_result.match_rate,
            passed=gap_result.match_rate >= self.config.threshold,
            feedback=f"ê³„íš ëŒ€ë¹„ êµ¬í˜„ ì¼ì¹˜ìœ¨: {gap_result.match_rate:.1%}",
            suggestions=suggestions,
            metadata={"phase": PDCAPhase.DO.value, "gap_analysis": gap_result}
        )
    
    async def evaluate_check(
        self,
        actual_outcome: str,
        expected_outcome: str
    ) -> EvaluationResult:
        """
        Check ë‹¨ê³„ í‰ê°€ - ê²°ê³¼ì˜ ê¸°ëŒ€ì¹˜ ì¶©ì¡±ë„ í‰ê°€
        
        Args:
            actual_outcome: ì‹¤ì œ ê²°ê³¼
            expected_outcome: ì˜ˆìƒ ê²°ê³¼
        
        Returns:
            EvaluationResult: í‰ê°€ ê²°ê³¼
        """
        # ê²°ê³¼ ë¹„êµ
        gap_result = await self.gap_analyzer.analyze(
            plan=expected_outcome,
            implementation=actual_outcome
        )
        
        return EvaluationResult(
            overall_score=gap_result.match_rate,
            passed=gap_result.match_rate >= self.config.threshold,
            feedback=f"ê¸°ëŒ€ ê²°ê³¼ ì¶©ì¡±ë¥ : {gap_result.match_rate:.1%}",
            suggestions=gap_result.recommendations,
            metadata={"phase": PDCAPhase.CHECK.value}
        )
    
    def _evaluate_criterion(self, text: str, criterion: str) -> float:
        """ê¸°ì¤€ë³„ íœ´ë¦¬ìŠ¤í‹± í‰ê°€"""
        text_lower = text.lower()
        
        if criterion == "completeness":
            # í•„ìˆ˜ ì„¹ì…˜ ì¡´ì¬ ì—¬ë¶€
            sections = ["ëª©í‘œ", "ìš”êµ¬ì‚¬í•­", "ë²”ìœ„", "ì¼ì •", "goal", "requirement", "scope"]
            found = sum(1 for s in sections if s in text_lower)
            return min(1.0, found / 3)
        
        elif criterion == "clarity":
            # ë¬¸ì¥ ê¸¸ì´ì™€ êµ¬ì¡°
            sentences = text.split(".")
            avg_length = sum(len(s.split()) for s in sentences) / max(len(sentences), 1)
            # ì ì ˆí•œ ë¬¸ì¥ ê¸¸ì´ (10-25ë‹¨ì–´)
            if 10 <= avg_length <= 25:
                return 0.9
            elif 5 <= avg_length <= 35:
                return 0.7
            else:
                return 0.5
        
        elif criterion == "feasibility":
            # êµ¬ì²´ì  ìˆ˜ì¹˜/ê¸°í•œ ì–¸ê¸‰
            import re
            numbers = len(re.findall(r'\d+', text))
            dates = len(re.findall(r'\d{4}[-/]\d{2}[-/]\d{2}', text))
            return min(1.0, (numbers + dates * 2) / 10)
        
        elif criterion == "measurability":
            # ì¸¡ì • ê°€ëŠ¥í•œ í‚¤ì›Œë“œ
            keywords = ["kpi", "metric", "ì¸¡ì •", "ì§€í‘œ", "percent", "%", "rate", "score"]
            found = sum(1 for k in keywords if k in text_lower)
            return min(1.0, found / 3)
        
        return 0.5


# ============================================================================
# LLM Judge
# ============================================================================

class LLMJudge:
    """
    LLM ê¸°ë°˜ í’ˆì§ˆ í‰ê°€ì (LLM-as-Judge)
    
    ================================================================================
    ğŸ“‹ ì—­í• : LLMì„ í™œìš©í•œ ë‹¤ì°¨ì› í’ˆì§ˆ í‰ê°€
    ğŸ“… ìµœì¢… ì—…ë°ì´íŠ¸: 2026ë…„ 2ì›”
    ================================================================================
    
    ğŸ¯ í‰ê°€ ë°©ì‹:
        - Single Rating: ë‹¨ì¼ ì ìˆ˜ í‰ê°€
        - Pairwise Comparison: A/B ë¹„êµ í‰ê°€
        - Multi-Dimension: ë‹¤ì°¨ì› í‰ê°€
        - Rubric-Based: ë£¨ë¸Œë¦­ ê¸°ë°˜ í‰ê°€
    
    ğŸ“Œ ì‚¬ìš© ì˜ˆì‹œ:
        >>> judge = LLMJudge(JudgeConfig(judge_model="gpt-5.2"))
        >>> 
        >>> # ë‹¨ì¼ í‰ê°€
        >>> verdict = await judge.evaluate(
        ...     output="AI ìƒì„± ì‘ë‹µ",
        ...     criteria="ì •í™•ì„±, ìœ ìš©ì„±, ëª…í™•ì„±",
        ...     context={"task": "ì½”ë“œ ë¦¬ë·°"}
        ... )
        >>> print(f"ì ìˆ˜: {verdict.score}/10")
        >>> 
        >>> # A/B ë¹„êµ
        >>> comparison = await judge.compare(
        ...     output_a="ì‘ë‹µ A",
        ...     output_b="ì‘ë‹µ B",
        ...     criteria="ì–´ëŠ ì‘ë‹µì´ ë” ì •í™•í•œê°€?"
        ... )
    """
    
    DEFAULT_RUBRIC = """
    í‰ê°€ ê¸°ì¤€ (1-10ì ):
    - 10ì : ì™„ë²½í•¨, ê°œì„  ì—¬ì§€ ì—†ìŒ
    - 8-9ì : ìš°ìˆ˜í•¨, ê²½ë¯¸í•œ ê°œì„  ê°€ëŠ¥
    - 6-7ì : ì–‘í˜¸í•¨, ì¼ë¶€ ê°œì„  í•„ìš”
    - 4-5ì : ë³´í†µ, ìƒë‹¹í•œ ê°œì„  í•„ìš”
    - 2-3ì : ë¯¸í¡, ë§ì€ ê°œì„  í•„ìš”
    - 1ì : ë§¤ìš° ë¶€ì¡±, ì „ë©´ ì¬ì‘ì—… í•„ìš”
    """
    
    def __init__(
        self,
        config: Optional[JudgeConfig] = None,
        llm_client: Optional[Any] = None
    ):
        self.config = config or JudgeConfig()
        self.llm_client = llm_client
        self.logger = logging.getLogger(__name__)
    
    async def evaluate(
        self,
        output: str,
        criteria: str,
        context: Optional[Dict[str, Any]] = None,
        reference: Optional[str] = None
    ) -> JudgeVerdict:
        """
        ë‹¨ì¼ ì¶œë ¥ í‰ê°€
        
        Args:
            output: í‰ê°€ ëŒ€ìƒ ì¶œë ¥
            criteria: í‰ê°€ ê¸°ì¤€
            context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸
            reference: ì°¸ì¡° ë‹µë³€ (ìˆëŠ” ê²½ìš°)
        
        Returns:
            JudgeVerdict: íŒê²°
        """
        # LLM í˜¸ì¶œì´ ì—†ì„ ê²½ìš° íœ´ë¦¬ìŠ¤í‹± í‰ê°€
        if not self.llm_client:
            return await self._heuristic_evaluate(output, criteria, context)
        
        prompt = self._build_evaluation_prompt(output, criteria, context, reference)
        
        try:
            # LLM API í˜¸ì¶œ (êµ¬í˜„ í•„ìš”)
            response = await self._call_llm(prompt)
            return self._parse_verdict(response)
        except Exception as e:
            self.logger.error(f"LLM í‰ê°€ ì‹¤íŒ¨: {e}")
            return await self._heuristic_evaluate(output, criteria, context)
    
    async def compare(
        self,
        output_a: str,
        output_b: str,
        criteria: str,
        context: Optional[Dict[str, Any]] = None
    ) -> JudgeVerdict:
        """
        A/B ë¹„êµ í‰ê°€
        
        Args:
            output_a: ì¶œë ¥ A
            output_b: ì¶œë ¥ B
            criteria: ë¹„êµ ê¸°ì¤€
            context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸
        
        Returns:
            JudgeVerdict: ë¹„êµ íŒê²°
        """
        # íœ´ë¦¬ìŠ¤í‹± ë¹„êµ
        score_a = await self._heuristic_evaluate(output_a, criteria, context)
        score_b = await self._heuristic_evaluate(output_b, criteria, context)
        
        if score_a.score > score_b.score + 0.5:
            comparison = "A > B"
        elif score_b.score > score_a.score + 0.5:
            comparison = "A < B"
        else:
            comparison = "A = B"
        
        return JudgeVerdict(
            score=(score_a.score + score_b.score) / 2,
            reasoning=f"A: {score_a.score:.1f}, B: {score_b.score:.1f}",
            comparison=comparison
        )
    
    async def multi_dimension_evaluate(
        self,
        output: str,
        dimensions: List[EvaluationDimension],
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[EvaluationDimension, JudgeVerdict]:
        """
        ë‹¤ì°¨ì› í‰ê°€
        
        Args:
            output: í‰ê°€ ëŒ€ìƒ
            dimensions: í‰ê°€ ì°¨ì› ëª©ë¡
            context: ì»¨í…ìŠ¤íŠ¸
        
        Returns:
            Dict: ì°¨ì›ë³„ íŒê²°
        """
        results = {}
        
        for dim in dimensions:
            criteria = self._dimension_to_criteria(dim)
            verdict = await self.evaluate(output, criteria, context)
            results[dim] = verdict
        
        return results
    
    async def _heuristic_evaluate(
        self,
        output: str,
        criteria: str,
        context: Optional[Dict[str, Any]] = None
    ) -> JudgeVerdict:
        """íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ í‰ê°€ (LLM ì—†ì´)"""
        score = 5.0  # ê¸°ë³¸ ì ìˆ˜
        strengths = []
        weaknesses = []
        
        # ê¸¸ì´ í‰ê°€
        word_count = len(output.split())
        if 50 <= word_count <= 500:
            score += 1
            strengths.append("ì ì ˆí•œ ì‘ë‹µ ê¸¸ì´")
        elif word_count < 20:
            score -= 1
            weaknesses.append("ì‘ë‹µì´ ë„ˆë¬´ ì§§ìŒ")
        elif word_count > 1000:
            score -= 0.5
            weaknesses.append("ì‘ë‹µì´ ë‹¤ì†Œ ê¹€")
        
        # êµ¬ì¡° í‰ê°€
        if output.count("\n") >= 2:
            score += 0.5
            strengths.append("ì ì ˆí•œ êµ¬ì¡°í™”")
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ (ê¸°ì¤€ì—ì„œ)
        criteria_words = set(criteria.lower().split())
        output_words = set(output.lower().split())
        overlap = len(criteria_words & output_words) / max(len(criteria_words), 1)
        if overlap > 0.3:
            score += 1
            strengths.append("ê¸°ì¤€ ê´€ë ¨ ë‚´ìš© í¬í•¨")
        
        # ì ìˆ˜ ì •ê·œí™” (1-10)
        score = max(1.0, min(10.0, score))
        
        return JudgeVerdict(
            score=score,
            reasoning=f"íœ´ë¦¬ìŠ¤í‹± í‰ê°€: {len(strengths)}ê°œ ê°•ì , {len(weaknesses)}ê°œ ì•½ì ",
            strengths=strengths,
            weaknesses=weaknesses
        )
    
    def _dimension_to_criteria(self, dim: EvaluationDimension) -> str:
        """í‰ê°€ ì°¨ì›ì„ ê¸°ì¤€ ë¬¸ìì—´ë¡œ ë³€í™˜"""
        mapping = {
            EvaluationDimension.TASK_COMPLETION: "ì‘ì—…ì´ ì™„ì „íˆ ìˆ˜í–‰ë˜ì—ˆëŠ”ê°€?",
            EvaluationDimension.FACTUAL_ACCURACY: "ì •ë³´ê°€ ì •í™•í•˜ê³  ì‚¬ì‹¤ì— ê¸°ë°˜í•˜ëŠ”ê°€?",
            EvaluationDimension.RESPONSE_QUALITY: "ì‘ë‹µì˜ í’ˆì§ˆì´ ë†’ê³  ìœ ìš©í•œê°€?",
            EvaluationDimension.CODE_QUALITY: "ì½”ë“œê°€ ê¹”ë”í•˜ê³  ëª¨ë²” ì‚¬ë¡€ë¥¼ ë”°ë¥´ëŠ”ê°€?",
            EvaluationDimension.TOOL_USAGE: "ë„êµ¬ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í–ˆëŠ”ê°€?",
            EvaluationDimension.INSTRUCTION_FOLLOWING: "ì§€ì‹œì‚¬í•­ì„ ì •í™•íˆ ë”°ëëŠ”ê°€?",
            EvaluationDimension.CREATIVITY: "ì°½ì˜ì ì´ê³  í˜ì‹ ì ì¸ ì ‘ê·¼ì¸ê°€?",
            EvaluationDimension.EFFICIENCY: "íš¨ìœ¨ì ìœ¼ë¡œ ì‘ì—…ì„ ìˆ˜í–‰í–ˆëŠ”ê°€?",
            EvaluationDimension.SAFETY: "ì•ˆì „í•˜ê³  ì±…ì„ê° ìˆëŠ” ì‘ë‹µì¸ê°€?",
        }
        return mapping.get(dim, "ì „ë°˜ì ì¸ í’ˆì§ˆì„ í‰ê°€í•˜ì‹œì˜¤.")
    
    def _build_evaluation_prompt(
        self,
        output: str,
        criteria: str,
        context: Optional[Dict[str, Any]],
        reference: Optional[str]
    ) -> str:
        """í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        rubric = self.config.rubric or self.DEFAULT_RUBRIC
        
        prompt = f"""ë‹¤ìŒ ì¶œë ¥ë¬¼ì„ í‰ê°€í•´ì£¼ì„¸ìš”.

## í‰ê°€ ê¸°ì¤€
{criteria}

## ë£¨ë¸Œë¦­
{rubric}

## í‰ê°€ ëŒ€ìƒ
{output}

"""
        if reference:
            prompt += f"""
## ì°¸ì¡° ë‹µë³€
{reference}
"""
        
        if context:
            prompt += f"""
## ì»¨í…ìŠ¤íŠ¸
{json.dumps(context, ensure_ascii=False, indent=2)}
"""
        
        prompt += """
## ì¶œë ¥ í˜•ì‹
JSONìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{
    "score": <1-10 ì ìˆ˜>,
    "reasoning": "<íŒë‹¨ ê·¼ê±°>",
    "strengths": ["<ê°•ì 1>", "<ê°•ì 2>"],
    "weaknesses": ["<ì•½ì 1>", "<ì•½ì 2>"]
}
"""
        return prompt
    
    async def _call_llm(self, prompt: str) -> str:
        """LLM API í˜¸ì¶œ (êµ¬í˜„ í•„ìš”)"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” OpenAI/Azure API í˜¸ì¶œ
        raise NotImplementedError("LLM client not configured")
    
    def _parse_verdict(self, response: str) -> JudgeVerdict:
        """ì‘ë‹µ íŒŒì‹±"""
        try:
            data = json.loads(response)
            return JudgeVerdict(
                score=float(data.get("score", 5)),
                reasoning=data.get("reasoning", ""),
                strengths=data.get("strengths", []),
                weaknesses=data.get("weaknesses", [])
            )
        except (json.JSONDecodeError, KeyError):
            return JudgeVerdict(score=5.0, reasoning="íŒŒì‹± ì‹¤íŒ¨")


# ============================================================================
# Check-Act Iterator (Evaluator-Optimizer Pattern)
# ============================================================================

class CheckActIterator:
    """
    Check-Act ë°˜ë³µ ìµœì í™” (Evaluator-Optimizer íŒ¨í„´)
    
    ================================================================================
    ğŸ“‹ ì—­í• : ìë™ ê°œì„  ë£¨í”„ë¥¼ í†µí•œ ì¶œë ¥ í’ˆì§ˆ í–¥ìƒ
    ğŸ“… ìµœì¢… ì—…ë°ì´íŠ¸: 2026ë…„ 2ì›” (bkit Evaluator-Optimizer ì˜ê°)
    ================================================================================
    
    ğŸ¯ ì‘ë™ ë°©ì‹ (bkit ìŠ¤íƒ€ì¼):
        1. ì´ˆê¸° ì¶œë ¥ ìƒì„±
        2. Check: í’ˆì§ˆ í‰ê°€ (ëª©í‘œ: 90%)
        3. ë¯¸ë‹¬ ì‹œ Act: í”¼ë“œë°± ê¸°ë°˜ ê°œì„ 
        4. 2-3 ë°˜ë³µ (ìµœëŒ€ 5íšŒ)
        5. ëª©í‘œ ë‹¬ì„± ë˜ëŠ” ìµœëŒ€ ë°˜ë³µ ë„ë‹¬ ì‹œ ì¢…ë£Œ
    
    ğŸ“Œ ì‚¬ìš© ì˜ˆì‹œ:
        >>> iterator = CheckActIterator(
        ...     evaluator=LLMJudge(),
        ...     optimizer=optimizer_function,
        ...     threshold=0.9,      # 90% ëª©í‘œ
        ...     max_iterations=5    # ìµœëŒ€ 5íšŒ
        ... )
        >>> 
        >>> result = await iterator.iterate(
        ...     initial_output="ì´ˆê¸° ìƒì„± ê²°ê³¼",
        ...     criteria="ì½”ë“œ í’ˆì§ˆ, ë¬¸ì„œí™”, í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€"
        ... )
        >>> 
        >>> print(f"ë°˜ë³µ íšŸìˆ˜: {result.iterations}")
        >>> print(f"ìµœì¢… ì ìˆ˜: {result.final_score:.1%}")
        >>> print(f"ê°œì„ ìœ¨: {result.improvement:.1%}")
    """
    
    def __init__(
        self,
        evaluator: Optional[LLMJudge] = None,
        optimizer: Optional[Callable] = None,
        config: Optional[IterationConfig] = None
    ):
        self.evaluator = evaluator or LLMJudge()
        self.optimizer = optimizer
        self.config = config or IterationConfig()
        self.logger = logging.getLogger(__name__)
    
    async def iterate(
        self,
        initial_output: str,
        criteria: str = "ì „ë°˜ì ì¸ í’ˆì§ˆ",
        context: Optional[Dict[str, Any]] = None
    ) -> IterationResult:
        """
        Check-Act ë°˜ë³µ ì‹¤í–‰
        
        Args:
            initial_output: ì´ˆê¸° ì¶œë ¥
            criteria: í‰ê°€ ê¸°ì¤€
            context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸
        
        Returns:
            IterationResult: ë°˜ë³µ ê²°ê³¼
        """
        current_output = initial_output
        score_history = []
        feedback_history = []
        
        if self.config.verbose:
            self.logger.info(f"Check-Act Iteration ì‹œì‘ (ëª©í‘œ: {self.config.threshold:.0%})")
        
        for iteration in range(1, self.config.max_iterations + 1):
            # Check: í‰ê°€
            verdict = await self.evaluator.evaluate(
                current_output, 
                criteria, 
                context
            )
            current_score = verdict.score / 10.0  # 0-1 ì •ê·œí™”
            score_history.append(current_score)
            
            if self.config.verbose:
                self.logger.info(f"  [{iteration}] ì ìˆ˜: {current_score:.1%}")
            
            # ëª©í‘œ ë‹¬ì„± í™•ì¸
            if current_score >= self.config.threshold:
                if self.config.verbose:
                    self.logger.info(f"âœ… ëª©í‘œ ë‹¬ì„±! ({current_score:.1%} >= {self.config.threshold:.0%})")
                
                return IterationResult(
                    final_output=current_output,
                    iterations=iteration,
                    score_history=score_history,
                    feedback_history=feedback_history,
                    converged=True,
                    improvement=current_score - score_history[0],
                    final_score=current_score
                )
            
            # ê°œì„  ì—¬ì§€ í™•ì¸
            if len(score_history) >= 2:
                improvement = current_score - score_history[-2]
                if improvement < self.config.improvement_threshold:
                    if self.config.verbose:
                        self.logger.info(f"âš ï¸ ê°œì„  ì •ì²´ ({improvement:.1%} < {self.config.improvement_threshold:.1%})")
                    
                    if self.config.early_stop:
                        break
            
            # Act: ê°œì„ 
            feedback = self._generate_feedback(verdict)
            feedback_history.append(feedback)
            
            if self.optimizer:
                current_output = await self._optimize(current_output, feedback)
            else:
                # ê¸°ë³¸ ìµœì í™” (í”¼ë“œë°± ì¶”ê°€ ìš”ì²­)
                current_output = await self._default_optimize(current_output, feedback)
        
        # ìµœëŒ€ ë°˜ë³µ ë„ë‹¬
        if self.config.verbose:
            self.logger.info(f"ğŸ”„ ìµœëŒ€ ë°˜ë³µ ë„ë‹¬ (ìµœì¢…: {score_history[-1]:.1%})")
        
        return IterationResult(
            final_output=current_output,
            iterations=len(score_history),
            score_history=score_history,
            feedback_history=feedback_history,
            converged=score_history[-1] >= self.config.threshold,
            improvement=score_history[-1] - score_history[0],
            final_score=score_history[-1]
        )
    
    def _generate_feedback(self, verdict: JudgeVerdict) -> str:
        """í‰ê°€ ê²°ê³¼ì—ì„œ í”¼ë“œë°± ìƒì„±"""
        feedback_parts = []
        
        if verdict.weaknesses:
            feedback_parts.append("ê°œì„  í•„ìš” ì‚¬í•­:")
            for w in verdict.weaknesses:
                feedback_parts.append(f"  - {w}")
        
        if verdict.reasoning:
            feedback_parts.append(f"\ní‰ê°€ ì˜ê²¬: {verdict.reasoning}")
        
        return "\n".join(feedback_parts)
    
    async def _optimize(self, output: str, feedback: str) -> str:
        """ìµœì í™” í•¨ìˆ˜ í˜¸ì¶œ"""
        if asyncio.iscoroutinefunction(self.optimizer):
            return await self.optimizer(output, feedback)
        else:
            return self.optimizer(output, feedback)
    
    async def _default_optimize(self, output: str, feedback: str) -> str:
        """ê¸°ë³¸ ìµœì í™” (ë³€ê²½ ì—†ìŒ)"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” LLMì„ í†µí•œ ìˆ˜ì • ìš”ì²­
        return output


# ============================================================================
# Gap Analyzer
# ============================================================================

class GapAnalyzer:
    """
    ê°­ ë¶„ì„ê¸° (bkit ìŠ¤íƒ€ì¼)
    
    ================================================================================
    ğŸ“‹ ì—­í• : ê³„íšê³¼ êµ¬í˜„ ê°„ì˜ ê°­ ë¶„ì„
    ğŸ“… ìµœì¢… ì—…ë°ì´íŠ¸: 2026ë…„ 2ì›”
    ================================================================================
    
    ğŸ“Œ ì‚¬ìš© ì˜ˆì‹œ:
        >>> analyzer = GapAnalyzer()
        >>> result = await analyzer.analyze(
        ...     plan="ì„¤ê³„ ë¬¸ì„œ",
        ...     implementation="êµ¬í˜„ ì½”ë“œ"
        ... )
        >>> print(f"ì¼ì¹˜ìœ¨: {result.match_rate:.1%}")
    """
    
    def __init__(self, llm_client: Optional[Any] = None):
        self.llm_client = llm_client
        self.logger = logging.getLogger(__name__)
    
    async def analyze(
        self,
        plan: str,
        implementation: str,
        expected: Optional[str] = None,
        actual: Optional[str] = None
    ) -> GapAnalysisResult:
        """
        ê°­ ë¶„ì„ ìˆ˜í–‰
        
        Args:
            plan: ê³„íš/ì„¤ê³„ ë¬¸ì„œ
            implementation: ì‹¤ì œ êµ¬í˜„
            expected: ì˜ˆìƒ ê²°ê³¼
            actual: ì‹¤ì œ ê²°ê³¼
        
        Returns:
            GapAnalysisResult: ë¶„ì„ ê²°ê³¼
        """
        # íœ´ë¦¬ìŠ¤í‹± ë¶„ì„
        gaps = []
        missing = []
        extra = []
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë¹„êµ
        plan_keywords = self._extract_keywords(plan)
        impl_keywords = self._extract_keywords(implementation)
        
        # ëˆ„ë½ëœ í•­ëª©
        missing_keywords = plan_keywords - impl_keywords
        for kw in missing_keywords:
            missing.append(kw)
            gaps.append({
                "type": "missing",
                "item": kw,
                "severity": GapSeverity.MAJOR.value,
                "recommendation": f"'{kw}' êµ¬í˜„ í•„ìš”"
            })
        
        # ì¶”ê°€ëœ í•­ëª© (ë²”ìœ„ ì´ˆê³¼)
        extra_keywords = impl_keywords - plan_keywords
        for kw in list(extra_keywords)[:5]:  # ìƒìœ„ 5ê°œë§Œ
            extra.append(kw)
        
        # ì¼ì¹˜ìœ¨ ê³„ì‚°
        if plan_keywords:
            match_rate = len(plan_keywords & impl_keywords) / len(plan_keywords)
        else:
            match_rate = 1.0 if not impl_keywords else 0.5
        
        # ì‹¬ê°ë„ ì§‘ê³„
        severity_summary = {s: 0 for s in GapSeverity}
        for gap in gaps:
            sev = GapSeverity(gap["severity"])
            severity_summary[sev] += 1
        
        # ê¶Œì¥ ì‚¬í•­
        recommendations = []
        if match_rate < 0.5:
            recommendations.append("âš ï¸ ê³„íš ëŒ€ë¹„ êµ¬í˜„ ì¼ì¹˜ìœ¨ì´ ë‚®ìŠµë‹ˆë‹¤. ì „ì²´ ê²€í†  í•„ìš”.")
        elif match_rate < 0.7:
            recommendations.append("ì¼ë¶€ ê¸°ëŠ¥ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤. ìš°ì„ ìˆœìœ„ë³„ êµ¬í˜„ í•„ìš”.")
        elif match_rate < 0.9:
            recommendations.append("ëŒ€ë¶€ë¶„ êµ¬í˜„ë˜ì—ˆìŠµë‹ˆë‹¤. ì„¸ë¶€ ì‚¬í•­ ì ê²€ ê¶Œì¥.")
        else:
            recommendations.append("âœ… ê³„íš ëŒ€ë¹„ êµ¬í˜„ì´ ì˜ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return GapAnalysisResult(
            match_rate=match_rate,
            gaps=gaps,
            missing_features=missing,
            extra_features=extra,
            severity_summary=severity_summary,
            recommendations=recommendations
        )
    
    def _extract_keywords(self, text: str) -> set:
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        import re
        
        # ì†Œë¬¸ì ë³€í™˜ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text.lower())
        
        # ë¶ˆìš©ì–´
        stopwords = {
            'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been',
            'and', 'or', 'but', 'if', 'then', 'else', 'when', 'where',
            'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë°', 'ë˜ëŠ”', 'ê·¸ë¦¬ê³ ',
            'def', 'class', 'return', 'import', 'from', 'async', 'await'
        }
        
        words = text.split()
        keywords = {w for w in words if len(w) > 2 and w not in stopwords}
        
        return keywords


# ============================================================================
# Agent Benchmark
# ============================================================================

class AgentBenchmark:
    """
    ì—ì´ì „íŠ¸ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸
    
    ================================================================================
    ğŸ“‹ ì—­í• : ì—ì´ì „íŠ¸ ì„±ëŠ¥ ì¸¡ì • ë° ë¹„êµ
    ğŸ“… ìµœì¢… ì—…ë°ì´íŠ¸: 2026ë…„ 2ì›”
    ================================================================================
    
    ğŸ“Œ ì‚¬ìš© ì˜ˆì‹œ:
        >>> benchmark = AgentBenchmark()
        >>> 
        >>> # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì¶”ê°€
        >>> benchmark.add_test_case(
        ...     name="simple_qa",
        ...     input="ì„œìš¸ì˜ ìˆ˜ë„ëŠ”?",
        ...     expected="ì„œìš¸ì€ ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ì…ë‹ˆë‹¤.",
        ...     criteria="factual_accuracy"
        ... )
        >>> 
        >>> # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        >>> result = await benchmark.run(agent)
        >>> print(f"í‰ê·  ì ìˆ˜: {result.avg_score:.1%}")
    """
    
    def __init__(
        self,
        suite_name: str = "default",
        evaluator: Optional[LLMJudge] = None
    ):
        self.suite_name = suite_name
        self.evaluator = evaluator or LLMJudge()
        self.test_cases: List[Dict[str, Any]] = []
        self.logger = logging.getLogger(__name__)
    
    def add_test_case(
        self,
        name: str,
        input_text: str,
        expected: Optional[str] = None,
        criteria: str = "quality",
        weight: float = 1.0
    ):
        """
        í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì¶”ê°€
        
        Args:
            name: í…ŒìŠ¤íŠ¸ ì´ë¦„
            input_text: ì…ë ¥
            expected: ì˜ˆìƒ ì¶œë ¥
            criteria: í‰ê°€ ê¸°ì¤€
            weight: ê°€ì¤‘ì¹˜
        """
        self.test_cases.append({
            "name": name,
            "input": input_text,
            "expected": expected,
            "criteria": criteria,
            "weight": weight
        })
    
    async def run(
        self,
        agent_fn: Callable,
        agent_name: str = "test_agent"
    ) -> BenchmarkResult:
        """
        ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        
        Args:
            agent_fn: ì—ì´ì „íŠ¸ í•¨ìˆ˜ (async)
            agent_name: ì—ì´ì „íŠ¸ ì´ë¦„
        
        Returns:
            BenchmarkResult: ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
        """
        scores = []
        details = []
        passed = 0
        failed = 0
        
        for tc in self.test_cases:
            try:
                # ì—ì´ì „íŠ¸ í˜¸ì¶œ
                if asyncio.iscoroutinefunction(agent_fn):
                    output = await agent_fn(tc["input"])
                else:
                    output = agent_fn(tc["input"])
                
                # í‰ê°€
                verdict = await self.evaluator.evaluate(
                    output=output,
                    criteria=tc["criteria"],
                    reference=tc.get("expected")
                )
                
                score = verdict.score / 10.0
                scores.append(score * tc["weight"])
                
                if score >= 0.7:
                    passed += 1
                else:
                    failed += 1
                
                details.append({
                    "name": tc["name"],
                    "score": score,
                    "passed": score >= 0.7,
                    "verdict": verdict
                })
                
            except Exception as e:
                self.logger.error(f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ ({tc['name']}): {e}")
                failed += 1
                scores.append(0.0)
                details.append({
                    "name": tc["name"],
                    "score": 0.0,
                    "passed": False,
                    "error": str(e)
                })
        
        return BenchmarkResult(
            agent_name=agent_name,
            test_suite=self.suite_name,
            total_tests=len(self.test_cases),
            passed=passed,
            failed=failed,
            scores=scores,
            details=details
        )


# ============================================================================
# Quality Metrics
# ============================================================================

class QualityMetrics:
    """
    í’ˆì§ˆ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸°
    
    ================================================================================
    ğŸ“‹ ì—­í• : ì—ì´ì „íŠ¸ í’ˆì§ˆ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ë¶„ì„
    ğŸ“… ìµœì¢… ì—…ë°ì´íŠ¸: 2026ë…„ 2ì›”
    ================================================================================
    
    ğŸ“Œ ì‚¬ìš© ì˜ˆì‹œ:
        >>> metrics = QualityMetrics()
        >>> 
        >>> # ë©”íŠ¸ë¦­ ê¸°ë¡
        >>> metrics.record("task_completion", 0.95)
        >>> metrics.record("response_time_ms", 250)
        >>> 
        >>> # ë¦¬í¬íŠ¸ ìƒì„±
        >>> report = metrics.generate_report()
    """
    
    def __init__(self):
        self.metrics: Dict[str, List[float]] = {}
        self.timestamps: Dict[str, List[datetime]] = {}
        self.logger = logging.getLogger(__name__)
    
    def record(self, name: str, value: float):
        """
        ë©”íŠ¸ë¦­ ê¸°ë¡
        
        Args:
            name: ë©”íŠ¸ë¦­ ì´ë¦„
            value: ê°’
        """
        if name not in self.metrics:
            self.metrics[name] = []
            self.timestamps[name] = []
        
        self.metrics[name].append(value)
        self.timestamps[name].append(datetime.now(timezone.utc))
    
    def get_stats(self, name: str) -> Dict[str, float]:
        """
        ë©”íŠ¸ë¦­ í†µê³„ ì¡°íšŒ
        
        Args:
            name: ë©”íŠ¸ë¦­ ì´ë¦„
        
        Returns:
            Dict: í†µê³„ (mean, std, min, max, count)
        """
        if name not in self.metrics or not self.metrics[name]:
            return {}
        
        values = self.metrics[name]
        return {
            "mean": statistics.mean(values),
            "std": statistics.stdev(values) if len(values) > 1 else 0,
            "min": min(values),
            "max": max(values),
            "count": len(values),
            "latest": values[-1]
        }
    
    def generate_report(self) -> QualityReport:
        """
        í’ˆì§ˆ ë¦¬í¬íŠ¸ ìƒì„±
        
        Returns:
            QualityReport: ë¦¬í¬íŠ¸
        """
        dimension_breakdown = {}
        overall_scores = []
        
        for name, values in self.metrics.items():
            if values:
                avg = statistics.mean(values)
                dimension_breakdown[name] = avg
                
                # 0-1 ë²”ìœ„ ë©”íŠ¸ë¦­ë§Œ ì „ì²´ ì ìˆ˜ì— í¬í•¨
                if 0 <= avg <= 1:
                    overall_scores.append(avg)
        
        overall_score = statistics.mean(overall_scores) if overall_scores else 0.0
        
        # íŠ¸ë Œë“œ ê³„ì‚°
        trends = []
        for name, values in self.metrics.items():
            if len(values) >= 2:
                recent = statistics.mean(values[-5:])
                older = statistics.mean(values[:-5]) if len(values) > 5 else values[0]
                trend = "improving" if recent > older else "declining" if recent < older else "stable"
                trends.append({
                    "metric": name,
                    "trend": trend,
                    "recent_avg": recent,
                    "change": recent - older
                })
        
        # ê¶Œì¥ ì‚¬í•­
        recommendations = []
        for name, stats in [(n, self.get_stats(n)) for n in self.metrics]:
            if stats and stats.get("mean", 1) < 0.7:
                recommendations.append(f"'{name}' ê°œì„  í•„ìš” (í˜„ì¬: {stats['mean']:.1%})")
        
        # ìš”ì•½
        if overall_score >= 0.9:
            summary = "ì „ë°˜ì ìœ¼ë¡œ ìš°ìˆ˜í•œ í’ˆì§ˆì…ë‹ˆë‹¤."
        elif overall_score >= 0.7:
            summary = "ì–‘í˜¸í•œ í’ˆì§ˆì´ë‚˜ ì¼ë¶€ ê°œì„  í•„ìš”í•©ë‹ˆë‹¤."
        elif overall_score >= 0.5:
            summary = "ê°œì„ ì´ í•„ìš”í•œ ì˜ì—­ì´ ìˆìŠµë‹ˆë‹¤."
        else:
            summary = "ì „ë°˜ì ì¸ í’ˆì§ˆ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤."
        
        return QualityReport(
            summary=summary,
            overall_score=overall_score,
            dimension_breakdown=dimension_breakdown,
            trends=trends,
            recommendations=recommendations
        )
    
    def reset(self):
        """ë©”íŠ¸ë¦­ ì´ˆê¸°í™”"""
        self.metrics.clear()
        self.timestamps.clear()
