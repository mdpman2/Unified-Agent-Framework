#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Unified Agent Framework - ì„¤ì • ëª¨ë“ˆ

ì „ì—­ ì„¤ì • ë° í”„ë ˆì„ì›Œí¬ êµ¬ì„±ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.
"""

import os
import logging
from dataclasses import dataclass, field
from typing import Optional

from dotenv import load_dotenv
from semantic_kernel.connectors.ai.open_ai.prompt_execution_settings.azure_chat_prompt_execution_settings import (
    AzureChatPromptExecutionSettings
)

from .exceptions import ConfigurationError

__all__ = [
    "Settings",
    "FrameworkConfig",
    "DEFAULT_LLM_MODEL",
    "DEFAULT_API_VERSION",
    "SUPPORTED_MODELS",
    "O_SERIES_MODELS",
    "MODELS_WITHOUT_TEMPERATURE",
    "supports_temperature",
    "create_execution_settings",
]


class Settings:
    """
    í”„ë ˆì„ì›Œí¬ ì „ì—­ ì„¤ì • - ëª¨ë“  ì„¤ì •ì„ í•œ ê³³ì—ì„œ ê´€ë¦¬

    ì‚¬ìš©ë²•:
        # ëª¨ë¸ ë³€ê²½
        Settings.DEFAULT_MODEL = "gpt-4.1"

        # ì„¤ì • í™•ì¸
        print(Settings.DEFAULT_MODEL)
    """

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # LLM ëª¨ë¸ ì„¤ì •
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    DEFAULT_MODEL: str = "gpt-5.2"           # ê¸°ë³¸ ëª¨ë¸
    DEFAULT_API_VERSION: str = "2024-08-01-preview"  # API ë²„ì „
    DEFAULT_TEMPERATURE: float = 0.7         # ê¸°ë³¸ Temperature (GPT-4 ê³„ì—´ë§Œ)
    DEFAULT_MAX_TOKENS: int = 1000           # ê¸°ë³¸ ìµœëŒ€ í† í° ìˆ˜

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ì§€ì› ëª¨ë¸ ëª©ë¡
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    SUPPORTED_MODELS: list = [
        # GPT-4 ê³„ì—´
        "gpt-4", "gpt-4o", "gpt-4o-mini", "gpt-4.1", "gpt-4.1-mini", "gpt-4.1-nano",
        # GPT-5 ê³„ì—´
        "gpt-5", "gpt-5.1", "gpt-5.2",
        # o-ì‹œë¦¬ì¦ˆ (Reasoning)
        "o1", "o1-mini", "o1-preview", "o3", "o3-mini", "o4-mini"
    ]

    # Temperature ë¯¸ì§€ì› ëª¨ë¸ (ìë™ìœ¼ë¡œ temperature íŒŒë¼ë¯¸í„° ì œì™¸)
    MODELS_WITHOUT_TEMPERATURE: list = [
        "gpt-5", "gpt-5.1", "gpt-5.2",
        "o1", "o1-mini", "o1-preview", "o3", "o3-mini", "o4-mini"
    ]

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # í”„ë ˆì„ì›Œí¬ ì„¤ì •
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    CHECKPOINT_DIR: str = "./checkpoints"    # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²½ë¡œ
    ENABLE_TELEMETRY: bool = True            # OpenTelemetry í™œì„±í™”
    ENABLE_EVENTS: bool = True               # ì´ë²¤íŠ¸ ì‹œìŠ¤í…œ í™œì„±í™”
    ENABLE_STREAMING: bool = False           # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ í™œì„±í™”
    MAX_CACHE_SIZE: int = 100                # ë©”ëª¨ë¦¬ ìºì‹œ ìµœëŒ€ í¬ê¸°

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Memory ì„¤ì • (AWS AgentCore íŒ¨í„´)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ENABLE_MEMORY_HOOKS: bool = True         # Memory Hook í™œì„±í™”
    MEMORY_NAMESPACE: str = "/conversation"  # ë©”ëª¨ë¦¬ ë„¤ì„ìŠ¤í˜ì´ìŠ¤
    MAX_MEMORY_TURNS: int = 20               # ìµœëŒ€ ëŒ€í™” í„´ ìˆ˜
    SESSION_TTL_HOURS: int = 24              # ì„¸ì…˜ ë§Œë£Œ ì‹œê°„ (ì‹œê°„)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Supervisor ì„¤ì • (SRE Agent íŒ¨í„´)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    AUTO_APPROVE_SIMPLE_PLANS: bool = True   # ê°„ë‹¨í•œ ê³„íš ìë™ ìŠ¹ì¸
    MAX_SUPERVISOR_ROUNDS: int = 5           # Supervisor ìµœëŒ€ ë¼ìš´ë“œ

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ë¡œê¹… ì„¤ì •
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    LOG_LEVEL: str = "INFO"                  # ë¡œê·¸ ë ˆë²¨
    LOG_FILE: str = "agent_framework.log"    # ë¡œê·¸ íŒŒì¼ ê²½ë¡œ


# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ì „ì—­ ë³€ìˆ˜ (Settings í´ë˜ìŠ¤ ì°¸ì¡°)
DEFAULT_LLM_MODEL = Settings.DEFAULT_MODEL
DEFAULT_API_VERSION = Settings.DEFAULT_API_VERSION
SUPPORTED_MODELS = Settings.SUPPORTED_MODELS
MODELS_WITHOUT_TEMPERATURE = Settings.MODELS_WITHOUT_TEMPERATURE
O_SERIES_MODELS = Settings.MODELS_WITHOUT_TEMPERATURE  # o-ì‹œë¦¬ì¦ˆ ëª¨ë¸ (temperature ë¯¸ì§€ì›)


@dataclass
class FrameworkConfig:
    """
    í”„ë ˆì„ì›Œí¬ ì„¤ì • - Settings í´ë˜ìŠ¤ì˜ ê°’ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©

    ì‚¬ìš©ë²•:
        # ê¸°ë³¸ ì„¤ì • ì‚¬ìš© (Settings í´ë˜ìŠ¤ ê°’ ì ìš©)
        config = FrameworkConfig()

        # ì»¤ìŠ¤í…€ ì„¤ì •
        config = FrameworkConfig(
            model="gpt-4o",
            temperature=0.5,
            checkpoint_dir="./my_checkpoints"
        )

        # í™˜ê²½ë³€ìˆ˜ì—ì„œ ìë™ ë¡œë“œ
        config = FrameworkConfig.from_env()
    """
    # LLM ì„¤ì • - Settings í´ë˜ìŠ¤ ì°¸ì¡°
    model: str = field(default_factory=lambda: Settings.DEFAULT_MODEL)
    api_version: str = field(default_factory=lambda: Settings.DEFAULT_API_VERSION)
    temperature: float = field(default_factory=lambda: Settings.DEFAULT_TEMPERATURE)
    max_tokens: int = field(default_factory=lambda: Settings.DEFAULT_MAX_TOKENS)

    # Azure ì„¤ì • (í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
    api_key: Optional[str] = None
    endpoint: Optional[str] = None
    deployment_name: Optional[str] = None

    # í”„ë ˆì„ì›Œí¬ ì„¤ì • - Settings í´ë˜ìŠ¤ ì°¸ì¡°
    checkpoint_dir: str = field(default_factory=lambda: Settings.CHECKPOINT_DIR)
    enable_telemetry: bool = field(default_factory=lambda: Settings.ENABLE_TELEMETRY)
    enable_events: bool = field(default_factory=lambda: Settings.ENABLE_EVENTS)
    enable_streaming: bool = field(default_factory=lambda: Settings.ENABLE_STREAMING)
    max_cache_size: int = field(default_factory=lambda: Settings.MAX_CACHE_SIZE)

    # Memory ì„¤ì • - Settings í´ë˜ìŠ¤ ì°¸ì¡°
    enable_memory_hooks: bool = field(default_factory=lambda: Settings.ENABLE_MEMORY_HOOKS)
    memory_namespace: str = field(default_factory=lambda: Settings.MEMORY_NAMESPACE)
    max_memory_turns: int = field(default_factory=lambda: Settings.MAX_MEMORY_TURNS)
    session_ttl_hours: int = field(default_factory=lambda: Settings.SESSION_TTL_HOURS)

    # Supervisor ì„¤ì • - Settings í´ë˜ìŠ¤ ì°¸ì¡°
    auto_approve_simple_plans: bool = field(default_factory=lambda: Settings.AUTO_APPROVE_SIMPLE_PLANS)
    max_supervisor_rounds: int = field(default_factory=lambda: Settings.MAX_SUPERVISOR_ROUNDS)

    # ë¡œê¹… ì„¤ì • - Settings í´ë˜ìŠ¤ ì°¸ì¡°
    log_level: str = field(default_factory=lambda: Settings.LOG_LEVEL)
    log_file: Optional[str] = field(default_factory=lambda: Settings.LOG_FILE)

    @classmethod
    def from_env(cls, dotenv_path: Optional[str] = None) -> 'FrameworkConfig':
        """
        í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„¤ì • ë¡œë“œ

        ì§€ì›í•˜ëŠ” í™˜ê²½ë³€ìˆ˜ (ìš°ì„ ìˆœìœ„ ìˆœì„œ):
        - API Key: AZURE_OPENAI_API_KEY
        - Endpoint: AZURE_OPENAI_ENDPOINT
        - Deployment: AZURE_OPENAI_DEPLOYMENT
        - API Version: AZURE_OPENAI_API_VERSION (ê¸°ë³¸: 2024-08-01-preview)
        """
        load_dotenv(dotenv_path)

        # API Key
        api_key = os.getenv("AZURE_OPENAI_API_KEY")

        # Endpoint
        endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")

        # Deployment Name
        deployment_name = os.getenv("AZURE_OPENAI_DEPLOYMENT")

        # í™˜ê²½ë³€ìˆ˜ ê°’ì—ì„œ ë”°ì˜´í‘œì™€ ê³µë°± ì œê±° (Windows .env íŒŒì¼ ë¬¸ì œ í•´ê²°)
        if api_key:
            api_key = api_key.strip().strip('"').strip("'").strip()
        if endpoint:
            endpoint = endpoint.strip().strip('"').strip("'").strip()
        if deployment_name:
            deployment_name = deployment_name.strip().strip('"').strip("'").strip()

        return cls(
            api_key=api_key,
            endpoint=endpoint,
            deployment_name=deployment_name,
            model=os.getenv("AZURE_OPENAI_MODEL", Settings.DEFAULT_MODEL),
            api_version=os.getenv("AZURE_OPENAI_API_VERSION", Settings.DEFAULT_API_VERSION),
        )

    def validate(self) -> bool:
        """ì„¤ì • ìœ íš¨ì„± ê²€ì¦"""
        missing = []
        if not self.api_key:
            missing.append("api_key (AZURE_OPENAI_API_KEY)")
        if not self.endpoint:
            missing.append("endpoint (AZURE_OPENAI_ENDPOINT)")
        if not self.deployment_name:
            missing.append("deployment_name (AZURE_OPENAI_DEPLOYMENT)")

        if missing:
            raise ConfigurationError(
                f"âŒ í•„ìˆ˜ ì„¤ì •ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤:\n" +
                "\n".join(f"  - {m}" for m in missing) +
                "\n\nğŸ’¡ .env íŒŒì¼ì„ ìƒì„±í•˜ê±°ë‚˜ í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”."
            )
        return True


def supports_temperature(model: str) -> bool:
    """
    ëª¨ë¸ì´ temperature íŒŒë¼ë¯¸í„°ë¥¼ ì§€ì›í•˜ëŠ”ì§€ í™•ì¸

    Args:
        model: ëª¨ë¸ ì´ë¦„ (ì˜ˆ: 'gpt-4.1', 'gpt-5', 'o1')

    Returns:
        bool: temperature ì§€ì› ì—¬ë¶€

    Note:
        GPT-5, o1, o3 ê³„ì—´ ëª¨ë¸ì€ temperatureë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    """
    model_lower = model.lower()
    return model_lower not in Settings.MODELS_WITHOUT_TEMPERATURE and \
           not any(model_lower.startswith(prefix) for prefix in ("gpt-5", "o1", "o3", "o4"))


def create_execution_settings(
    model: str,
    temperature: float = 0.7,
    max_tokens: int = 1000,
    service_id: Optional[str] = None,
    **kwargs
) -> AzureChatPromptExecutionSettings:
    """
    ëª¨ë¸ì— ë”°ë¼ ì ì ˆí•œ ì‹¤í–‰ ì„¤ì • ìƒì„±

    Args:
        model: ëª¨ë¸ ì´ë¦„
        temperature: ì˜¨ë„ ì„¤ì • (ì§€ì›í•˜ëŠ” ëª¨ë¸ì—ë§Œ ì ìš©)
        max_tokens: ìµœëŒ€ í† í° ìˆ˜
        service_id: ì„œë¹„ìŠ¤ ID (ì—†ìœ¼ë©´ model ì‚¬ìš©)
        **kwargs: ì¶”ê°€ ì„¤ì •

    Returns:
        AzureChatPromptExecutionSettings ì¸ìŠ¤í„´ìŠ¤
    """
    settings_kwargs = {
        "max_tokens": max_tokens,
        "service_id": service_id or model,
        **kwargs
    }

    # Temperature ì§€ì› ëª¨ë¸ì—ë§Œ temperature ì¶”ê°€
    if supports_temperature(model):
        settings_kwargs["temperature"] = temperature
    else:
        logging.info(f"â„¹ï¸ ëª¨ë¸ '{model}'ì€(ëŠ”) temperatureë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í•´ë‹¹ íŒŒë¼ë¯¸í„°ë¥¼ ìƒëµí•©ë‹ˆë‹¤.")

    return AzureChatPromptExecutionSettings(**settings_kwargs)
