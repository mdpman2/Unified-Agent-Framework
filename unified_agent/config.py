#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Unified Agent Framework - ì„¤ì • ëª¨ë“ˆ (Configuration Module)

================================================================================
ğŸ“ íŒŒì¼ ìœ„ì¹˜: unified_agent/config.py
ğŸ“‹ ì—­í• : ì „ì—­ ì„¤ì • ë° í”„ë ˆì„ì›Œí¬ êµ¬ì„± ê´€ë¦¬
ğŸ“… ìµœì¢… ì—…ë°ì´íŠ¸: 2026ë…„ 1ì›”
================================================================================

ğŸ¯ ì£¼ìš” ê¸°ëŠ¥:
    1. Settings í´ë˜ìŠ¤ - ì „ì—­ ì„¤ì • ê´€ë¦¬ (ëª¨ë¸, API ë²„ì „, ê¸°ëŠ¥ í† ê¸€ ë“±)
    2. FrameworkConfig - ì¸ìŠ¤í„´ìŠ¤ ë‹¨ìœ„ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ ìë™ ë¡œë“œ ì§€ì›)
    3. ëª¨ë¸ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ - temperature ì§€ì›, ë©€í‹°ëª¨ë‹¬, ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° í™•ì¸
    4. ì‹¤í–‰ ì„¤ì • ìƒì„± - AzureChatPromptExecutionSettings ì¸ìŠ¤í„´ìŠ¤ ìƒì„±

ğŸ”§ ì§€ì› ëª¨ë¸ (2026ë…„ 1ì›” ê¸°ì¤€):
    - OpenAI: GPT-5.2, GPT-5.1 Codex, GPT-4.1, o4-mini, o3
    - Anthropic: Claude Opus 4.5, Claude Sonnet 4.5 (Microsoft Foundry)
    - xAI: Grok-4, Grok-4 Fast Reasoning (Microsoft Foundry)
    - DeepSeek: V3.2, R1-0528
    - Meta: Llama 4 Maverick, Llama 4 Scout
    - Microsoft: Phi-4, Phi-4 Reasoning

ğŸ“Œ ì‚¬ìš© ì˜ˆì‹œ:
    >>> from unified_agent.config import Settings, FrameworkConfig
    >>>
    >>> # ì „ì—­ ëª¨ë¸ ë³€ê²½
    >>> Settings.DEFAULT_MODEL = "gpt-5.2"
    >>>
    >>> # í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„¤ì • ë¡œë“œ
    >>> config = FrameworkConfig.from_env()
    >>> config.validate()  # í•„ìˆ˜ ì„¤ì • ê²€ì¦

âš ï¸ ì£¼ì˜ì‚¬í•­:
    - Reasoning ëª¨ë¸(o1, o3, o4, GPT-5 ê¸°ë³¸)ì€ temperatureë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    - í™˜ê²½ë³€ìˆ˜ ì„¤ì • ì‹œ ë”°ì˜´í‘œì™€ ê³µë°±ì— ì£¼ì˜í•˜ì„¸ìš”.
    - LARGE_CONTEXT_MODELSëŠ” 100K+ í† í°ì„ ì§€ì›í•©ë‹ˆë‹¤.

ğŸ”— ê´€ë ¨ ë¬¸ì„œ:
    - Azure OpenAI: https://learn.microsoft.com/azure/ai-services/openai/
    - Microsoft Agent Framework: https://github.com/microsoft/agent-framework
"""

import os
import logging
from dataclasses import dataclass, field
from typing import Optional

from dotenv import load_dotenv
from semantic_kernel.connectors.ai.open_ai.prompt_execution_settings.azure_chat_prompt_execution_settings import (
    AzureChatPromptExecutionSettings
)

from .exceptions import ConfigurationError

__all__ = [
    "Settings",
    "FrameworkConfig",
    "DEFAULT_LLM_MODEL",
    "DEFAULT_API_VERSION",
    "SUPPORTED_MODELS",
    "O_SERIES_MODELS",
    "MODELS_WITHOUT_TEMPERATURE",
    "supports_temperature",
    "is_multimodal_model",
    "is_large_context_model",
    "get_model_context_window",
    "create_execution_settings",
]


class Settings:
    """
    í”„ë ˆì„ì›Œí¬ ì „ì—­ ì„¤ì • í´ë˜ìŠ¤ (Singleton-like Pattern)

    ================================================================================
    ğŸ“‹ ì—­í• : ëª¨ë“  ì „ì—­ ì„¤ì •ì„ í•œ ê³³ì—ì„œ ì¤‘ì•™ ì§‘ì¤‘ ê´€ë¦¬
    ğŸ“… ìµœì¢… ì—…ë°ì´íŠ¸: 2026ë…„ 1ì›”
    ================================================================================

    ğŸ¯ ì£¼ìš” ê¸°ëŠ¥:
        - LLM ëª¨ë¸ ì„¤ì • (ê¸°ë³¸ ëª¨ë¸, API ë²„ì „, ì˜¨ë„, í† í° ë“±)
        - ì§€ì› ëª¨ë¸ ëª©ë¡ ê´€ë¦¬ (SUPPORTED_MODELS, MODELS_WITHOUT_TEMPERATURE)
        - í”„ë ˆì„ì›Œí¬ ê¸°ëŠ¥ í† ê¸€ (ìŠ¤íŠ¸ë¦¬ë°, í…”ë ˆë©”íŠ¸ë¦¬, ì´ë²¤íŠ¸ ë“±)
        - Memory ì‹œìŠ¤í…œ ì„¤ì • (í›…, ë„¤ì„ìŠ¤í˜ì´ìŠ¤, TTL ë“±)
        - MCP (Model Context Protocol) ì„¤ì •
        - Multi-Agent ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ì„¤ì •
        - RAI (Responsible AI) ì„¤ì •

    ğŸ“Œ ì‚¬ìš© ì˜ˆì‹œ:
        >>> # ê¸°ë³¸ ëª¨ë¸ ë³€ê²½ (ëŸ°íƒ€ì„)
        >>> Settings.DEFAULT_MODEL = "claude-opus-4-5"
        >>>
        >>> # ìŠ¤íŠ¸ë¦¬ë° ë¹„í™œì„±í™”
        >>> Settings.ENABLE_STREAMING = False
        >>>
        >>> # MCP ì„¤ì • ì¡°ì •
        >>> Settings.MCP_APPROVAL_MODE = "always"  # ëª¨ë“  MCP í˜¸ì¶œì— ìŠ¹ì¸ í•„ìš”
        >>>
        >>> # ì§€ì› ëª¨ë¸ í™•ì¸
        >>> print(Settings.SUPPORTED_MODELS)

    ğŸ”§ 2026ë…„ 1ì›” ì—…ë°ì´íŠ¸ ë‚´ì—­:
        âœ… GPT-5.2 ì‹œë¦¬ì¦ˆ: gpt-5.2, gpt-5.2-chat, gpt-5.2-codex (ìµœì‹ )
        âœ… GPT-5.1 Codex: gpt-5.1-codex, gpt-5.1-codex-mini, gpt-5.1-codex-max
        âœ… o4-mini: o3-mini í›„ì† Reasoning ëª¨ë¸
        âœ… Claude 4.5: claude-opus-4-5, claude-sonnet-4-5 (Microsoft Foundry)
        âœ… Grok-4: grok-4, grok-4-fast-reasoning, grok-4-fast-non-reasoning
        âœ… DeepSeek: V3.2, V3.2-speciale, R1-0528 (Reasoning)
        âœ… Llama 4: llama-4-maverick-17b, llama-4-scout-17b (Meta)
        âœ… Phi-4: phi-4, phi-4-reasoning, phi-4-multimodal-instruct

    âš ï¸ ì£¼ì˜ì‚¬í•­:
        - í´ë˜ìŠ¤ ë³€ìˆ˜ì´ë¯€ë¡œ ëª¨ë“  ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ê³µìœ ë©ë‹ˆë‹¤.
        - ìŠ¤ë ˆë“œ ì•ˆì „í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë©€í‹°ìŠ¤ë ˆë“œ í™˜ê²½ì—ì„œëŠ” ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.
        - í™˜ê²½ë³€ìˆ˜ë¥¼ í†µí•œ ì„¤ì •ì€ FrameworkConfig.from_env()ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

    ğŸ”— ì°¸ê³ :
        - FrameworkConfig: ì¸ìŠ¤í„´ìŠ¤ ë‹¨ìœ„ ì„¤ì •
        - supports_temperature(): ëª¨ë¸ë³„ temperature ì§€ì› í™•ì¸
        - create_execution_settings(): ì‹¤í–‰ ì„¤ì • ìƒì„±
    """

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # LLM ëª¨ë¸ ì„¤ì • (2026ë…„ ìµœì‹ )
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    DEFAULT_MODEL: str = "gpt-5.2"           # ê¸°ë³¸ ëª¨ë¸ (2026ë…„ ìµœì‹ )
    DEFAULT_API_VERSION: str = "2025-12-01-preview"  # API ë²„ì „ (ìµœì‹ )
    DEFAULT_TEMPERATURE: float = 0.7         # ê¸°ë³¸ Temperature (GPT-4 ê³„ì—´ë§Œ)
    DEFAULT_MAX_TOKENS: int = 4096           # ê¸°ë³¸ ìµœëŒ€ í† í° ìˆ˜ (ì¦ê°€)
    DEFAULT_CONTEXT_WINDOW: int = 200000     # ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ì§€ì› ëª¨ë¸ ëª©ë¡ (2026ë…„ 1ì›” ê¸°ì¤€)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    SUPPORTED_MODELS: list = [
        # GPT-4 ê³„ì—´ (Legacy)
        "gpt-4", "gpt-4o", "gpt-4o-mini", "gpt-4.1", "gpt-4.1-mini", "gpt-4.1-nano",
        # GPT-5 ê³„ì—´ (2025ë…„ ì¶œì‹œ)
        "gpt-5", "gpt-5-mini", "gpt-5-nano", "gpt-5-chat", "gpt-5-pro",
        # GPT-5.1 ê³„ì—´
        "gpt-5.1", "gpt-5.1-chat", "gpt-5.1-codex", "gpt-5.1-codex-mini", "gpt-5.1-codex-max",
        # GPT-5.2 ê³„ì—´ (2026ë…„ ìµœì‹ )
        "gpt-5.2", "gpt-5.2-chat", "gpt-5.2-codex",
        # o-ì‹œë¦¬ì¦ˆ (Reasoning Models)
        "o1", "o1-mini", "o1-preview", "o3", "o3-mini", "o3-pro", "o4-mini",
        # Claude ì‹œë¦¬ì¦ˆ (Anthropic - Microsoft Foundry ì§€ì›)
        "claude-opus-4-5", "claude-sonnet-4-5", "claude-haiku-4-5", "claude-opus-4-1",
        # Grok ì‹œë¦¬ì¦ˆ (xAI - Microsoft Foundry ì§€ì›)
        "grok-4", "grok-4-fast-reasoning", "grok-4-fast-non-reasoning",
        "grok-3", "grok-3-mini", "grok-code-fast-1",
        # DeepSeek ì‹œë¦¬ì¦ˆ
        "deepseek-v3.2", "deepseek-v3.2-speciale", "deepseek-v3.1", "deepseek-r1-0528", "deepseek-r1",
        # Meta Llama 4 ì‹œë¦¬ì¦ˆ
        "llama-4-maverick-17b-128e-instruct-fp8", "llama-4-scout-17b-16e-instruct",
        "llama-3.3-70b-instruct",
        # Microsoft Phi ì‹œë¦¬ì¦ˆ
        "phi-4", "phi-4-reasoning", "phi-4-mini-reasoning", "phi-4-multimodal-instruct",
        # Mistral ì‹œë¦¬ì¦ˆ
        "mistral-large-3", "mistral-medium-2505", "mistral-small-2503",
        # ê¸°íƒ€
        "codex-mini", "computer-use-preview", "gpt-oss-120b"
    ]

    # Temperature ë¯¸ì§€ì› ëª¨ë¸ (ìë™ìœ¼ë¡œ temperature íŒŒë¼ë¯¸í„° ì œì™¸)
    # Reasoning ëª¨ë¸ ë° ì¼ë¶€ íŠ¹ìˆ˜ ëª¨ë¸
    MODELS_WITHOUT_TEMPERATURE: list = [
        # GPT-5 Reasoning ê³„ì—´
        "gpt-5", "gpt-5.1", "gpt-5.2", "gpt-5-pro",
        "gpt-5.1-codex", "gpt-5.1-codex-mini", "gpt-5.1-codex-max", "gpt-5.2-codex",
        # o-ì‹œë¦¬ì¦ˆ (ëª¨ë‘ Reasoning)
        "o1", "o1-mini", "o1-preview", "o3", "o3-mini", "o3-pro", "o4-mini",
        # DeepSeek Reasoning
        "deepseek-r1", "deepseek-r1-0528",
        # Phi Reasoning
        "phi-4-reasoning", "phi-4-mini-reasoning",
        # Codex íŠ¹ìˆ˜ ëª¨ë¸
        "codex-mini"
    ]

    # ëŒ€ìš©ëŸ‰ ì»¨í…ìŠ¤íŠ¸ ëª¨ë¸ (100K+ tokens)
    LARGE_CONTEXT_MODELS: list = [
        "gpt-5.2", "gpt-5.2-codex", "gpt-5.1", "gpt-5.1-codex", "gpt-5.1-codex-max",
        "gpt-4.1", "gpt-4.1-mini", "claude-opus-4-5", "claude-sonnet-4-5",
        "grok-4-fast-reasoning", "llama-4-scout-17b-16e-instruct"
    ]

    # Multimodal ëª¨ë¸ (ì´ë¯¸ì§€/ì˜¤ë””ì˜¤ ì…ë ¥ ì§€ì›)
    MULTIMODAL_MODELS: list = [
        "gpt-5.2", "gpt-5.2-chat", "gpt-5.1", "gpt-5.1-chat", "gpt-5",
        "gpt-4o", "gpt-4o-mini", "gpt-4.1",
        "claude-opus-4-5", "claude-sonnet-4-5", "claude-haiku-4-5",
        "grok-4", "grok-4-fast-reasoning",
        "phi-4-multimodal-instruct", "computer-use-preview"
    ]

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # í”„ë ˆì„ì›Œí¬ ì„¤ì •
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    CHECKPOINT_DIR: str = "./checkpoints"    # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²½ë¡œ
    ENABLE_TELEMETRY: bool = True            # OpenTelemetry í™œì„±í™”
    ENABLE_EVENTS: bool = True               # ì´ë²¤íŠ¸ ì‹œìŠ¤í…œ í™œì„±í™”
    ENABLE_STREAMING: bool = True            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ í™œì„±í™” (ê¸°ë³¸ í™œì„±í™”)
    MAX_CACHE_SIZE: int = 500                # ë©”ëª¨ë¦¬ ìºì‹œ ìµœëŒ€ í¬ê¸° (ì¦ê°€)
    ENABLE_PARALLEL_TOOLS: bool = True       # ë³‘ë ¬ ë„êµ¬ í˜¸ì¶œ í™œì„±í™”
    MAX_PARALLEL_TOOL_CALLS: int = 5         # ìµœëŒ€ ë³‘ë ¬ ë„êµ¬ í˜¸ì¶œ ìˆ˜

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Memory ì„¤ì • (Microsoft Agent Framework íŒ¨í„´)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ENABLE_MEMORY_HOOKS: bool = True         # Memory Hook í™œì„±í™”
    MEMORY_NAMESPACE: str = "/conversation"  # ë©”ëª¨ë¦¬ ë„¤ì„ìŠ¤í˜ì´ìŠ¤
    MAX_MEMORY_TURNS: int = 50               # ìµœëŒ€ ëŒ€í™” í„´ ìˆ˜ (ì¦ê°€)
    SESSION_TTL_HOURS: int = 72              # ì„¸ì…˜ ë§Œë£Œ ì‹œê°„ (ì¦ê°€)
    ENABLE_SEMANTIC_MEMORY: bool = True      # ì‹œë§¨í‹± ë©”ëª¨ë¦¬ í™œì„±í™”
    MEMORY_EMBEDDING_MODEL: str = "text-embedding-3-large"  # ì„ë² ë”© ëª¨ë¸

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # MCP (Model Context Protocol) ì„¤ì • - 2026 ìµœì‹ 
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ENABLE_MCP: bool = True                  # MCP í™œì„±í™”
    MCP_AUTO_CONNECT: bool = True            # MCP ìë™ ì—°ê²°
    MCP_RECONNECT_ATTEMPTS: int = 3          # MCP ì¬ì—°ê²° ì‹œë„ íšŸìˆ˜
    MCP_REQUEST_TIMEOUT: int = 30            # MCP ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
    MCP_APPROVAL_MODE: str = "selective"     # MCP ìŠ¹ì¸ ëª¨ë“œ (always/never/selective)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Multi-Agent Orchestration ì„¤ì • (Microsoft Agent Framework íŒ¨í„´)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    AUTO_APPROVE_SIMPLE_PLANS: bool = True   # ê°„ë‹¨í•œ ê³„íš ìë™ ìŠ¹ì¸
    MAX_SUPERVISOR_ROUNDS: int = 10          # Supervisor ìµœëŒ€ ë¼ìš´ë“œ (ì¦ê°€)
    ORCHESTRATION_MODE: str = "adaptive"     # ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ëª¨ë“œ (supervisor/sequential/parallel/adaptive)
    ENABLE_HANDOFF: bool = True              # ì—ì´ì „íŠ¸ ê°„ Handoff í™œì„±í™”
    MAX_CONCURRENT_AGENTS: int = 5           # ìµœëŒ€ ë™ì‹œ ì—ì´ì „íŠ¸ ìˆ˜
    ENABLE_REFLECTION: bool = True           # ë°˜ì„±(Reflection) íŒ¨í„´ í™œì„±í™”

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # RAI (Responsible AI) ì„¤ì •
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ENABLE_RAI_VALIDATION: bool = True       # RAI ê²€ì¦ í™œì„±í™”
    RAI_STRICT_MODE: bool = False            # RAI ì—„ê²© ëª¨ë“œ
    RAI_CONTENT_SAFETY_LEVEL: str = "medium" # ì½˜í…ì¸  ì•ˆì „ ë ˆë²¨ (low/medium/high)
    ENABLE_PII_DETECTION: bool = True        # PII ê°ì§€ í™œì„±í™”

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ë¡œê¹… ë° íŠ¸ë ˆì´ì‹± ì„¤ì •
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    LOG_LEVEL: str = "INFO"                  # ë¡œê·¸ ë ˆë²¨
    LOG_FILE: str = "agent_framework.log"    # ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
    ENABLE_TRACE_LOGGING: bool = True        # íŠ¸ë ˆì´ìŠ¤ ë¡œê¹… í™œì„±í™”
    TRACE_EXPORT_ENDPOINT: str = ""          # íŠ¸ë ˆì´ìŠ¤ ë‚´ë³´ë‚´ê¸° ì—”ë“œí¬ì¸íŠ¸


# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ì „ì—­ ë³€ìˆ˜ (Settings í´ë˜ìŠ¤ ì°¸ì¡°)
DEFAULT_LLM_MODEL = Settings.DEFAULT_MODEL
DEFAULT_API_VERSION = Settings.DEFAULT_API_VERSION
SUPPORTED_MODELS = Settings.SUPPORTED_MODELS
MODELS_WITHOUT_TEMPERATURE = Settings.MODELS_WITHOUT_TEMPERATURE
O_SERIES_MODELS = Settings.MODELS_WITHOUT_TEMPERATURE  # o-ì‹œë¦¬ì¦ˆ ëª¨ë¸ (temperature ë¯¸ì§€ì›)


@dataclass
class FrameworkConfig:
    """
    í”„ë ˆì„ì›Œí¬ ì¸ìŠ¤í„´ìŠ¤ ì„¤ì • (Dataclass)

    ================================================================================
    ğŸ“‹ ì—­í• : ê°œë³„ ì¸ìŠ¤í„´ìŠ¤ ë‹¨ìœ„ì˜ ì„¤ì • ê´€ë¦¬ (Settings í´ë˜ìŠ¤ ê°’ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©)
    ğŸ“… ìµœì¢… ì—…ë°ì´íŠ¸: 2026ë…„ 1ì›”
    ================================================================================

    ğŸ¯ ì£¼ìš” ê¸°ëŠ¥:
        - LLM ì„¤ì •: ëª¨ë¸, API ë²„ì „, ì˜¨ë„, ìµœëŒ€ í† í°
        - Azure ì„¤ì •: API í‚¤, ì—”ë“œí¬ì¸íŠ¸, ë°°í¬ëª… (í™˜ê²½ë³€ìˆ˜ ìë™ ë¡œë“œ)
        - í”„ë ˆì„ì›Œí¬ ì„¤ì •: ì²´í¬í¬ì¸íŠ¸, í…”ë ˆë©”íŠ¸ë¦¬, ì´ë²¤íŠ¸, ìŠ¤íŠ¸ë¦¬ë°
        - Memory ì„¤ì •: í›… í™œì„±í™”, ë„¤ì„ìŠ¤í˜ì´ìŠ¤, ìµœëŒ€ í„´ ìˆ˜, TTL
        - Supervisor ì„¤ì •: ìë™ ìŠ¹ì¸, ìµœëŒ€ ë¼ìš´ë“œ
        - ë¡œê¹… ì„¤ì •: ë ˆë²¨, íŒŒì¼ ê²½ë¡œ

    ğŸ“Œ ì‚¬ìš© ì˜ˆì‹œ:
        >>> # ë°©ë²• 1: ê¸°ë³¸ ì„¤ì • (Settings í´ë˜ìŠ¤ ê°’ ì‚¬ìš©)
        >>> config = FrameworkConfig()
        >>>
        >>> # ë°©ë²• 2: ì»¤ìŠ¤í…€ ì„¤ì •
        >>> config = FrameworkConfig(
        ...     model="gpt-5.2",
        ...     temperature=0.5,
        ...     enable_streaming=True,
        ...     checkpoint_dir="./my_checkpoints"
        ... )
        >>>
        >>> # ë°©ë²• 3: í™˜ê²½ë³€ìˆ˜ì—ì„œ ìë™ ë¡œë“œ (ê¶Œì¥)
        >>> config = FrameworkConfig.from_env()
        >>> config.validate()  # í•„ìˆ˜ ì„¤ì • ê²€ì¦ (api_key, endpoint, deployment_name)
        >>>
        >>> # ë°©ë²• 4: .env íŒŒì¼ ê²½ë¡œ ì§€ì •
        >>> config = FrameworkConfig.from_env(dotenv_path="./production.env")

    ğŸ”§ ì§€ì› í™˜ê²½ë³€ìˆ˜:
        - AZURE_OPENAI_API_KEY: Azure OpenAI API í‚¤ (í•„ìˆ˜)
        - AZURE_OPENAI_ENDPOINT: Azure OpenAI ì—”ë“œí¬ì¸íŠ¸ URL (í•„ìˆ˜)
        - AZURE_OPENAI_DEPLOYMENT: ëª¨ë¸ ë°°í¬ëª… (í•„ìˆ˜)
        - AZURE_OPENAI_API_VERSION: API ë²„ì „ (ì„ íƒ, ê¸°ë³¸: 2025-12-01-preview)
        - AZURE_OPENAI_MODEL: ëª¨ë¸ëª… (ì„ íƒ, ê¸°ë³¸: gpt-5.2)

    âš ï¸ ì£¼ì˜ì‚¬í•­:
        - validate() ë©”ì„œë“œë¡œ í•„ìˆ˜ ì„¤ì • í™•ì¸ í•„ìˆ˜
        - Windows .env íŒŒì¼ì˜ ë”°ì˜´í‘œ/ê³µë°± ìë™ ì²˜ë¦¬ë¨
        - dataclassì´ë¯€ë¡œ ë¶ˆë³€ì„±ì„ ë³´ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

    ğŸ”— ì°¸ê³ :
        - Settings: ì „ì—­ ì„¤ì • (í´ë˜ìŠ¤ ë³€ìˆ˜)
        - ConfigurationError: ì„¤ì • ì˜¤ë¥˜ ì‹œ ë°œìƒí•˜ëŠ” ì˜ˆì™¸
    """
    # LLM ì„¤ì • - Settings í´ë˜ìŠ¤ ì°¸ì¡°
    model: str = field(default_factory=lambda: Settings.DEFAULT_MODEL)
    api_version: str = field(default_factory=lambda: Settings.DEFAULT_API_VERSION)
    temperature: float = field(default_factory=lambda: Settings.DEFAULT_TEMPERATURE)
    max_tokens: int = field(default_factory=lambda: Settings.DEFAULT_MAX_TOKENS)

    # Azure ì„¤ì • (í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
    api_key: Optional[str] = None
    endpoint: Optional[str] = None
    deployment_name: Optional[str] = None

    # í”„ë ˆì„ì›Œí¬ ì„¤ì • - Settings í´ë˜ìŠ¤ ì°¸ì¡°
    checkpoint_dir: str = field(default_factory=lambda: Settings.CHECKPOINT_DIR)
    enable_telemetry: bool = field(default_factory=lambda: Settings.ENABLE_TELEMETRY)
    enable_events: bool = field(default_factory=lambda: Settings.ENABLE_EVENTS)
    enable_streaming: bool = field(default_factory=lambda: Settings.ENABLE_STREAMING)
    max_cache_size: int = field(default_factory=lambda: Settings.MAX_CACHE_SIZE)

    # Memory ì„¤ì • - Settings í´ë˜ìŠ¤ ì°¸ì¡°
    enable_memory_hooks: bool = field(default_factory=lambda: Settings.ENABLE_MEMORY_HOOKS)
    memory_namespace: str = field(default_factory=lambda: Settings.MEMORY_NAMESPACE)
    max_memory_turns: int = field(default_factory=lambda: Settings.MAX_MEMORY_TURNS)
    session_ttl_hours: int = field(default_factory=lambda: Settings.SESSION_TTL_HOURS)

    # Supervisor ì„¤ì • - Settings í´ë˜ìŠ¤ ì°¸ì¡°
    auto_approve_simple_plans: bool = field(default_factory=lambda: Settings.AUTO_APPROVE_SIMPLE_PLANS)
    max_supervisor_rounds: int = field(default_factory=lambda: Settings.MAX_SUPERVISOR_ROUNDS)

    # ë¡œê¹… ì„¤ì • - Settings í´ë˜ìŠ¤ ì°¸ì¡°
    log_level: str = field(default_factory=lambda: Settings.LOG_LEVEL)
    log_file: Optional[str] = field(default_factory=lambda: Settings.LOG_FILE)

    @classmethod
    def from_env(cls, dotenv_path: Optional[str] = None) -> 'FrameworkConfig':
        """
        í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„¤ì • ë¡œë“œ

        ì§€ì›í•˜ëŠ” í™˜ê²½ë³€ìˆ˜ (ìš°ì„ ìˆœìœ„ ìˆœì„œ):
        - API Key: AZURE_OPENAI_API_KEY
        - Endpoint: AZURE_OPENAI_ENDPOINT
        - Deployment: AZURE_OPENAI_DEPLOYMENT
        - API Version: AZURE_OPENAI_API_VERSION (ê¸°ë³¸: 2024-08-01-preview)
        """
        load_dotenv(dotenv_path)

        # API Key
        api_key = os.getenv("AZURE_OPENAI_API_KEY")

        # Endpoint
        endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")

        # Deployment Name
        deployment_name = os.getenv("AZURE_OPENAI_DEPLOYMENT")

        # í™˜ê²½ë³€ìˆ˜ ê°’ì—ì„œ ë”°ì˜´í‘œì™€ ê³µë°± ì œê±° (Windows .env íŒŒì¼ ë¬¸ì œ í•´ê²°)
        if api_key:
            api_key = api_key.strip().strip('"').strip("'").strip()
        if endpoint:
            endpoint = endpoint.strip().strip('"').strip("'").strip()
        if deployment_name:
            deployment_name = deployment_name.strip().strip('"').strip("'").strip()

        return cls(
            api_key=api_key,
            endpoint=endpoint,
            deployment_name=deployment_name,
            model=os.getenv("AZURE_OPENAI_MODEL", Settings.DEFAULT_MODEL),
            api_version=os.getenv("AZURE_OPENAI_API_VERSION", Settings.DEFAULT_API_VERSION),
        )

    def validate(self) -> bool:
        """ì„¤ì • ìœ íš¨ì„± ê²€ì¦"""
        missing = []
        if not self.api_key:
            missing.append("api_key (AZURE_OPENAI_API_KEY)")
        if not self.endpoint:
            missing.append("endpoint (AZURE_OPENAI_ENDPOINT)")
        if not self.deployment_name:
            missing.append("deployment_name (AZURE_OPENAI_DEPLOYMENT)")

        if missing:
            raise ConfigurationError(
                f"âŒ í•„ìˆ˜ ì„¤ì •ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤:\n" +
                "\n".join(f"  - {m}" for m in missing) +
                "\n\nğŸ’¡ .env íŒŒì¼ì„ ìƒì„±í•˜ê±°ë‚˜ í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”."
            )
        return True


def supports_temperature(model: str) -> bool:
    """
    ëª¨ë¸ì˜ temperature íŒŒë¼ë¯¸í„° ì§€ì› ì—¬ë¶€ í™•ì¸

    ================================================================================
    ğŸ“‹ ì—­í• : ì£¼ì–´ì§„ ëª¨ë¸ì´ temperature íŒŒë¼ë¯¸í„°ë¥¼ ì§€ì›í•˜ëŠ”ì§€ í™•ì¸
    ğŸ“… ìµœì¢… ì—…ë°ì´íŠ¸: 2026ë…„ 1ì›”
    ================================================================================

    ğŸ¯ ê¸°ëŠ¥ ì„¤ëª…:
        temperatureëŠ” LLM ì¶œë ¥ì˜ ë¬´ì‘ìœ„ì„±ì„ ì¡°ì ˆí•˜ëŠ” íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤.
        - 0.0: ê²°ì •ë¡ ì  (í•­ìƒ ë™ì¼í•œ ì¶œë ¥)
        - 1.0: ë†’ì€ ë¬´ì‘ìœ„ì„± (ì°½ì˜ì  ì¶œë ¥)

        Reasoning ëª¨ë¸(o1, o3, o4, GPT-5 ê¸°ë³¸ ë“±)ì€ ë‚´ë¶€ì ìœ¼ë¡œ ì¶”ë¡  ê³¼ì •ì„
        ì‚¬ìš©í•˜ë¯€ë¡œ temperature íŒŒë¼ë¯¸í„°ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        ì´ëŸ¬í•œ ëª¨ë¸ì— temperatureë¥¼ ì „ë‹¬í•˜ë©´ API ì˜¤ë¥˜ê°€ ë°œìƒí•©ë‹ˆë‹¤.

    Args:
        model (str): ëª¨ë¸ ì´ë¦„
            ì˜ˆ: 'gpt-4.1', 'gpt-5.2-chat', 'o3', 'claude-opus-4-5'

    Returns:
        bool: temperature ì§€ì› ì—¬ë¶€
            - True: temperature íŒŒë¼ë¯¸í„° ì‚¬ìš© ê°€ëŠ¥
            - False: temperature íŒŒë¼ë¯¸í„° ì‚¬ìš© ë¶ˆê°€ (API í˜¸ì¶œ ì‹œ ì œì™¸ í•„ìš”)

    ğŸ“Œ ì‚¬ìš© ì˜ˆì‹œ:
        >>> supports_temperature("gpt-4o")  # True (Chat ëª¨ë¸)
        >>> supports_temperature("gpt-5.2-chat")  # True (Chat ëª¨ë¸)
        >>> supports_temperature("gpt-5.2")  # False (Reasoning ëª¨ë¸)
        >>> supports_temperature("o4-mini")  # False (Reasoning ëª¨ë¸)
        >>> supports_temperature("claude-opus-4-5")  # True (Claude)

    ğŸ”§ Temperature ë¯¸ì§€ì› ëª¨ë¸ (2026ë…„ 1ì›” ê¸°ì¤€):
        - GPT-5 Reasoning: gpt-5, gpt-5.1, gpt-5.2, gpt-5-pro
        - GPT-5 Codex: gpt-5.1-codex, gpt-5.2-codex (ì½”ë“œ íŠ¹í™”)
        - o-ì‹œë¦¬ì¦ˆ ì „ì²´: o1, o1-mini, o3, o3-mini, o3-pro, o4-mini
        - DeepSeek Reasoning: deepseek-r1, deepseek-r1-0528
        - Phi Reasoning: phi-4-reasoning, phi-4-mini-reasoning
        - Codex íŠ¹ìˆ˜: codex-mini

    âš ï¸ ì£¼ì˜ì‚¬í•­:
        - 'chat' ì ‘ë¯¸ì‚¬ê°€ ìˆëŠ” ëª¨ë¸ì€ temperature ì§€ì› (ì˜ˆ: gpt-5.2-chat)
        - ìƒˆë¡œìš´ ëª¨ë¸ ì¶”ê°€ ì‹œ Settings.MODELS_WITHOUT_TEMPERATURE ì—…ë°ì´íŠ¸ í•„ìš”

    ğŸ”— ì°¸ê³ :
        - create_execution_settings(): ìë™ìœ¼ë¡œ ì´ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„¤ì • ìƒì„±
        - Settings.MODELS_WITHOUT_TEMPERATURE: ë¯¸ì§€ì› ëª¨ë¸ ëª©ë¡
    """
    model_lower = model.lower()

    # chat ëª¨ë¸ì€ temperature ì§€ì›
    if "chat" in model_lower:
        return True

    # ëª…ì‹œì ìœ¼ë¡œ temperature ë¯¸ì§€ì› ëª¨ë¸ í™•ì¸
    if model_lower in [m.lower() for m in Settings.MODELS_WITHOUT_TEMPERATURE]:
        return False

    # Reasoning ëª¨ë¸ ê³„ì—´ íŒ¨í„´ í™•ì¸
    reasoning_prefixes = (
        "gpt-5", "o1", "o3", "o4",
        "deepseek-r", "phi-4-reasoning", "phi-4-mini-reasoning",
        "codex"
    )
    return not any(model_lower.startswith(prefix) for prefix in reasoning_prefixes)


def is_multimodal_model(model: str) -> bool:
    """
    ëª¨ë¸ì˜ ë©€í‹°ëª¨ë‹¬ (ì´ë¯¸ì§€/ì˜¤ë””ì˜¤/ë¹„ë””ì˜¤ ì…ë ¥) ì§€ì› ì—¬ë¶€ í™•ì¸

    ================================================================================
    ğŸ“‹ ì—­í• : ì£¼ì–´ì§„ ëª¨ë¸ì´ í…ìŠ¤íŠ¸ ì™¸ ì…ë ¥(ì´ë¯¸ì§€, ì˜¤ë””ì˜¤ ë“±)ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸
    ğŸ“… ìµœì¢… ì—…ë°ì´íŠ¸: 2026ë…„ 1ì›”
    ================================================================================

    ğŸ¯ ê¸°ëŠ¥ ì„¤ëª…:
        ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì€ í…ìŠ¤íŠ¸ë¿ë§Œ ì•„ë‹ˆë¼ ì´ë¯¸ì§€, ì˜¤ë””ì˜¤, ë¹„ë””ì˜¤ ë“±
        ë‹¤ì–‘í•œ í˜•íƒœì˜ ì…ë ¥ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

        ì§€ì› ì…ë ¥ ìœ í˜•:
        - ì´ë¯¸ì§€: JPEG, PNG, GIF, WebP
        - ì˜¤ë””ì˜¤: MP3, WAV (ì¼ë¶€ ëª¨ë¸)
        - ë¹„ë””ì˜¤: MP4 (ì¼ë¶€ ëª¨ë¸, ì˜ˆ: phi-4-multimodal-instruct)

    Args:
        model (str): ëª¨ë¸ ì´ë¦„

    Returns:
        bool: ë©€í‹°ëª¨ë‹¬ ì§€ì› ì—¬ë¶€
            - True: ì´ë¯¸ì§€/ì˜¤ë””ì˜¤ ì…ë ¥ ì²˜ë¦¬ ê°€ëŠ¥
            - False: í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬ ê°€ëŠ¥

    ğŸ“Œ ì‚¬ìš© ì˜ˆì‹œ:
        >>> is_multimodal_model("gpt-5.2")  # True
        >>> is_multimodal_model("gpt-5.2-codex")  # False (ì½”ë“œ íŠ¹í™”)
        >>> is_multimodal_model("claude-opus-4-5")  # True
        >>> is_multimodal_model("o3")  # False

    ğŸ”§ ë©€í‹°ëª¨ë‹¬ ì§€ì› ëª¨ë¸ (2026ë…„ 1ì›” ê¸°ì¤€):
        - OpenAI: gpt-5.2, gpt-5.2-chat, gpt-5.1, gpt-5.1-chat, gpt-5, gpt-4o
        - Anthropic: claude-opus-4-5, claude-sonnet-4-5, claude-haiku-4-5
        - xAI: grok-4, grok-4-fast-reasoning
        - Microsoft: phi-4-multimodal-instruct
        - íŠ¹ìˆ˜: computer-use-preview (í™”ë©´ ìº¡ì²˜ ì…ë ¥)
    """
    return model.lower() in [m.lower() for m in Settings.MULTIMODAL_MODELS]


def is_large_context_model(model: str) -> bool:
    """
    ëª¨ë¸ì˜ ëŒ€ìš©ëŸ‰ ì»¨í…ìŠ¤íŠ¸ (100K+ í† í°) ì§€ì› ì—¬ë¶€ í™•ì¸

    ================================================================================
    ğŸ“‹ ì—­í• : ì£¼ì–´ì§„ ëª¨ë¸ì´ 100,000 í† í° ì´ìƒì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸
    ğŸ“… ìµœì¢… ì—…ë°ì´íŠ¸: 2026ë…„ 1ì›”
    ================================================================================

    ğŸ¯ ê¸°ëŠ¥ ì„¤ëª…:
        ëŒ€ìš©ëŸ‰ ì»¨í…ìŠ¤íŠ¸ ëª¨ë¸ì€ ê¸´ ë¬¸ì„œ, ì½”ë“œë² ì´ìŠ¤, ëŒ€í™” ê¸°ë¡ ë“±ì„
        í•œ ë²ˆì— ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

        ì¼ë°˜ì ì¸ ì»¨í…ìŠ¤íŠ¸ í¬ê¸°:
        - í‘œì¤€: 8K ~ 32K í† í°
        - ëŒ€ìš©ëŸ‰: 100K ~ 200K í† í°
        - ì´ˆëŒ€ìš©ëŸ‰: 400K+ í† í° (GPT-5.2, GPT-4.1)
        - ê·¹ëŒ€ìš©ëŸ‰: 1M ~ 10M í† í° (GPT-4.1, Llama 4 Scout)

    Args:
        model (str): ëª¨ë¸ ì´ë¦„

    Returns:
        bool: ëŒ€ìš©ëŸ‰ ì»¨í…ìŠ¤íŠ¸ ì§€ì› ì—¬ë¶€ (100K+ í† í°)

    ğŸ“Œ ì‚¬ìš© ì˜ˆì‹œ:
        >>> is_large_context_model("gpt-5.2")  # True (400K)
        >>> is_large_context_model("gpt-4o")  # False (128K)
        >>> is_large_context_model("gpt-4.1")  # True (1M)

    ğŸ”§ ëŒ€ìš©ëŸ‰ ì»¨í…ìŠ¤íŠ¸ ëª¨ë¸ (2026ë…„ 1ì›” ê¸°ì¤€):
        - 400K: gpt-5.2, gpt-5.2-codex, gpt-5.1, gpt-5.1-codex-max
        - 200K: claude-opus-4-5, claude-sonnet-4-5, o3, o4-mini
        - 1M: gpt-4.1, gpt-4.1-mini, gpt-4.1-nano
        - 2M: grok-4-fast-reasoning
        - 10M: llama-4-scout-17b-16e-instruct (ìµœëŒ€)
    """
    return model.lower() in [m.lower() for m in Settings.LARGE_CONTEXT_MODELS]


def get_model_context_window(model: str) -> int:
    """
    ëª¨ë¸ì˜ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° í¬ê¸° (í† í° ìˆ˜) ë°˜í™˜

    ================================================================================
    ğŸ“‹ ì—­í• : ì£¼ì–´ì§„ ëª¨ë¸ì´ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ í† í° ìˆ˜ ë°˜í™˜
    ğŸ“… ìµœì¢… ì—…ë°ì´íŠ¸: 2026ë…„ 1ì›”
    ================================================================================

    ğŸ¯ ê¸°ëŠ¥ ì„¤ëª…:
        ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°ëŠ” ëª¨ë¸ì´ í•œ ë²ˆì— ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ì…ë ¥ê³¼ ì¶œë ¥ì˜
        ì´ í† í° ìˆ˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ ê°’ì„ ì´ˆê³¼í•˜ë©´ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ê±°ë‚˜
        ì´ì „ ë‚´ìš©ì´ ì˜ë¦½ë‹ˆë‹¤.

        í† í° â‰ˆ ë‹¨ì–´ ë¹„ìœ¨ (ì˜ì–´ ê¸°ì¤€):
        - 1 í† í° â‰ˆ 0.75 ë‹¨ì–´ (ë˜ëŠ” 4ì)
        - 100K í† í° â‰ˆ 75,000 ë‹¨ì–´ â‰ˆ 300í˜ì´ì§€

    Args:
        model (str): ëª¨ë¸ ì´ë¦„

    Returns:
        int: ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° í¬ê¸° (í† í° ìˆ˜)
             ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ì€ Settings.DEFAULT_CONTEXT_WINDOW ë°˜í™˜

    ğŸ“Œ ì‚¬ìš© ì˜ˆì‹œ:
        >>> get_model_context_window("gpt-5.2")  # 400000
        >>> get_model_context_window("gpt-4.1")  # 1000000
        >>> get_model_context_window("llama-4-scout-17b-16e-instruct")  # 10000000
        >>> get_model_context_window("unknown-model")  # 200000 (ê¸°ë³¸ê°’)

    ğŸ”§ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ëª©ë¡ (2026ë…„ 1ì›” ê¸°ì¤€):
        - 128K: gpt-5.2-chat, gpt-5.1-chat
        - 200K: gpt-5, claude-opus-4-5, o3, o4-mini (ê¸°ë³¸ê°’)
        - 400K: gpt-5.2, gpt-5.1-codex, gpt-5-pro
        - 1M: gpt-4.1 ì‹œë¦¬ì¦ˆ
        - 2M: grok-4-fast-reasoning
        - 10M: llama-4-scout-17b-16e-instruct (ìµœëŒ€)
    """
    model_lower = model.lower()

    # 2026ë…„ ìµœì‹  ëª¨ë¸ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°
    context_windows = {
        # GPT-5.2 ì‹œë¦¬ì¦ˆ
        "gpt-5.2": 400000,
        "gpt-5.2-codex": 400000,
        "gpt-5.2-chat": 128000,
        # GPT-5.1 ì‹œë¦¬ì¦ˆ
        "gpt-5.1": 400000,
        "gpt-5.1-codex": 400000,
        "gpt-5.1-codex-max": 400000,
        "gpt-5.1-codex-mini": 400000,
        "gpt-5.1-chat": 128000,
        # GPT-5 ì‹œë¦¬ì¦ˆ
        "gpt-5": 200000,
        "gpt-5-pro": 400000,
        # GPT-4.1 ì‹œë¦¬ì¦ˆ
        "gpt-4.1": 1000000,
        "gpt-4.1-mini": 1000000,
        "gpt-4.1-nano": 1000000,
        # o-ì‹œë¦¬ì¦ˆ
        "o3": 200000,
        "o4-mini": 200000,
        # Claude ì‹œë¦¬ì¦ˆ
        "claude-opus-4-5": 200000,
        "claude-sonnet-4-5": 200000,
        # Grok ì‹œë¦¬ì¦ˆ
        "grok-4-fast-reasoning": 2000000,
        # Llama ì‹œë¦¬ì¦ˆ
        "llama-4-scout-17b-16e-instruct": 10000000,
    }

    return context_windows.get(model_lower, Settings.DEFAULT_CONTEXT_WINDOW)


def create_execution_settings(
    model: str,
    temperature: float = 0.7,
    max_tokens: int = 1000,
    service_id: Optional[str] = None,
    **kwargs
) -> AzureChatPromptExecutionSettings:
    """
    ëª¨ë¸ì— ë”°ë¼ ì ì ˆí•œ ì‹¤í–‰ ì„¤ì • ìƒì„±

    Args:
        model: ëª¨ë¸ ì´ë¦„
        temperature: ì˜¨ë„ ì„¤ì • (ì§€ì›í•˜ëŠ” ëª¨ë¸ì—ë§Œ ì ìš©)
        max_tokens: ìµœëŒ€ í† í° ìˆ˜
        service_id: ì„œë¹„ìŠ¤ ID (ì—†ìœ¼ë©´ model ì‚¬ìš©)
        **kwargs: ì¶”ê°€ ì„¤ì •

    Returns:
        AzureChatPromptExecutionSettings ì¸ìŠ¤í„´ìŠ¤
    """
    settings_kwargs = {
        "max_tokens": max_tokens,
        "service_id": service_id or model,
        **kwargs
    }

    # Temperature ì§€ì› ëª¨ë¸ì—ë§Œ temperature ì¶”ê°€
    if supports_temperature(model):
        settings_kwargs["temperature"] = temperature
    else:
        logging.info(f"â„¹ï¸ ëª¨ë¸ '{model}'ì€(ëŠ”) temperatureë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í•´ë‹¹ íŒŒë¼ë¯¸í„°ë¥¼ ìƒëµí•©ë‹ˆë‹¤.")

    return AzureChatPromptExecutionSettings(**settings_kwargs)
