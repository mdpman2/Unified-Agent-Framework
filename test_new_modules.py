#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Unified Agent Framework v3.5 - ì‹ ê·œ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸

================================================================================
ğŸ“ íŒŒì¼: test_new_modules.py
ğŸ“‹ ì—­í• : v3.5 ì‹ ê·œ ëª¨ë“ˆ (Structured Output, Evaluation) ì „ìš© í…ŒìŠ¤íŠ¸
ğŸ“… ìµœì¢… ì—…ë°ì´íŠ¸: 2026ë…„ 2ì›” 4ì¼
âœ… ê²°ê³¼: ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼
================================================================================

í…ŒìŠ¤íŠ¸ ë‚´ìš©:
    1. Structured Output
       - OutputSchema ìƒì„± ë° ê²€ì¦
       - StructuredOutputParser JSON íŒŒì‹±
       - StructuredOutputValidator ìŠ¤í‚¤ë§ˆ ê²€ì¦
       - pydantic_to_schema ë³€í™˜
    
    2. Evaluation (PDCA + LLM-as-Judge)
       - PDCAEvaluator PDCA ì‚¬ì´í´ í‰ê°€
       - LLMJudge í’ˆì§ˆ í‰ê°€
       - CheckActIterator ìë™ ê°œì„  ë£¨í”„
       - GapAnalyzer ê°­ ë¶„ì„
       - QualityMetrics í’ˆì§ˆ ë©”íŠ¸ë¦­

ì‹¤í–‰ ë°©ë²•:
    $ python test_new_modules.py
"""

import asyncio
import sys
sys.path.insert(0, r"d:\Azure-openai-sample\Unified-agent-framework")

from unified_agent import (
    # Structured Output
    OutputSchema,
    StructuredOutputConfig,
    StructuredOutputParser,
    StructuredOutputValidator,
    pydantic_to_schema,
    # Evaluation
    PDCAEvaluator,
    LLMJudge,
    CheckActIterator,
    GapAnalyzer,
    QualityMetrics,
    AgentBenchmark,
    EvaluationConfig,
    JudgeConfig,
    IterationConfig,
    EvaluationDimension,
    PDCAPhase,
    QualityLevel,
)


async def test_structured_output():
    """Structured Output í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 60)
    print("ğŸ§ª Structured Output í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # 1. ìŠ¤í‚¤ë§ˆ ì •ì˜ í…ŒìŠ¤íŠ¸
    schema = OutputSchema(
        name="PersonInfo",
        description="ê°œì¸ ì •ë³´ ìŠ¤í‚¤ë§ˆ",
        schema={
            "type": "object",
            "properties": {
                "name": {"type": "string", "description": "ì´ë¦„"},
                "age": {"type": "integer", "description": "ë‚˜ì´"},
                "email": {"type": "string", "description": "ì´ë©”ì¼"}
            },
            "required": ["name", "age"]
        },
        strict=True
    )
    print(f"âœ… ìŠ¤í‚¤ë§ˆ ìƒì„±: {schema.name}")
    
    # 2. JSON Schema ë³€í™˜ (OpenAI í˜•ì‹)
    openai_format = schema.to_openai_format()
    print(f"âœ… OpenAI í˜•ì‹ ë³€í™˜: {openai_format['json_schema']['name']}")
    
    # 3. Parser í…ŒìŠ¤íŠ¸
    parser = StructuredOutputParser()
    
    # ì •ìƒ JSON íŒŒì‹±
    valid_json = '{"name": "í™ê¸¸ë™", "age": 30, "email": "hong@example.com"}'
    result = parser.parse(valid_json, schema)
    print(f"âœ… ì •ìƒ JSON íŒŒì‹±: {result.data}")
    
    # ì½”ë“œ ë¸”ë¡ JSON íŒŒì‹±
    code_block_json = '```json\n{"name": "ê¹€ì² ìˆ˜", "age": 25}\n```'
    code_result = parser.parse(code_block_json, schema)
    print(f"âœ… ì½”ë“œ ë¸”ë¡ JSON íŒŒì‹±: {code_result.data}")
    
    # 4. Validator í…ŒìŠ¤íŠ¸
    validator = StructuredOutputValidator(schema)
    
    valid_data = {"name": "ì´ì˜í¬", "age": 25}
    is_valid, errors = validator.validate(valid_data)
    print(f"âœ… ìœ íš¨ì„± ê²€ì¦ (ìœ íš¨): {is_valid}, ì˜¤ë¥˜: {errors}")
    
    invalid_data = {"name": "ë°•ë¯¼ìˆ˜"}  # age ëˆ„ë½
    is_valid, errors = validator.validate(invalid_data)
    print(f"âœ… ìœ íš¨ì„± ê²€ì¦ (ë¬´íš¨): {is_valid}, ì˜¤ë¥˜: {errors}")
    
    print("\nâœ… Structured Output í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


async def test_evaluation():
    """Evaluation ëª¨ë“ˆ í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 60)
    print("ğŸ§ª Evaluation ëª¨ë“ˆ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # 1. Gap Analyzer í…ŒìŠ¤íŠ¸
    print("\nğŸ“‹ Gap Analyzer í…ŒìŠ¤íŠ¸...")
    analyzer = GapAnalyzer()
    
    plan = """
    ## í”„ë¡œì íŠ¸ ëª©í‘œ
    1. ì‚¬ìš©ì ì¸ì¦ ì‹œìŠ¤í…œ êµ¬í˜„
    2. ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™
    3. REST API ê°œë°œ
    4. í…ŒìŠ¤íŠ¸ ì½”ë“œ ì‘ì„±
    """
    
    implementation = """
    # êµ¬í˜„ ì™„ë£Œ ë‚´ìš©
    - ì‚¬ìš©ì ì¸ì¦: JWT í† í° ê¸°ë°˜ êµ¬í˜„
    - ë°ì´í„°ë² ì´ìŠ¤: PostgreSQL ì—°ë™ ì™„ë£Œ
    - REST API: CRUD ì—”ë“œí¬ì¸íŠ¸ êµ¬í˜„
    """
    
    gap_result = await analyzer.analyze(plan, implementation)
    print(f"âœ… ì¼ì¹˜ìœ¨: {gap_result.match_rate:.1%}")
    print(f"âœ… ëˆ„ë½ í•­ëª©: {gap_result.missing_features[:3]}")
    print(f"âœ… ê¶Œì¥ ì‚¬í•­: {gap_result.recommendations}")
    
    # 2. PDCA Evaluator í…ŒìŠ¤íŠ¸
    print("\nğŸ“‹ PDCA Evaluator í…ŒìŠ¤íŠ¸...")
    pdca = PDCAEvaluator()
    
    # Plan í‰ê°€
    plan_doc = """
    ## í”„ë¡œì íŠ¸ ê³„íš
    
    ### ëª©í‘œ
    AI ì±—ë´‡ ì‹œìŠ¤í…œ êµ¬ì¶•
    
    ### ìš”êµ¬ì‚¬í•­
    - ìì—°ì–´ ì²˜ë¦¬ ê¸°ëŠ¥
    - ë‹¤êµ­ì–´ ì§€ì›
    - 24ì‹œê°„ ìš´ì˜
    
    ### ë²”ìœ„
    ì›¹ ë° ëª¨ë°”ì¼ ì§€ì›
    
    ### ì¼ì •
    2026-03-01 ~ 2026-06-30
    """
    
    plan_result = await pdca.evaluate_plan(plan_doc)
    print(f"âœ… Plan í‰ê°€ ì ìˆ˜: {plan_result.overall_score:.1%}")
    print(f"âœ… í’ˆì§ˆ ìˆ˜ì¤€: {plan_result.quality_level.value}")
    
    # 3. LLM Judge í…ŒìŠ¤íŠ¸
    print("\nğŸ“‹ LLM Judge í…ŒìŠ¤íŠ¸...")
    judge = LLMJudge()
    
    output = """
    ì•ˆë…•í•˜ì„¸ìš”! Azure OpenAI ServiceëŠ” Microsoft Azure í´ë¼ìš°ë“œ í”Œë«í¼ì—ì„œ 
    ì œê³µí•˜ëŠ” AI ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤. GPT-4, GPT-5.2 ë“±ì˜ ìµœì‹  ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©°,
    ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ë³´ì•ˆê³¼ ê·œì • ì¤€ìˆ˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    
    ì£¼ìš” ê¸°ëŠ¥:
    1. í…ìŠ¤íŠ¸ ìƒì„±
    2. ì½”ë“œ ìƒì„±
    3. ì´ë¯¸ì§€ ë¶„ì„
    4. ìŒì„± ì¸ì‹
    """
    
    verdict = await judge.evaluate(
        output=output,
        criteria="ì •í™•ì„±, ì™„ì „ì„±, ëª…í™•ì„±"
    )
    print(f"âœ… Judge ì ìˆ˜: {verdict.score:.1f}/10")
    print(f"âœ… ê°•ì : {verdict.strengths}")
    print(f"âœ… ì•½ì : {verdict.weaknesses}")
    
    # 4. Check-Act Iterator í…ŒìŠ¤íŠ¸
    print("\nğŸ“‹ Check-Act Iterator í…ŒìŠ¤íŠ¸...")
    
    config = IterationConfig(
        threshold=0.7,      # 70% ëª©í‘œ (í…ŒìŠ¤íŠ¸ìš© ë‚®ì¶¤)
        max_iterations=3,   # ìµœëŒ€ 3íšŒ
        verbose=True
    )
    
    iterator = CheckActIterator(
        evaluator=judge,
        config=config
    )
    
    initial_output = "AzureëŠ” í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤."  # ì§§ì€ ì‘ë‹µ
    
    result = await iterator.iterate(
        initial_output=initial_output,
        criteria="ì‘ë‹µì˜ ê¸¸ì´ì™€ ì •ë³´ëŸ‰"
    )
    
    print(f"\nâœ… ë°˜ë³µ íšŸìˆ˜: {result.iterations}")
    print(f"âœ… ìµœì¢… ì ìˆ˜: {result.final_score:.1%}")
    print(f"âœ… ì ìˆ˜ ì´ë ¥: {[f'{s:.1%}' for s in result.score_history]}")
    
    # 5. Quality Metrics í…ŒìŠ¤íŠ¸
    print("\nğŸ“‹ Quality Metrics í…ŒìŠ¤íŠ¸...")
    metrics = QualityMetrics()
    
    # ë©”íŠ¸ë¦­ ê¸°ë¡
    for i in range(5):
        metrics.record("task_completion", 0.8 + i * 0.02)
        metrics.record("response_time_ms", 200 + i * 10)
    
    # í†µê³„ ì¡°íšŒ
    stats = metrics.get_stats("task_completion")
    print(f"âœ… task_completion í†µê³„: mean={stats['mean']:.2f}, min={stats['min']:.2f}, max={stats['max']:.2f}")
    
    # ë¦¬í¬íŠ¸ ìƒì„±
    report = metrics.generate_report()
    print(f"âœ… í’ˆì§ˆ ë¦¬í¬íŠ¸ ìš”ì•½: {report.summary}")
    print(f"âœ… ì¢…í•© ì ìˆ˜: {report.overall_score:.1%}")
    
    print("\nâœ… Evaluation í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


async def test_benchmark():
    """Benchmark í…ŒìŠ¤íŠ¸"""
    print("\n" + "=" * 60)
    print("ğŸ§ª Agent Benchmark í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    benchmark = AgentBenchmark(suite_name="simple_qa")
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì¶”ê°€
    benchmark.add_test_case(
        name="capital_question",
        input_text="ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ”?",
        expected="ì„œìš¸",
        criteria="ì •í™•ì„±"
    )
    benchmark.add_test_case(
        name="math_question",
        input_text="1 + 1 = ?",
        expected="2",
        criteria="ì •í™•ì„±"
    )
    
    # ê°„ë‹¨í•œ ì—ì´ì „íŠ¸ í•¨ìˆ˜
    async def simple_agent(query: str) -> str:
        if "ìˆ˜ë„" in query:
            return "ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸ì…ë‹ˆë‹¤."
        elif "1 + 1" in query:
            return "1 + 1 = 2 ì…ë‹ˆë‹¤."
        return "ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    result = await benchmark.run(simple_agent, "test_agent")
    
    print(f"âœ… í…ŒìŠ¤íŠ¸ ê²°ê³¼: {result.passed}/{result.total_tests} í†µê³¼")
    print(f"âœ… í‰ê·  ì ìˆ˜: {result.avg_score:.1%}")
    
    print("\nâœ… Benchmark í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("=" * 60)
    print("ğŸš€ Unified Agent Framework v3.5 ëª¨ë“ˆ í…ŒìŠ¤íŠ¸")
    print("   - Structured Output")
    print("   - Evaluation (PDCA, LLM-as-Judge, Check-Act)")
    print("=" * 60)
    
    await test_structured_output()
    await test_evaluation()
    await test_benchmark()
    
    print("\n" + "=" * 60)
    print("ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())
